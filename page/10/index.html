<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hui&#39;s Blog">
<meta property="og:url" content="http://example.com/page/10/index.html">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="六一">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Hui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Code Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Linux%20%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Linux%20%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6/" class="post-title-link" itemprop="url">Linux 系统中的日志管理机制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:47:39" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Command/" itemprop="url" rel="index"><span itemprop="name">Command</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Linux-的日志管理机制"><a href="#Linux-的日志管理机制" class="headerlink" title="Linux 的日志管理机制"></a>Linux 的日志管理机制</h3><hr>
<h3 id="一、Linux-日志管理机制概述"><a href="#一、Linux-日志管理机制概述" class="headerlink" title="一、Linux 日志管理机制概述"></a>一、Linux 日志管理机制概述</h3><p>Linux 系统主要通过 <strong>rsyslog</strong> (或早期版本中的 <code>syslog-ng</code>, <code>syslogd</code>) 和 <strong>journald</strong> (systemd 日志管理器) 这两种机制来收集、处理和存储系统日志。</p>
<h4 id="1-rsyslog-或-syslog-ng"><a href="#1-rsyslog-或-syslog-ng" class="headerlink" title="1. rsyslog (或 syslog-ng)"></a>1. <code>rsyslog</code> (或 <code>syslog-ng</code>)</h4><ul>
<li><strong>传统且广泛使用：</strong> <code>rsyslog</code> 是一个强大的日志处理工具，它能够从内核、应用程序和网络设备收集日志，并根据配置文件将日志写入到本地文件、转发到远程服务器或执行其他操作。</li>
<li><strong>文件存储：</strong> 日志通常以纯文本形式存储在 <code>/var/log</code> 目录下，按类型分类。</li>
<li><strong>配置文件：</strong> 主要配置文件是 <code>/etc/rsyslog.conf</code>。</li>
<li><strong>日志轮转：</strong> 配合 <code>logrotate</code> 工具进行日志文件的轮转、压缩和清理，防止日志文件无限增长填满磁盘。</li>
</ul>
<h4 id="2-journald-systemd-Journal"><a href="#2-journald-systemd-Journal" class="headerlink" title="2. journald (systemd Journal)"></a>2. <code>journald</code> (systemd Journal)</h4><ul>
<li><strong>现代 Linux 的核心日志系统：</strong> <code>journald</code> 是 <code>systemd</code> 组建的一部分，是现代 Linux 发行版（如 CentOS 7+, Ubuntu 15.04+）中默认的日志记录服务。</li>
<li><strong>二进制存储：</strong> <code>journald</code> 将所有日志事件存储在一个结构化的二进制文件中，而不是传统的纯文本文件。这使得日志的查询和分析更加高效，支持按字段过滤和结构化输出。</li>
<li><strong>统一收集：</strong> 它收集来自内核、<code>systemd</code> 服务、各种应用程序以及标准输出&#x2F;错误的日志。</li>
<li><strong>临时或持久化：</strong> 默认情况下，<code>journald</code> 日志存储在内存中 (<code>/run/log/journal</code>)，系统重启后会丢失。但可以配置为持久化存储到磁盘 (<code>/var/log/journal</code>)。</li>
</ul>
<hr>
<h3 id="二、如何查看系统日志"><a href="#二、如何查看系统日志" class="headerlink" title="二、如何查看系统日志"></a>二、如何查看系统日志</h3><h4 id="1-使用-journalctl-主要针对-journald-日志"><a href="#1-使用-journalctl-主要针对-journald-日志" class="headerlink" title="1. 使用 journalctl (主要针对 journald 日志)"></a>1. 使用 <code>journalctl</code> (主要针对 <code>journald</code> 日志)</h4><p><code>journalctl</code> 是 <code>systemd</code> 日志管理器的客户端工具，它是查看 <code>journald</code> 日志的首选命令。</p>
<ul>
<li><p><strong>查看所有日志：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl</span><br></pre></td></tr></table></figure>
<p>（输出可能非常多，按 <code>Page Up/Down</code> 翻页，<code>q</code> 退出）</p>
</li>
<li><p><strong>实时查看最新日志 (类似 <code>tail -f</code>)：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -f</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>按时间过滤：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">journalctl --since <span class="string">&quot;2023-10-26 10:00:00&quot;</span></span><br><span class="line">journalctl --since <span class="string">&quot;yesterday&quot;</span> --<span class="keyword">until</span> <span class="string">&quot;now&quot;</span></span><br><span class="line">journalctl -S <span class="string">&quot;2 days ago&quot;</span>     <span class="comment"># 查看最近2天的日志</span></span><br><span class="line">journalctl -u <span class="string">&quot;1 hour ago&quot;</span>     <span class="comment"># 查看最近1小时的日志（请注意这个参数U是unit，不是指时间）</span></span><br></pre></td></tr></table></figure>
<p><strong>更正：</strong> 查看最近时间段的日志通常使用 <code>-S</code> 或 <code>--since</code> 和 <code>-U</code> 或 <code>--until</code>。<br><br><code>--since &quot;2 days ago&quot;</code> 是正确的。<br><br><code>--since &quot;1 hour ago&quot;</code> 是正确的。<br></p>
</li>
<li><p><strong>按服务单元过滤：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u sshd.service     <span class="comment"># 查看 sshd 服务的日志</span></span><br><span class="line">journalctl -u nginx.service --since <span class="string">&quot;1 hour ago&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>按优先级过滤 (debug, info, notice, warning, err, crit, alert, emerg)：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">journalctl -p err              <span class="comment"># 查看所有错误级别及以上的日志</span></span><br><span class="line">journalctl -p warning -u httpd.service <span class="comment"># 查看 httpd 服务的警告及以上日志</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>按可执行文件路径过滤：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl /usr/sbin/sshd</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>按用户 ID (UID) 过滤：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl _UID=1000</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>按进程 ID (PID) 过滤：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl _PID=1234</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>显示内核日志：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -k</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>显示人类可读的输出格式：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">journalctl -o short            <span class="comment"># 默认短格式</span></span><br><span class="line">journalctl -o verbose          <span class="comment"># 详细格式</span></span><br><span class="line">journalctl -o json             <span class="comment"># JSON 格式，便于机器解析</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>查看指定数量的最新日志条目：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -n 20               <span class="comment"># 显示最新的20条日志</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-查看-var-log-目录下的日志文件-主要针对-rsyslog-传统日志"><a href="#2-查看-var-log-目录下的日志文件-主要针对-rsyslog-传统日志" class="headerlink" title="2. 查看 /var/log 目录下的日志文件 (主要针对 rsyslog &#x2F; 传统日志)"></a>2. 查看 <code>/var/log</code> 目录下的日志文件 (主要针对 <code>rsyslog</code> &#x2F; 传统日志)</h4><p>虽然 <code>journald</code> 越来越普及，但许多系统仍然配置 <code>rsyslog</code> 将日志写入 <code>/var/log</code>。很多应用程序也会直接将日志写入这里。</p>
<ul>
<li><p><strong>常用命令：</strong> <code>cat</code>, <code>less</code>, <code>more</code>, <code>tail</code>, <code>grep</code>。</p>
</li>
<li><p><strong><code>/var/log/messages</code> 或 <code>/var/log/syslog</code>：</strong></p>
<ul>
<li><strong>用途：</strong> 存放系统核心日志信息，包括启动信息、内核消息、服务信息等。</li>
<li><strong>查看：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tail</span> -f /var/log/messages    <span class="comment"># 实时查看最新消息</span></span><br><span class="line">less /var/log/syslog         <span class="comment"># 逐页查看</span></span><br><span class="line">grep <span class="string">&quot;error&quot;</span> /var/log/messages <span class="comment"># 搜索错误信息</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong><code>/var/log/auth.log</code> (Debian&#x2F;Ubuntu) 或 <code>/var/log/secure</code> (CentOS&#x2F;RHEL)：</strong></p>
<ul>
<li><strong>用途：</strong> 存放与认证和安全相关的日志，如用户登录、sudo 命令使用、SSH 连接尝试等。</li>
<li><strong>查看：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tail</span> -f /var/log/auth.log</span><br><span class="line">grep <span class="string">&quot;Failed password&quot;</span> /var/log/secure <span class="comment"># 查找 SSH 暴力破解尝试</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong><code>/var/log/kern.log</code>：</strong></p>
<ul>
<li><strong>用途：</strong> 存放内核相关的日志，包括硬件错误、驱动问题等。</li>
</ul>
</li>
<li><p><strong><code>/var/log/dmesg</code>：</strong></p>
<ul>
<li><strong>用途：</strong> 存放系统启动时的内核环缓冲信息。也可以直接使用 <code>dmesg</code> 命令查看。</li>
<li><strong>查看：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dmesg -T <span class="comment"># 显示带时间戳的内核日志</span></span><br><span class="line">dmesg | grep -i error <span class="comment"># 查找错误信息</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong><code>/var/log/faillog</code>：</strong> 记录用户登录失败信息（二进制文件，使用 <code>faillog</code> 命令查看）。</p>
</li>
<li><p><strong><code>/var/log/&lt;application_name&gt;/</code>：</strong></p>
<ul>
<li><strong>用途：</strong> 许多应用程序会在 <code>/var/log</code> 下创建自己的子目录来存放日志。例如，Web 服务器 (Apache&#x2F;Nginx)、数据库 (MySQL&#x2F;PostgreSQL) 等。</li>
<li><strong>示例：</strong><ul>
<li>Nginx 访问日志: <code>/var/log/nginx/access.log</code></li>
<li>Apache 错误日志: <code>/var/log/apache2/error.log</code> (Ubuntu) 或 <code>/var/log/httpd/error_log</code> (CentOS)</li>
<li>MySQL 日志: <code>/var/log/mysql/error.log</code> 或 <code>/var/log/mysqld.log</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="三、如何管理系统日志"><a href="#三、如何管理系统日志" class="headerlink" title="三、如何管理系统日志"></a>三、如何管理系统日志</h3><p>日志管理不仅仅是查看，还包括配置、轮转和清理，以防止日志文件无限制地增长并占用大量磁盘空间。</p>
<h4 id="1-journald-日志管理"><a href="#1-journald-日志管理" class="headerlink" title="1. journald 日志管理"></a>1. <code>journald</code> 日志管理</h4><ul>
<li><p><strong>持久化存储：</strong> 默认情况下，<code>journald</code> 日志存储在 <code>/run/log/journal</code>，重启后清空。要持久化存储，需要创建 <code>/var/log/journal</code> 目录并适当设置权限：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p /var/log/journal</span><br><span class="line"><span class="built_in">sudo</span> systemd-tmpfiles --create --prefix /var/log/journal</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart systemd-journald</span><br></pre></td></tr></table></figure>
<ul>
<li>这样日志会存储到 <code>/var/log/journal/&lt;machine-id&gt;/</code> 目录下。</li>
</ul>
</li>
<li><p><strong>限制日志大小：</strong> 编辑 <code>/etc/systemd/journald.conf</code> 文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/systemd/journald.conf</span><br></pre></td></tr></table></figure>
<p>查找或添加以下行来限制日志大小：</p>
<ul>
<li><code>SystemMaxUse=500M</code>：限制所有持久化日志文件的总大小不超过 500MB。</li>
<li><code>SystemKeepFree=15%</code>：确保至少有 15% 的磁盘空间可用。</li>
<li><code>RuntimeMaxUse=100M</code>：限制内存中日志的总大小不超过 100MB (针对非持久化存储)。<br>修改后重启 <code>journald</code> 服务：<code>sudo systemctl restart systemd-journald</code>。</li>
</ul>
</li>
<li><p><strong>手动清理日志：</strong></p>
<ul>
<li>按大小清理（保留最新 100M）：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> journalctl --vacuum-size=100M</span><br></pre></td></tr></table></figure></li>
<li>按时间清理（保留最新 7 天的日志）：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> journalctl --vacuum-time=7d</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="2-rsyslog-logrotate-日志管理"><a href="#2-rsyslog-logrotate-日志管理" class="headerlink" title="2. rsyslog &#x2F; logrotate 日志管理"></a>2. <code>rsyslog</code> &#x2F; <code>logrotate</code> 日志管理</h4><p><code>logrotate</code> 是一个用于管理和轮转日志文件的实用工具。它通常作为 cron job 每日运行。</p>
<ul>
<li><p><strong><code>logrotate</code> 主配置文件：</strong> <code>/etc/logrotate.conf</code></p>
<ul>
<li>定义了默认的轮转策略，并包含其他配置目录。</li>
</ul>
</li>
<li><p><strong>应用程序特定的轮转配置：</strong> <code>logrotate</code> 会从 <code>/etc/logrotate.d/</code> 目录中读取各个应用程序的配置文件。</p>
<ul>
<li><strong>示例：</strong> <code>/etc/logrotate.d/nginx</code> 可能包含 Nginx 日志的轮转配置。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/var/log/nginx/*.<span class="built_in">log</span> &#123;            <span class="comment"># 指定要轮转的日志文件</span></span><br><span class="line">    daily                         <span class="comment"># 每天轮转</span></span><br><span class="line">    missingok                     <span class="comment"># 如果日志文件丢失，不报错</span></span><br><span class="line">    rotate 7                      <span class="comment"># 保留 7 份旧日志文件</span></span><br><span class="line">    compress                      <span class="comment"># 压缩旧日志文件</span></span><br><span class="line">    delaycompress                 <span class="comment"># 在下一次轮转时才压缩最近的旧日志（避免服务写到被压缩的文件）</span></span><br><span class="line">    notifempty                    <span class="comment"># 如果日志文件为空，不轮转</span></span><br><span class="line">    create 0640 nginx adm         <span class="comment"># 轮转后创建新日志文件，并设置权限和属主/组</span></span><br><span class="line">    sharedscripts                 <span class="comment"># 共享脚本，只在所有日志文件处理完毕后执行一次</span></span><br><span class="line">    postrotate                    <span class="comment"># 轮转后执行的命令</span></span><br><span class="line">        <span class="keyword">if</span> [ -f /var/run/nginx.pid ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">kill</span> -USR1 `<span class="built_in">cat</span> /var/run/nginx.pid` <span class="comment"># 重启 Nginx 以打开新的日志文件</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>常用配置项解释：</strong></p>
<ul>
<li><code>daily</code>, <code>weekly</code>, <code>monthly</code>, <code>yearly</code>：轮转频率。</li>
<li><code>rotate N</code>：保留 N 份旧日志文件。</li>
<li><code>compress</code>：压缩轮转后的日志文件。</li>
<li><code>delaycompress</code>：延迟压缩。</li>
<li><code>notifempty</code>：如果日志文件为空，不轮转。</li>
<li><code>missingok</code>：如果日志文件不存在，不报错。</li>
<li><code>create [mode owner group]</code>：轮转后创建新的空日志文件并设置权限。</li>
<li><code>mail user</code>：将旧日志发送给指定用户。</li>
<li><code>postrotate / endscript</code>：在轮转后执行的脚本。</li>
<li><code>prerotate / endscript</code>：在轮转前执行的脚本。</li>
</ul>
</li>
<li><p><strong>手动执行 <code>logrotate</code> (测试配置)：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> logrotate -f /etc/logrotate.d/nginx <span class="comment"># 强制轮转指定的配置文件</span></span><br><span class="line"><span class="built_in">sudo</span> logrotate -d /etc/logrotate.d/nginx <span class="comment"># 调试模式，只显示将要执行的操作，不实际执行</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-配置-rsyslog-高级管理"><a href="#3-配置-rsyslog-高级管理" class="headerlink" title="3. 配置 rsyslog (高级管理)"></a>3. 配置 <code>rsyslog</code> (高级管理)</h4><p><code>rsyslog</code> 的配置文件 <code>/etc/rsyslog.conf</code> 允许你定义日志源、目的地和处理规则。</p>
<ul>
<li><p><strong>模块加载：</strong></p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span>(load=<span class="string">&quot;imuxsock&quot;</span>)  # 从本地 Unix socket 接收日志</span><br><span class="line"><span class="keyword">module</span>(load=<span class="string">&quot;imjournal&quot;</span>) # 从 journald 接收日志</span><br><span class="line"><span class="keyword">module</span>(load=<span class="string">&quot;imudp&quot;</span>)     # 接收 UDP 协议的日志 (通常是远程设备)</span><br><span class="line"><span class="keyword">module</span>(load=<span class="string">&quot;imtcp&quot;</span>)     # 接收 TCP 协议的日志 (通常是远程设备)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>规则定义 (facility.priority action)：</strong></p>
<ul>
<li><code>facility</code> (来源)：<code>auth</code>, <code>cron</code>, <code>daemon</code>, <code>kern</code>, <code>mail</code>, <code>syslog</code>, <code>user</code>, <code>local0</code>-<code>local7</code> 等。</li>
<li><code>priority</code> (级别)：<code>debug</code>, <code>info</code>, <code>notice</code>, <code>warning</code>, <code>err</code>, <code>crit</code>, <code>alert</code>, <code>emerg</code>。</li>
<li><code>action</code> (操作)：文件路径、远程主机、用户等。</li>
</ul>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">*.info;mail.none;authpriv.none;cron.none                /var/log/messages # 将除邮件、认证、定时任务外的所有 info 级别日志写入 /var/log/messages</span></span><br><span class="line">authpriv.*                                              /<span class="keyword">var</span>/<span class="keyword">log</span>/secure   # 所有认证相关的日志写入 /<span class="keyword">var</span>/<span class="keyword">log</span>/secure</span><br><span class="line">mail.*                                                  -/<span class="keyword">var</span>/<span class="keyword">log</span>/maillog # 所有邮件日志写入 /<span class="keyword">var</span>/<span class="keyword">log</span>/maillog (开头`-`表示异步写入，不阻塞)</span><br><span class="line">cron.*                                                  /<span class="keyword">var</span>/<span class="keyword">log</span>/cron</span><br><span class="line">kern.*                                                  /<span class="keyword">var</span>/<span class="keyword">log</span>/kern.<span class="keyword">log</span></span><br><span class="line">uucp,<span class="keyword">news</span>.crit                                          /<span class="keyword">var</span>/<span class="keyword">log</span>/spooler</span><br><span class="line">local7.*                                                /<span class="keyword">var</span>/<span class="keyword">log</span>/<span class="keyword">boot</span>.<span class="keyword">log</span></span><br><span class="line"># 转发所有认证日志到远程服务器 192.168.1.10 的 UDP 端口 514</span><br><span class="line">auth.*                                                  @192.168.1.10:514</span><br><span class="line"># 或 TCP 协议 (更可靠):</span><br><span class="line">auth.*                                                  @@192.168.1.10:514</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>重启 <code>rsyslog</code> 服务使配置生效：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart rsyslog</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>有效的日志管理是运维工作不可或缺的一部分。掌握 <code>journalctl</code> 命令可以高效查询现代 Linux 系统的大部分日志，而理解 <code>/var/log</code> 目录结构和 <code>logrotate</code> 配置则能帮助我们管理传统的基于文件的日志，并确保日志信息不会耗尽存储空间。通过结合这些工具和最佳实践，运维工程师能够更好地监控、排查和保障 Linux 系统的稳定运行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Linux%20%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6%E9%94%99%E8%AF%AF%E4%BF%AE%E6%8A%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Linux%20%E7%B3%BB%E7%BB%9F%E6%96%87%E4%BB%B6%E9%94%99%E8%AF%AF%E4%BF%AE%E6%8A%A4/" class="post-title-link" itemprop="url">Linux 系统文件错误修护</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:47:26" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Manage/" itemprop="url" rel="index"><span itemprop="name">Manage</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="修复-Linux-文件系统错误"><a href="#修复-Linux-文件系统错误" class="headerlink" title="修复 Linux 文件系统错误"></a>修复 Linux 文件系统错误</h3><hr>
<h3 id="一、文件系统错误的危害"><a href="#一、文件系统错误的危害" class="headerlink" title="一、文件系统错误的危害"></a>一、文件系统错误的危害</h3><p>文件系统错误可能导致以下问题：</p>
<ul>
<li><strong>数据丢失或损坏：</strong> 文件内容错乱，甚至文件消失。</li>
<li><strong>系统无法启动：</strong> 根文件系统损坏可能导致系统无法挂载核心目录。</li>
<li><strong>应用程序崩溃：</strong> 读取到损坏文件或元数据引发应用程序错误。</li>
<li><strong>性能下降：</strong> 文件系统需要花费额外时间处理错误，导致读写变慢。</li>
<li><strong>挂载失败：</strong> 损坏的文件系统可能无法被正确挂载。</li>
</ul>
<hr>
<h3 id="二、检查文件系统错误的工具：fsck"><a href="#二、检查文件系统错误的工具：fsck" class="headerlink" title="二、检查文件系统错误的工具：fsck"></a>二、检查文件系统错误的工具：<code>fsck</code></h3><p>Linux 中用于检查和修复文件系统错误的主要工具是 <code>fsck</code> （file system check）。它实际上是一个前端程序，会根据文件系统的类型（如 ext4, xfs, fat32 等）自动调用相应的特定工具，例如 <code>fsck.ext4</code>, <code>fsck.xfs</code> 等。</p>
<p><strong>重要原则：</strong><br><strong>绝对不要在已挂载（mounted）的文件系统上运行 <code>fsck</code>，特别是读写状态下挂载的文件系统。</strong> 这可能导致数据进一步损坏或丢失。唯一的例外是根文件系统 <code>/</code>，在某些情况下需要在运行时以只读方式进行检查，但通常更推荐在系统启动时或进入恢复模式进行检查。</p>
<h4 id="1-检查和修复未被挂载的文件系统"><a href="#1-检查和修复未被挂载的文件系统" class="headerlink" title="1. 检查和修复未被挂载的文件系统"></a>1. 检查和修复未被挂载的文件系统</h4><p>这是最安全和推荐的方式。</p>
<p><strong>步骤：</strong></p>
<ol>
<li><p><strong>确定要检查的分区：</strong><br>使用 <code>lsblk</code> 或 <code>fdisk -l</code> 命令查看分区信息，找到待检查的文件系统对应的设备名，例如 <code>/dev/sdb1</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsblk -f</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAME        FSTYPE LABEL UUID                                 MOUNTPOINT</span><br><span class="line">sda</span><br><span class="line">├─sda1      ext4   boot  a1b2c3d4-e5f6<span class="string">-7890</span><span class="string">-1234</span><span class="string">-567890</span>abcdef /boot</span><br><span class="line">└─sda2      ext4   data  b2c3d4e5-f6a7<span class="string">-8901</span><span class="string">-2345</span><span class="string">-67890</span>abcdef0 /data</span><br></pre></td></tr></table></figure>
<p>假设我们要检查 <code>/data</code> 分区，其设备名为 <code>/dev/sda2</code>。</p>
</li>
<li><p><strong>卸载文件系统：</strong><br>在运行 <code>fsck</code> 之前，必须确保目标文件系统是未挂载的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> umount /data</span><br></pre></td></tr></table></figure>
<p>如果文件系统正在被使用，你可能需要停止相关服务或关闭使用该文件系统的应用程序。如果无法卸载（例如它包含当前工作目录），你可能需要重启到单用户模式或使用 Live CD&#x2F;USB。</p>
</li>
<li><p><strong>运行 <code>fsck</code> 命令：</strong><br>最简单的运行方式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck /dev/sda2</span><br></pre></td></tr></table></figure>
<ul>
<li><code>fsck</code> 会自动检测文件系统类型并调用对应的工具。</li>
<li>在发现错误时，<code>fsck</code> 通常会提示用户进行确认（<code>y/n</code>）。</li>
</ul>
<p><strong>常用选项：</strong></p>
<ul>
<li><code>-y</code>：对所有问题都回答“是” (yes)，自动修复所有检测到的错误。<strong>请谨慎使用此选项，它可能在某些情况下删除数据！</strong> 尤其当不确定错误性质时，最好手动确认。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck -y /dev/sda2</span><br></pre></td></tr></table></figure></li>
<li><code>-n</code>：对所有问题都回答“否” (no)，只检查不修复。这常用于预先查看有哪些错误。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck -n /dev/sda2</span><br></pre></td></tr></table></figure></li>
<li><code>f</code>：强制检查，即使文件系统看起来“干净”也执行检查。有时系统会跳过“干净”文件系统的检查。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck -f /dev/sda2</span><br></pre></td></tr></table></figure></li>
<li><code>-p</code>：自动修复（预检查），如果错误是安全的（不涉及数据丢失），则自动修复。这比 <code>-y</code> 更安全。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck -p /dev/sda2</span><br></pre></td></tr></table></figure></li>
<li><code>-v</code>：显示详细的检查过程信息。</li>
</ul>
<p><strong><code>fsck.ext4</code> 的特定选项 (当文件系统是 ext2&#x2F;3&#x2F;4 时 <code>fsck</code> 会调用 <code>e2fsck</code>)：</strong></p>
<ul>
<li><code>-C 0</code>：在检查时显示进度条。</li>
<li><code>-c</code>：检查磁盘坏道，并将坏道标记出来（耗时较长）。</li>
<li><code>-b superblock</code>：使用指定的备用超级块。当主超级块损坏时非常有用。</li>
</ul>
</li>
<li><p><strong>重新挂载文件系统：</strong><br>检查和修复完成后，重新挂载文件系统。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> mount /dev/sda2 /data</span><br></pre></td></tr></table></figure>
<p>并检查是否能够正常访问文件。</p>
</li>
</ol>
<h4 id="2-检查根文件系统"><a href="#2-检查根文件系统" class="headerlink" title="2. 检查根文件系统 /"></a>2. 检查根文件系统 <code>/</code></h4><p>根文件系统 <code>/</code> 是一个特殊情况，因为它是系统运行所必需的，不能像普通数据盘那样随意卸载。</p>
<p><strong>方法一：系统启动时自动检查 (推荐)</strong></p>
<ul>
<li>Linux 系统通常配置为在启动时自动检查根文件系统，尤其是在非正常关机后。这通过 <code>/etc/fstab</code> 配置中的 <code>fs_passno</code> 字段（最后一个数字）。如果设置为 <code>1</code>，则在开机时第一个被检查；设置为 <code>2</code>，则第二个被检查；设置为 <code>0</code> 则不检查。</li>
<li>如果你想强制下次启动时检查根文件系统，可以创建一个特殊文件：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">touch</span> /forcefsck</span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br></pre></td></tr></table></figure>
系统会在下次启动时检测到 <code>/forcefsck</code> 文件，强制进行根文件系统的 <code>fsck</code> 检查（通常是 <code>fsck -a</code> 或 <code>fsck -y</code>），完成后会删除该文件。</li>
</ul>
<p><strong>方法二：进入单用户模式或救援模式</strong><br>这种方法更安全和可靠，因为在这种模式下，根文件系统通常以只读方式挂载，或者完全不挂载，方便进行修复。</p>
<ol>
<li><strong>重启系统。</strong></li>
<li>在 GRUB 启动菜单出现时，按 <code>e</code> 键编辑启动项。</li>
<li>找到以 <code>linux</code> 开头的一行，在其末尾添加 <code>single</code> 或 <code>init=/bin/bash</code>（前者进入单用户模式，后者直接进入一个繁忙的 shell，根文件系统往往是只读挂载的）。</li>
<li>按 <code>Ctrl+X</code> 或 <code>F10</code> 启动。</li>
<li>进入单用户模式后，如果根文件系统是只读挂载的，需要先重新挂载为读写模式：<code>mount -o remount,rw /</code>。如果出现错误，可能无法 remount。此时应直接运行 <code>fsck</code>，因为它应以只读方式检测。<ul>
<li>如果 <code>/</code> 被标记为只读 (read-only)：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fsck -f /dev/sdaX  <span class="comment"># sdaX 是你的根分区设备名</span></span><br></pre></td></tr></table></figure></li>
<li>如果需要手动挂载为只读：<code>mount -o ro /dev/sdaX /mnt</code>，然后 <code>fsck /dev/sdaX</code>。</li>
</ul>
</li>
<li>修复完成后，重启系统：<code>reboot</code>。</li>
</ol>
<p><strong>方法三：使用 Live CD&#x2F;USB</strong><br>如果系统无法启动，或者上述方法无效，使用 Live CD&#x2F;USB 是最佳选择。</p>
<ol>
<li>通过 Live CD&#x2F;USB 启动你的 Linux 系统。</li>
<li>打开终端。</li>
<li>使用 <code>lsblk -f</code> 找到你的根分区或其他需要修复的分区，例如 <code>/dev/sda1</code>。</li>
<li><strong>确保该分区未被挂载。</strong> 如果 Live 系统自动挂载了它，先 <code>sudo umount /media/your_root</code>。</li>
<li>运行 <code>fsck</code>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> fsck -f -y /dev/sda1  <span class="comment"># 替换为你的根分区设备名</span></span><br></pre></td></tr></table></figure></li>
<li>完成后，移除 Live CD&#x2F;USB 并重启系统。</li>
</ol>
<hr>
<h3 id="三、XFS-文件系统的检查和修复"><a href="#三、XFS-文件系统的检查和修复" class="headerlink" title="三、XFS 文件系统的检查和修复"></a>三、XFS 文件系统的检查和修复</h3><p>对于 XFS 文件系统，<code>fsck</code> 命令只是一个包装器，实际会调用 <code>xfs_repair</code>。</p>
<p><strong>重要！</strong> <code>xfs_repair</code> <strong>和 ext4 的 <code>fsck.ext4</code> 不同</strong>：</p>
<ul>
<li><code>xfs_repair</code> 在修复时，如果不加 <code>-L</code> 选项，它只会对日志进行恢复，不会深入文件系统结构本身进行全面检查和修复。</li>
<li>当文件系统损坏严重（例如元数据损坏）时，需要使用 <code>-L</code> 选项，这会<strong>丢弃文件系统的日志</strong>。<strong>丢弃日志意味着会丢失最近未同步到磁盘的数据！</strong> 务必谨慎使用 <code>-L</code>。</li>
</ul>
<p><strong>步骤：</strong></p>
<ol>
<li><p><strong>确定 XFS 文件系统分区：</strong> 例如 <code>/dev/sdd1</code>。</p>
</li>
<li><p><strong>卸载文件系统：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> umount /mount/point  <span class="comment"># 例如 umount /data</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>运行 <code>xfs_repair</code>：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> xfs_repair /dev/sdd1</span><br></pre></td></tr></table></figure>
<ul>
<li>如果 <code>/var/log/messages</code> 或 <code>dmesg</code> 显示 XFS 日志损坏，通常可以直接修复：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> xfs_repair -L /dev/sdd1  <span class="comment"># 谨慎使用，会丢弃日志</span></span><br></pre></td></tr></table></figure></li>
<li>如果需要检查但不修复（dry run），可以尝试：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fsck.xfs -n /dev/sdd1  <span class="comment"># 实际是调用 xfs_repair -n，但 XFS 自身不提供纯粹的“只检查不修复”模式</span></span><br><span class="line"><span class="comment"># xfs_repair -n 模拟修复过程，但不会真的写入磁盘。仍然建议在非挂载状态进行。</span></span><br></pre></td></tr></table></figure></li>
<li>XFS 没有像 ext4 那样显式的 <code>fsck</code> 报告。<code>xfs_repair</code> 运行时会显示它正在做的事情，完成后如果没有错误则表示修复成功。</li>
</ul>
</li>
<li><p><strong>重新挂载文件系统：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> mount /dev/sdd1 /mount/point</span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="四、数据备份与恢复的保障"><a href="#四、数据备份与恢复的保障" class="headerlink" title="四、数据备份与恢复的保障"></a>四、数据备份与恢复的保障</h3><p>作为运维工程师，我深知数据安全高于一切。在检查和修复文件系统错误之前，<strong>数据备份是最高优先级！</strong> 即使 <code>fsck</code> 和 <code>xfs_repair</code> 是为了修复数据，但在极少数情况下，它们也可能导致数据丢失，尤其是在文件系统严重损坏且需要强力修复时。</p>
<p><strong>备份策略：</strong></p>
<ul>
<li><strong>定期备份：</strong> 对所有关键数据和配置进行定期（每日&#x2F;每周&#x2F;每月）备份。</li>
<li><strong>增量&#x2F;差异备份：</strong> 结合全面备份，减少备份时间和存储空间。</li>
<li><strong>多地备份：</strong> 将备份数据存储在不同的物理位置（异地备份）以防范灾难。</li>
<li><strong>备份验证：</strong> 定期测试备份数据是否可恢复，确保备份的有效性。</li>
<li><strong>RAID&#x2F;LVM：</strong> 对于生产系统，使用 RAID (独立磁盘冗余阵列) 来提高数据冗余性和可用性。使用 LVM (逻辑卷管理) 来提供更灵活的存储管理，例如快照（Snapshot）功能，可以在执行 <code>fsck</code> 前创建文件系统的快照，以便在修复过程中出现问题时可以回滚到快照状态。</li>
</ul>
<p><strong>在修复前：</strong><br>如果条件允许，在对关键文件系统进行 <code>fsck</code> 操作前，我会尝试以下操作：</p>
<ol>
<li><strong>尽可能地复制数据：</strong> 如果文件系统还能读，尽管可能有错误，也尽量将重要数据复制到另一个健康的存储介质上。</li>
<li><strong>LVM 快照：</strong> 如果文件系统在 LVM 逻辑卷上，创建快照是最安全的方式。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lvcreate --size 1G --snapshot --name /dev/vg0/lv_data_snap /dev/vg0/lv_data</span><br><span class="line"><span class="comment"># 对快照进行 fsck</span></span><br><span class="line">fsck /dev/vg0/lv_data_snap</span><br><span class="line"><span class="comment"># 如果修复成功，移除快照</span></span><br><span class="line">lvremove /dev/vg0/lv_data_snap</span><br><span class="line"><span class="comment"># 如果修复失败或导致问题，可以从快照恢复</span></span><br><span class="line"><span class="comment"># lvconvert --merge /dev/vg0/lv_data_snap</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<p><strong>总结：</strong></p>
<p>检查和修复 Linux 文件系统错误是运维工作中避免更大损失的关键技能。核心流程是：<strong>识别问题分区 -&gt; 确保分区未挂载 -&gt; 运行 <code>fsck</code> (或 <code>xfs_repair</code>) -&gt; 重新挂载 -&gt; 验证。</strong> 并且，始终牢记在所有操作之前，<strong>数据安全是第一位的，备份是最好的保障。</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Linux%20%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Linux%20%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3/" class="post-title-link" itemprop="url">Linux 网络接口</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:47:05" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Manage/" itemprop="url" rel="index"><span itemprop="name">Manage</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Linux-系统中的网络接"><a href="#Linux-系统中的网络接" class="headerlink" title="Linux 系统中的网络接"></a>Linux 系统中的网络接</h3><hr>
<h3 id="一、Linux-网络接口配置概述"><a href="#一、Linux-网络接口配置概述" class="headerlink" title="一、Linux 网络接口配置概述"></a>一、Linux 网络接口配置概述</h3><p>在 Linux 中，网络接口的配置主要涉及到以下几个方面：</p>
<ol>
<li><strong>接口命名：</strong> 识别网络设备的名称（例如 <code>eth0</code>, <code>enp0s3</code>, <code>ens33</code> 等）。</li>
<li><strong>IP 地址配置：</strong> 设置静态或动态（DHCP）IP 地址、子网掩码、广播地址。</li>
<li><strong>网关配置：</strong> 设置默认路由，使系统能够访问外部网络。</li>
<li><strong>DNS 配置：</strong> 设置域名解析服务器。</li>
<li><strong>开&#x2F;关接口：</strong> 启用或禁用网络接口。</li>
<li><strong>持久化配置：</strong> 使网络配置在系统重启后依然有效。</li>
</ol>
<p>历史上，Linux 有几种不同的网络配置工具和方法，但目前主流的发行版倾向于使用 <code>ip</code> 命令和系统网络管理工具（如 <code>netplan</code>, <code>NetworkManager</code> 等）。</p>
<hr>
<h3 id="二、查看网络接口信息"><a href="#二、查看网络接口信息" class="headerlink" title="二、查看网络接口信息"></a>二、查看网络接口信息</h3><p>在配置之前，首先要了解当前网络接口的状态。</p>
<h4 id="1-ip-addr-show-或-ip-a：查看网络接口的-IP-地址和状态"><a href="#1-ip-addr-show-或-ip-a：查看网络接口的-IP-地址和状态" class="headerlink" title="1. ip addr show 或 ip a：查看网络接口的 IP 地址和状态"></a>1. <code>ip addr show</code> 或 <code>ip a</code>：查看网络接口的 IP 地址和状态</h4><p>这是现代 Linux 系统中查看网络接口信息的首选命令。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip addr show</span><br><span class="line"><span class="comment"># 或者简写</span></span><br><span class="line">ip a</span><br></pre></td></tr></table></figure>

<p><strong>示例输出：</strong></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">1</span>: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="number">65536</span> qdisc noqueue state UNKNOWN group default qlen <span class="number">1000</span></span><br><span class="line">    <span class="attribute">link</span>/loopback <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> brd <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">    <span class="attribute">inet</span> <span class="number">127.0.0.1</span>/<span class="number">8</span> scope host lo</span><br><span class="line">       <span class="attribute">valid_lft</span> forever preferred_lft forever</span><br><span class="line">    <span class="attribute">inet6</span> ::<span class="number">1</span>/<span class="number">128</span> scope host</span><br><span class="line">       <span class="attribute">valid_lft</span> forever preferred_lft forever</span><br><span class="line"><span class="attribute">2</span>: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="number">1500</span> qdisc fq_codel state UP group default qlen <span class="number">1000</span></span><br><span class="line">    <span class="attribute">link</span>/ether <span class="number">08</span>:<span class="number">00</span>:<span class="number">27</span>:<span class="number">1</span>c:e1:<span class="number">98</span> brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    <span class="attribute">inet</span> <span class="number">192.168.1.100</span>/<span class="number">24</span> brd <span class="number">192.168.1.255</span> scope global dynamic enp0s3</span><br><span class="line">       <span class="attribute">valid_lft</span> <span class="number">86256</span>sec preferred_lft <span class="number">86256</span>sec</span><br><span class="line">    <span class="attribute">inet6</span> fe80::a00:<span class="number">27</span>ff:fe1c:e198/<span class="number">64</span> scope link</span><br><span class="line">       <span class="attribute">valid_lft</span> forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>lo</code></strong>: 回环接口，用于本机通信。</li>
<li><strong><code>enp0s3</code></strong>: 物理网卡接口（名称可能因为内核规则 <code>udev</code> 而不同，例如 <code>eth0</code>, <code>ens33</code> 等）。</li>
<li><strong><code>&lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;</code></strong>: 接口状态，<code>UP</code> 表示接口已启用。</li>
<li><strong><code>mtu</code></strong>: 最大传输单元。</li>
<li><strong><code>link/ether</code></strong>: 网卡的 MAC 地址。</li>
<li><strong><code>inet</code></strong>: IPv4 地址信息。<code>192.168.1.100/24</code> 表示 IP 地址和 CIDR 格式的子网掩码（&#x2F;24 等同于 255.255.255.0）。<code>dynamic</code> 表示 IP 是通过 DHCP 获取的。</li>
<li><strong><code>inet6</code></strong>: IPv6 地址信息。</li>
</ul>
<h4 id="2-ip-route-show-或-ip-r：查看路由表"><a href="#2-ip-route-show-或-ip-r：查看路由表" class="headerlink" title="2. ip route show 或 ip r：查看路由表"></a>2. <code>ip route show</code> 或 <code>ip r</code>：查看路由表</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip route show</span><br><span class="line"><span class="comment"># 或者简写</span></span><br><span class="line">ip r</span><br></pre></td></tr></table></figure>

<p><strong>示例输出：</strong></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">default</span> via <span class="number">192.168.1.1</span> dev enp0s3 proto dhcp metric <span class="number">100</span></span><br><span class="line"><span class="attribute">192</span>.<span class="number">168</span>.<span class="number">1</span>.<span class="number">0</span>/<span class="number">24</span> dev enp0s3 proto kernel scope link src <span class="number">192.168.1.100</span> metric <span class="number">100</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>default via 192.168.1.1 dev enp0s3</code></strong>: 默认网关是 <code>192.168.1.1</code>，通过 <code>enp0s3</code> 接口。</li>
</ul>
<h4 id="3-cat-etc-resolv-conf：查看-DNS-服务器配置"><a href="#3-cat-etc-resolv-conf：查看-DNS-服务器配置" class="headerlink" title="3. cat /etc/resolv.conf：查看 DNS 服务器配置"></a>3. <code>cat /etc/resolv.conf</code>：查看 DNS 服务器配置</h4><p>该文件记录了系统用于域名解析的 DNS 服务器地址。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/resolv.conf</span><br></pre></td></tr></table></figure>

<p><strong>示例输出：</strong></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generated by NetworkManager</span></span><br><span class="line"><span class="attribute">search</span> localdomain</span><br><span class="line">nameserver <span class="number">192.168.1.1</span></span><br><span class="line">nameserver <span class="number">8.8.8.8</span></span><br></pre></td></tr></table></figure>

<hr>
<h3 id="三、临时配置网络接口-使用-ip-命令"><a href="#三、临时配置网络接口-使用-ip-命令" class="headerlink" title="三、临时配置网络接口 (使用 ip 命令)"></a>三、临时配置网络接口 (使用 <code>ip</code> 命令)</h3><p>这些配置在系统重启后会失效，主要用于测试或临时修改。需要 <code>sudo</code> 权限。</p>
<h4 id="1-启用-禁用网络接口"><a href="#1-启用-禁用网络接口" class="headerlink" title="1. 启用&#x2F;禁用网络接口"></a>1. 启用&#x2F;禁用网络接口</h4><ul>
<li><strong>启用接口：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip <span class="built_in">link</span> <span class="built_in">set</span> dev enp0s3 up</span><br></pre></td></tr></table></figure></li>
<li><strong>禁用接口：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip <span class="built_in">link</span> <span class="built_in">set</span> dev enp0s3 down</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-配置静态-IP-地址"><a href="#2-配置静态-IP-地址" class="headerlink" title="2. 配置静态 IP 地址"></a>2. 配置静态 IP 地址</h4><p>假设接口名为 <code>enp0s3</code>，IP 地址为 <code>192.168.1.100</code>，子网掩码为 <code>255.255.255.0</code> (即 <code>/24</code>)。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip addr add 192.168.1.100/24 dev enp0s3</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line"><span class="built_in">sudo</span> ip addr add 192.168.1.100/255.255.255.0 dev enp0s3</span><br></pre></td></tr></table></figure>

<h4 id="3-删除-IP-地址"><a href="#3-删除-IP-地址" class="headerlink" title="3. 删除 IP 地址"></a>3. 删除 IP 地址</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip addr del 192.168.1.100/24 dev enp0s3</span><br></pre></td></tr></table></figure>

<h4 id="4-配置默认网关"><a href="#4-配置默认网关" class="headerlink" title="4. 配置默认网关"></a>4. 配置默认网关</h4><p>假设网关 IP 为 <code>192.168.1.1</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip route add default via 192.168.1.1 dev enp0s3</span><br></pre></td></tr></table></figure>

<h4 id="5-删除默认网关"><a href="#5-删除默认网关" class="headerlink" title="5. 删除默认网关"></a>5. 删除默认网关</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ip route del default via 192.168.1.1</span><br></pre></td></tr></table></figure>

<h4 id="6-配置-DNS-服务器"><a href="#6-配置-DNS-服务器" class="headerlink" title="6. 配置 DNS 服务器"></a>6. 配置 DNS 服务器</h4><p>DNS 服务器配置通常通过修改 <code>/etc/resolv.conf</code> 文件实现。但<strong>不建议直接手动编辑此文件</strong>，因为它可能被 <code>NetworkManager</code> 或 <code>systemd-resolved</code> 等服务覆盖。临时修改时可以：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;nameserver 8.8.8.8&quot;</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/resolv.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;nameserver 8.8.4.4&quot;</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> -a /etc/resolv.conf <span class="comment"># -a 追加</span></span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong> 如果系统使用了 <code>NetworkManager</code> 或 <code>systemd-resolved</code>，直接修改 <code>/etc/resolv.conf</code> 可能会在重启后或网络服务重启后失效。推荐通过这些服务的配置接口进行 DNS 配置以实现持久化。</p>
<hr>
<h3 id="四、持久化配置网络接口"><a href="#四、持久化配置网络接口" class="headerlink" title="四、持久化配置网络接口"></a>四、持久化配置网络接口</h3><p>持久化配置是生产环境中的标准做法，确保系统重启后网络设置依然有效。不同的 Linux 发行版有不同的网络管理工具。</p>
<h4 id="1-Debian-Ubuntu-系列-etc-network-interfaces"><a href="#1-Debian-Ubuntu-系列-etc-network-interfaces" class="headerlink" title="1. Debian&#x2F;Ubuntu 系列 (/etc/network/interfaces)"></a>1. Debian&#x2F;Ubuntu 系列 (<code>/etc/network/interfaces</code>)</h4><p>这是传统的 Debian&#x2F;Ubuntu 网络配置方式，适用于没有 <code>NetworkManager</code> 的服务器环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/network/interfaces</span><br></pre></td></tr></table></figure>

<p><strong>示例（静态 IP 配置）：</strong></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">source</span> /etc/network/interfaces.d/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line"></span><br><span class="line"><span class="comment"># The primary network interface (enp0s3)</span></span><br><span class="line">auto enp0s3             <span class="comment"># 系统启动时自动激活接口</span></span><br><span class="line">iface enp0s3 inet static <span class="comment"># 接口配置为静态IP</span></span><br><span class="line"></span><br><span class="line">    address <span class="number">192.168.1.100</span>   <span class="comment"># IP地址</span></span><br><span class="line">    netmask <span class="number">255.255.255.0</span>   <span class="comment"># 子网掩码</span></span><br><span class="line">    gateway <span class="number">192.168.1.1</span>     <span class="comment"># 默认网关</span></span><br><span class="line">    dns-nameservers <span class="number">8.8.8.8</span> <span class="number">8.8.4.4</span> <span class="comment"># DNS服务器</span></span><br></pre></td></tr></table></figure>

<p><strong>示例（DHCP 配置）：</strong></p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># The primary network <span class="keyword">interface</span> (<span class="symbol">enp0s3</span>)</span><br><span class="line"><span class="symbol">auto</span> <span class="symbol">enp0s3</span></span><br><span class="line"><span class="symbol">iface</span> <span class="symbol">enp0s3</span> <span class="symbol">inet</span> <span class="symbol">dhcp</span></span><br></pre></td></tr></table></figure>

<p><strong>应用配置：</strong></p>
<ul>
<li>重启网络服务（推荐，但可能导致短暂断网）：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart networking</span><br></pre></td></tr></table></figure></li>
<li>或者重启整个系统。</li>
<li>或使用 <code>ifdown</code> &#x2F; <code>ifup</code> 命令禁用&#x2F;启用接口：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ifdown enp0s3</span><br><span class="line"><span class="built_in">sudo</span> ifup enp0s3</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-CentOS-RHEL-Fedora-系列-etc-sysconfig-network-scripts-ifcfg"><a href="#2-CentOS-RHEL-Fedora-系列-etc-sysconfig-network-scripts-ifcfg" class="headerlink" title="2. CentOS&#x2F;RHEL&#x2F;Fedora 系列 (/etc/sysconfig/network-scripts/ifcfg-&lt;interface&gt;)"></a>2. CentOS&#x2F;RHEL&#x2F;Fedora 系列 (<code>/etc/sysconfig/network-scripts/ifcfg-&lt;interface&gt;</code>)</h4><p>这是 Red Hat 系列发行版常用的配置方式。每个接口对应一个 <code>ifcfg-</code> 文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/sysconfig/network-scripts/ifcfg-enp0s3</span><br></pre></td></tr></table></figure>

<p><strong>示例（静态 IP 配置）：</strong></p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">TYPE</span>=<span class="string">&quot;Ethernet&quot;</span></span><br><span class="line"><span class="attr">PROXY_METHOD</span>=<span class="string">&quot;none&quot;</span></span><br><span class="line"><span class="attr">BROWSER_ONLY</span>=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="attr">BOOTPROTO</span>=<span class="string">&quot;static&quot;</span>          <span class="comment"># 启动协议：none/static (静态), dhcp (动态)</span></span><br><span class="line"><span class="attr">DEFROUTE</span>=<span class="string">&quot;yes&quot;</span></span><br><span class="line"><span class="attr">IPV4_FAILURE_FATAL</span>=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="attr">IPV6INIT</span>=<span class="string">&quot;yes&quot;</span></span><br><span class="line"><span class="attr">IPV6_AUTOCONF</span>=<span class="string">&quot;yes&quot;</span></span><br><span class="line"><span class="attr">IPV6_DEFROUTE</span>=<span class="string">&quot;yes&quot;</span></span><br><span class="line"><span class="attr">IPV6_FAILURE_FATAL</span>=<span class="string">&quot;no&quot;</span></span><br><span class="line"><span class="attr">IPV6_ADDR_GEN_MODE</span>=<span class="string">&quot;stable-privacy&quot;</span></span><br><span class="line"><span class="attr">NAME</span>=<span class="string">&quot;enp0s3&quot;</span></span><br><span class="line"><span class="attr">UUID</span>=<span class="string">&quot;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;</span> <span class="comment"># 每个接口的唯一标识符</span></span><br><span class="line"><span class="attr">DEVICE</span>=<span class="string">&quot;enp0s3&quot;</span>             <span class="comment"># 设备名称</span></span><br><span class="line"><span class="attr">ONBOOT</span>=<span class="string">&quot;yes&quot;</span>                <span class="comment"># 系统启动时是否激活接口</span></span><br><span class="line"><span class="attr">IPADDR</span>=<span class="string">&quot;192.168.1.100&quot;</span>      <span class="comment"># IP 地址</span></span><br><span class="line"><span class="attr">PREFIX</span>=<span class="string">&quot;24&quot;</span>                 <span class="comment"># CIDR 格式的子网掩码</span></span><br><span class="line"><span class="attr">GATEWAY</span>=<span class="string">&quot;192.168.1.1&quot;</span>       <span class="comment"># 默认网关</span></span><br><span class="line"><span class="attr">DNS1</span>=<span class="string">&quot;8.8.8.8&quot;</span>              <span class="comment"># DNS 服务器1</span></span><br><span class="line"><span class="attr">DNS2</span>=<span class="string">&quot;8.8.4.4&quot;</span>              <span class="comment"># DNS 服务器2</span></span><br></pre></td></tr></table></figure>

<p><strong>示例（DHCP 配置）：</strong></p>
<figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">TYPE</span><span class="operator">=</span><span class="string">&quot;Ethernet&quot;</span></span><br><span class="line"><span class="attribute">BOOTPROTO</span><span class="operator">=</span><span class="string">&quot;dhcp&quot;</span></span><br><span class="line"><span class="attribute">NAME</span><span class="operator">=</span><span class="string">&quot;enp0s3&quot;</span></span><br><span class="line"><span class="attribute">DEVICE</span><span class="operator">=</span><span class="string">&quot;enp0s3&quot;</span></span><br><span class="line"><span class="attribute">ONBOOT</span><span class="operator">=</span><span class="string">&quot;yes&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>应用配置：</strong></p>
<ul>
<li>重启网络服务：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl restart network</span><br></pre></td></tr></table></figure></li>
<li>或者重启整个系统。</li>
<li>或使用 <code>ifdown</code> &#x2F; <code>ifup</code>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> ifdown enp0s3</span><br><span class="line"><span class="built_in">sudo</span> ifup enp0s3</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="3-Ubuntu-Server-18-04-Netplan-etc-netplan-yaml"><a href="#3-Ubuntu-Server-18-04-Netplan-etc-netplan-yaml" class="headerlink" title="3. Ubuntu Server 18.04+ &#x2F; Netplan (/etc/netplan/*.yaml)"></a>3. Ubuntu Server 18.04+ &#x2F; Netplan (<code>/etc/netplan/*.yaml</code>)</h4><p>Netplan 是 Ubuntu Server 18.04 及更高版本默认的网络配置抽象层，使用 YAML 格式配置文件，后台可以是 <code>networkd</code> 或 <code>NetworkManager</code>。它旨在简化复杂的网络配置。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> vim /etc/netplan/01-netcfg.yaml <span class="comment"># 文件名可能不同</span></span><br></pre></td></tr></table></figure>

<p><strong>示例（静态 IP 配置）：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">version:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">renderer:</span> <span class="string">networkd</span> <span class="comment"># 或 NetworkManager，取决于你的需求和桌面环境</span></span><br><span class="line">  <span class="attr">ethernets:</span></span><br><span class="line">    <span class="attr">enp0s3:</span>          <span class="comment"># 接口名称</span></span><br><span class="line">      <span class="attr">dhcp4:</span> <span class="literal">no</span>      <span class="comment"># 禁用IPv4 DHCP</span></span><br><span class="line">      <span class="attr">addresses:</span> [<span class="number">192.168</span><span class="number">.1</span><span class="number">.100</span><span class="string">/24</span>] <span class="comment"># IP地址和子网掩码</span></span><br><span class="line">      <span class="attr">routes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">to:</span> <span class="string">default</span></span><br><span class="line">          <span class="attr">via:</span> <span class="number">192.168</span><span class="number">.1</span><span class="number">.1</span> <span class="comment"># 默认网关</span></span><br><span class="line">      <span class="attr">nameservers:</span></span><br><span class="line">        <span class="attr">addresses:</span> [<span class="number">8.8</span><span class="number">.8</span><span class="number">.8</span>, <span class="number">8.8</span><span class="number">.4</span><span class="number">.4</span>] <span class="comment"># DNS服务器</span></span><br></pre></td></tr></table></figure>

<p><strong>示例（DHCP 配置）：</strong></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">network:</span></span><br><span class="line">  <span class="attr">version:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">renderer:</span> <span class="string">networkd</span></span><br><span class="line">  <span class="attr">ethernets:</span></span><br><span class="line">    <span class="attr">enp0s3:</span></span><br><span class="line">      <span class="attr">dhcp4:</span> <span class="literal">yes</span> <span class="comment"># 启用IPv4 DHCP</span></span><br></pre></td></tr></table></figure>

<p><strong>应用配置：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> netplan try <span class="comment"># 尝试应用配置，提供回滚机制</span></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="built_in">sudo</span> netplan apply <span class="comment"># 立即应用配置</span></span><br></pre></td></tr></table></figure>

<h4 id="4-NetworkManager-桌面环境和一些服务器"><a href="#4-NetworkManager-桌面环境和一些服务器" class="headerlink" title="4. NetworkManager (桌面环境和一些服务器)"></a>4. NetworkManager (桌面环境和一些服务器)</h4><p><code>NetworkManager</code> 是一个动态的网络管理服务，特别适用于桌面环境和笔记本电脑，它提供了图形界面和命令行工具 <code>nmcli</code> 来管理网络连接。</p>
<ul>
<li><strong>查看连接：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nmcli connection show</span><br></pre></td></tr></table></figure></li>
<li><strong>创建新的静态连接：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nmcli connection add <span class="built_in">type</span> ethernet con-name my-static-conn ifname enp0s3 ip4 192.168.1.100/24 gw4 192.168.1.1</span><br><span class="line"><span class="built_in">sudo</span> nmcli connection modify my-static-conn ipv4.dns <span class="string">&quot;8.8.8.8 8.8.4.4&quot;</span></span><br><span class="line"><span class="built_in">sudo</span> nmcli connection up my-static-conn</span><br></pre></td></tr></table></figure></li>
<li><strong>创建新的 DHCP 连接：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nmcli connection add <span class="built_in">type</span> ethernet con-name my-dhcp-conn ifname enp0s3 autoconnect <span class="built_in">yes</span></span><br><span class="line"><span class="built_in">sudo</span> nmcli connection up my-dhcp-conn</span><br></pre></td></tr></table></figure></li>
<li><strong>启用&#x2F;禁用连接：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nmcli connection up my-connection</span><br><span class="line"><span class="built_in">sudo</span> nmcli connection down my-connection</span><br></pre></td></tr></table></figure></li>
<li><strong>删除连接：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nmcli connection delete my-connection</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h3 id="五、配置最佳实践"><a href="#五、配置最佳实践" class="headerlink" title="五、配置最佳实践"></a>五、配置最佳实践</h3><ol>
<li><strong>确定接口名：</strong> 使用 <code>ip a</code> 或 <code>lsblk -f</code> 确定物理网卡的接口名。注意，传统的 <code>eth0, eth1</code> 命名现在可能被 <code>enpYsX</code> 或 <code>ensX</code> 等预测式网络接口名称取代。</li>
<li><strong>备份配置文件：</strong> 在修改任何网络配置文件之前，务必备份原文件。<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> /etc/network/interfaces /etc/network/interfaces.bak</span><br></pre></td></tr></table></figure></li>
<li><strong>避免同时使用多个网络管理工具：</strong> 例如，不要同时配置 <code>/etc/network/interfaces</code> 和 <code>Netplan</code>，或者 <code>NetworkManager</code> 和传统配置文件，这会导致冲突和不可预测的行为。</li>
<li><strong>远程操作：</strong> 通过 SSH 远程配置网络时，务必小心。建议在一个单独的会话中保留一个活动连接，或者使用 <code>screen</code>&#x2F;<code>tmux</code>，并确保你有一个恢复计划（例如，物理访问或带外管理）。<code>netplan try</code> 命令提供了一个回滚机制，在远程配置时非常有用。</li>
<li><strong>DNS 配置：</strong> 对于静态 IP，务必配置正确的 DNS 服务器，否则无法解析域名。</li>
<li><strong>防火墙：</strong> 配置网络接口后，别忘了配置防火墙（如 <code>ufw</code>, <code>firewalld</code>, <code>iptables</code>）以确保系统安全。</li>
</ol>
<p>通过掌握上述命令和配置方法，您可以有效地管理 Linux 服务器的网络接口。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Linux%20%E7%BD%91%E7%BB%9C%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4%E6%80%9D%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Linux%20%E7%BD%91%E7%BB%9C%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4%E6%80%9D%E8%B7%AF/" class="post-title-link" itemprop="url">Linux 网络故障排除思路</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:47:01" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Manage/" itemprop="url" rel="index"><span itemprop="name">Manage</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>好的，作为一名资深运维工程师，当面对“服务无法访问，但服务器可以 Ping 通，且本地服务似乎运行正常”这类棘手的网络故障时，仅仅依赖 <code>netstat</code> 和简单的防火墙检查是不够的。我们需要更深入、更精细的工具来逐层剥离复杂性，找出问题的根源。这里我将扩展之前的故障排查，整合更多强大的工具，形成一个更全面的排除流程。</p>
<hr>
<h3 id="故障场景回顾与排查目标"><a href="#故障场景回顾与排查目标" class="headerlink" title="故障场景回顾与排查目标"></a>故障场景回顾与排查目标</h3><ul>
<li><strong>现象：</strong> Web 服务（如 Nginx 监听 80 端口）从外部无法访问，但服务器能 Ping 通，服务进程显示运行正常，且本地 <code>curl http://localhost:80</code> 正常。</li>
<li><strong>排查目标：</strong> 找出流量在哪里被阻挡了。是从客户端发出去就错了？还是在网络传输中丢失？还是在服务器端被防火墙拦截？或者服务进程本身的问题？</li>
</ul>
<hr>
<h3 id="全面的多工具排查步骤"><a href="#全面的多工具排查步骤" class="headerlink" title="全面的多工具排查步骤"></a>全面的多工具排查步骤</h3><h4 id="阶段-1-客户端测试与初步确认"><a href="#阶段-1-客户端测试与初步确认" class="headerlink" title="阶段 1: 客户端测试与初步确认"></a><strong>阶段 1: 客户端测试与初步确认</strong></h4><ol>
<li><p><strong><code>ping &lt;server_ip&gt;</code></strong></p>
<ul>
<li><strong>目的：</strong> 确认基本的网络连通性（二层&#x2F;三层）。</li>
<li><strong>结果：</strong> 确认服务器可达。</li>
<li><strong>备注：</strong> 如果 Ping 不通，问题可能在更底层：客户端网络配置、服务器网卡问题、路由器故障、防火墙完全禁用 ICMP 等。此时需要从网络拓扑的物理层到数据链路层开始排查。</li>
</ul>
</li>
<li><p><strong><code>telnet &lt;server_ip&gt; &lt;port&gt;</code> 或 <code>nc -vz &lt;server_ip&gt; &lt;port&gt;</code> (客户端)</strong></p>
<ul>
<li><strong>目的：</strong> 尝试建立到服务器目标端口的 TCP 连接。这是判断端口是否开放的最直接方式。</li>
<li><strong><code>telnet</code> 结果：</strong><ul>
<li><code>Connected to &lt;server_ip&gt;.</code>: 连接成功，端口开放。</li>
<li><code>Connection refused</code>: 服务器拒绝连接。通常是服务没运行，或者服务监听在127.0.0.1，或者防火墙返回了 RST 包。</li>
<li><code>Connection timed out</code>: 服务器没有响应。通常是防火墙拦截了（静默丢弃），或者网络路径中断。</li>
</ul>
</li>
<li><strong><code>nc -vz</code> 结果：</strong><ul>
<li><code>&lt;server_ip&gt; (&lt;server_ip&gt;): connection succeeded</code>: 连接成功。</li>
<li><code>&lt;server_ip&gt; (&lt;server_ip&gt;) 80 (http) : Connection refused</code>: 连接被拒绝。</li>
<li><code>nc: connect to &lt;server_ip&gt; port 80 (tcp) failed: Connection timed out</code>: 连接超时。</li>
</ul>
</li>
<li><strong>结论：</strong><ul>
<li>如果本地 <code>telnet/nc</code> 都成功，但远程失败，则强烈指示网络路径中有防火墙或安全组阻挡。</li>
<li>如果是 <code>Connection refused</code>，说明对方服务器收到了你的请求，但主动拒绝了。这可能表示服务未运行，或运行在其他端口，或仅监听本地。</li>
<li>如果是 <code>Connection timed out</code>，说明请求没有到达服务器，或者到达了被防火墙静默丢弃了。这是最常见的防火墙静默拦截现象。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="阶段-2-服务器端内部检查与服务状态验证"><a href="#阶段-2-服务器端内部检查与服务状态验证" class="headerlink" title="阶段 2: 服务器端内部检查与服务状态验证"></a><strong>阶段 2: 服务器端内部检查与服务状态验证</strong></h4><ol>
<li><p><strong><code>systemctl status &lt;service_name&gt;</code> &#x2F; <code>ps -ef | grep &lt;service_name&gt;</code></strong></p>
<ul>
<li><strong>目的：</strong> 确认服务进程是否正常运行。</li>
<li><strong>结果：</strong> 确认服务处于 <code>running</code> 状态。</li>
</ul>
</li>
<li><p><strong><code>netstat -tulnp | grep &lt;port&gt;</code> 或 <code>ss -tulnp | grep &lt;port&gt;</code></strong></p>
<ul>
<li><strong>目的：</strong> 确认服务是否在正确的端口上监听，且监听在 <code>0.0.0.0</code> (所有接口) 而非 <code>127.0.0.1</code> (本地回环)。</li>
<li><strong>结果：</strong> 期望看到 <code>0.0.0.0:&lt;port&gt;</code> 处于 <code>LISTEN</code> 状态。</li>
<li><strong>如果监听在 <code>127.0.0.1</code>：</strong> 这是服务配置问题。需要修改服务配置文件 (<code>nginx.conf</code>, <code>apache2.conf</code>, <code>server.xml</code> 等)，将其从 <code>127.0.0.1:&lt;port&gt;</code> 改为 <code>0.0.0.0:&lt;port&gt;</code> 或不指定 IP。然后重启服务。</li>
</ul>
</li>
<li><p><strong><code>curl http://localhost:&lt;port&gt;</code> 或 <code>curl http://127.0.0.1:&lt;port&gt;</code> (服务器本地)</strong></p>
<ul>
<li><strong>目的：</strong> 确认服务在本地是可访问的，排除服务应用层的问题。</li>
<li><strong>结果：</strong> 200 OK 或预期的响应。</li>
</ul>
</li>
</ol>
<h4 id="阶段-3-服务器端防火墙检查"><a href="#阶段-3-服务器端防火墙检查" class="headerlink" title="阶段 3: 服务器端防火墙检查"></a><strong>阶段 3: 服务器端防火墙检查</strong></h4><ol>
<li><p><strong><code>iptables -L -n -v</code> &#x2F; <code>firewall-cmd --list-all</code> &#x2F; <code>ufw status</code></strong></p>
<ul>
<li><strong>目的：</strong> 检查操作系统内部防火墙规则是否允许目标端口的入站连接。</li>
<li><strong>如何排查：</strong><ul>
<li><strong><code>iptables</code>：</strong> 检查 <code>INPUT</code> 链。注意规则顺序。通常，如果第一条是 <code>ACCEPT</code> 特定端口，后面如果是 <code>DROP</code> 或 <code>REJECT</code> 则可能没问题。但如果默认策略是 <code>DROP</code> 而没有显式 <code>ACCEPT</code> 规则，或者有针对该端口的 <code>REJECT/DROP</code> 规则靠前，那就是问题所在。</li>
<li><strong><code>firewalld</code>：</strong> 查看 <code>public</code> 或服务所在区域的配置，确认目标端口是否在 <code>ports</code> 或 <code>services</code> 列表中。</li>
<li><strong><code>ufw</code>：</strong> 查看状态列表，确认该端口是否显示为 <code>ALLOW IN</code>。</li>
</ul>
</li>
<li><strong>解决方案：</strong> 如果发现防火墙阻挡，根据使用的防火墙工具，添加相应的规则。<ul>
<li><code>firewall-cmd --permanent --add-port=80/tcp &amp;&amp; firewall-cmd --reload</code></li>
<li><code>ufw allow 80/tcp</code></li>
<li><code>iptables -I INPUT -p tcp --dport 80 -j ACCEPT</code> (临时，需要保存才能永久)</li>
</ul>
</li>
<li><strong>特别注意：</strong> 如果是 <code>REJECT</code> 动作，<code>nc/telnet</code> 会立刻返回 <code>Connection refused</code>；如果是 <code>DROP</code> 动作，<code>nc/telnet</code> 会超时。</li>
</ul>
</li>
<li><p><strong><code>setenforce 0</code> &#x2F; <code>sestatus</code> (SELinux)</strong></p>
<ul>
<li><strong>目的：</strong> 检查 SELinux 是否限制了服务的网络访问或端口绑定。虽然常见防火墙问题更普遍，但 SELinux 也可能引起类似问题。</li>
<li><strong>如何排查：</strong><ul>
<li><code>getenforce</code> 或 <code>sestatus</code> 查看状态。如果为 <code>Enforcing</code>，则启用。</li>
<li>检查 <code>audit.log</code> (<code>/var/log/audit/audit.log</code>)，寻找 <code>AVC</code> 拒绝相关的日志。</li>
</ul>
</li>
<li><strong>解决方案：</strong><ul>
<li>临时关闭或设为 Permissive 模式：<code>sudo setenforce 0</code>。如果问题解决，那基本就是 SELinux 问题。</li>
<li>长期解决方案：针对性地添加 SELinux 策略，如 <code>semanage port -a -t http_port_t -p tcp 8080</code> 或创建自定义模块。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="阶段-4-网络流量捕获与分析-数据包层面"><a href="#阶段-4-网络流量捕获与分析-数据包层面" class="headerlink" title="阶段 4: 网络流量捕获与分析 (数据包层面)"></a><strong>阶段 4: 网络流量捕获与分析 (数据包层面)</strong></h4><p>这是定位网络故障的终极武器，能够看到数据包是否真正到达，以及服务器如何响应。</p>
<ol>
<li><p><strong><code>tcpdump</code> (服务器端)</strong></p>
<ul>
<li><strong>目的：</strong> 在服务器的网络接口上捕获流量，检查客户端的请求是否到达，以及服务器的响应是否发出去。</li>
<li><strong>命令示例：</strong><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> tcpdump -i &lt;interface&gt; port &lt;port_number&gt; -nn -vv</span><br><span class="line"><span class="comment"># 示例：sudo tcpdump -i eth0 port 80 -nn -vv</span></span><br><span class="line"><span class="comment"># -i &lt;interface&gt;: 指定监听的网卡接口，如 eth0, ens33 等</span></span><br><span class="line"><span class="comment"># port &lt;port_number&gt;: 仅捕获指定端口的流量</span></span><br><span class="line"><span class="comment"># -nn: 不将IP地址解析为主机名，不将端口号解析为服务名，加快显示速度</span></span><br><span class="line"><span class="comment"># -vv: 显示详细的协议信息</span></span><br></pre></td></tr></table></figure></li>
<li><strong>如何排查：</strong><ul>
<li><strong>从客户端发起连接 (<code>nc &lt;server_ip&gt; 80</code> 或 <code>curl &lt;server_ip&gt;</code>)</strong></li>
<li><strong>观察 <code>tcpdump</code> 输出：</strong><ul>
<li><p><strong>如果没有任何输出：</strong> 客户端的请求根本没有到达服务器。问题在服务器上游的网络设备（如路由器、硬件防火墙、云安全组）或客户端的网络配置。</p>
</li>
<li><p><strong>如果只看到 <code>SYN</code> 包入站，但没有 <code>SYN-ACK</code> 出站：</strong> 服务器收到了 <code>SYN</code> 请求，但没有回应 <code>SYN-ACK</code>。这强烈指示是<strong>服务器内部的防火墙（<code>iptables</code>, <code>firewalld</code>, <code>ufw</code>）静默丢弃了数据包</strong>。这种情况 <code>nc/telnet</code> 会表现为超时。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端发 SYN</span></span><br><span class="line"><span class="variable">&lt;client_ip&gt;</span>.12345 &gt; <span class="variable">&lt;server_ip&gt;</span>.80: Flags [S], seq 1234567, win 65535, options [mss 1460,sackOK,TS val 1234567,ecr 0,nop,ws 8], length 0</span><br><span class="line"><span class="comment"># 服务器无响应</span></span><br></pre></td></tr></table></figure></li>
<li><p><strong>如果看到 <code>SYN</code> 入站，紧接着 <code>RST</code> 出站：</strong> 服务器收到了 <code>SYN</code> 请求，但主动拒绝了连接。这通常表示<strong>服务没有运行在那个端口</strong>，或者<strong>服务监听在 <code>127.0.0.1</code> 导致无法接受外部连接</strong>，或者是<strong>防火墙（如 <code>iptables</code> 使用 <code>REJECT</code> 动作）明确拒绝了连接</strong>。这种情况 <code>nc/telnet</code> 会表现为 <code>Connection refused</code>。</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 客户端发 SYN</span></span><br><span class="line"><span class="variable">&lt;client_ip&gt;</span>.12345 &gt; <span class="variable">&lt;server_ip&gt;</span>.80: Flags [S], seq 1234567, win 65535, options [mss 1460,sackOK,TS val 1234567,ecr 0,nop,ws 8], length 0</span><br><span class="line"><span class="comment"># 服务器回复 RST</span></span><br><span class="line"><span class="variable">&lt;server_ip&gt;</span>.80 &gt; <span class="variable">&lt;client_ip&gt;</span>.12345: Flags [R.], seq 0, ack 1234568, win 0, length 0</span><br></pre></td></tr></table></figure></li>
<li><p><strong>如果看到 <code>SYN</code> 入站，<code>SYN-ACK</code> 出站，然后 <code>ACK</code> 入站 (TCP三次握手完成)，但后续应用层数据不通：</strong> 这表明 TCP 连接已经建立，问题出在应用层。检查服务日志、应用层配置或代理配置等。</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;client_ip&gt;<span class="string">.12345</span> &gt; &lt;server_ip&gt;<span class="string">.80</span>: Flags [S], <span class="string">...</span> <span class="params">(SYN)</span></span><br><span class="line">&lt;server_ip&gt;<span class="string">.80</span> &gt; &lt;client_ip&gt;<span class="string">.12345</span>: Flags [S.], <span class="string">...</span> <span class="params">(SYN-ACK)</span></span><br><span class="line">&lt;client_ip&gt;<span class="string">.12345</span> &gt; &lt;server_ip&gt;<span class="string">.80</span>: Flags [.], <span class="string">...</span> <span class="params">(ACK)</span></span><br><span class="line"><span class="comment"># 之后客户端期望发送 HTTP GET 请求，但服务没有收到或没有回应</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Wireshark&#x2F;tcpdump (客户端)</strong></p>
<ul>
<li><strong>目的：</strong> 如果服务器端 <code>tcpdump</code> 没看到任何流量，那么需要回溯到客户端，在客户端捕获流量，确认请求是否已经发出。</li>
<li><strong>命令：</strong> <code>tcpdump -i &lt;interface&gt; host &lt;server_ip&gt; and port &lt;port_number&gt; -nn -vv</code> (在客户端运行)</li>
<li><strong>如何排查：</strong><ul>
<li>如果客户端 <code>tcpdump</code> 显示 <code>SYN</code> 包已发出，但服务器端未收到，这强烈暗示<strong>中间网络设备</strong>（路由器、硬件防火墙、云平台安全组）拦截了流量。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="阶段-5-云平台安全组-网络ACL检查-云服务器特有"><a href="#阶段-5-云平台安全组-网络ACL检查-云服务器特有" class="headerlink" title="阶段 5: 云平台安全组&#x2F;网络ACL检查 (云服务器特有)"></a><strong>阶段 5: 云平台安全组&#x2F;网络ACL检查 (云服务器特有)</strong></h4><ol>
<li><strong>云平台控制台</strong><ul>
<li><strong>目的：</strong> 这层防护在虚拟机实例之外，是流量到达服务器前最重要的一关。</li>
<li><strong>如何排查：</strong> 登录到云服务提供商控制台，找到你的服务器实例。<ul>
<li><strong>安全组（Security Group）：</strong> 检查其入站规则，确认目标端口是否对源 IP (<code>0.0.0.0/0</code> 或特定IP) 开放。</li>
<li><strong>网络ACL (Network ACL &#x2F; NACL)：</strong> 检查关联的NACL规则，确保入站和出站规则都允许目标端口的流量，特别是无状态的NACL，需要同时有入站和出站规则。</li>
</ul>
</li>
<li><strong>解决方案：</strong> 添加或修改规则以允许必要的入口流量。这通常是生效最快的。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="总结故障排除流程图"><a href="#总结故障排除流程图" class="headerlink" title="总结故障排除流程图"></a>总结故障排除流程图</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    subgraph &quot;初步诊断&quot;</span><br><span class="line">        A[&quot;客户端无法访问Web服务&lt;br/&gt;(例如 80端口)&quot;] --&gt; B&#123;&quot;客户端 Ping 服务器 IP？&quot;&#125;;</span><br><span class="line">        B -- &quot;NO&quot; --&gt; B_NO[&quot;排查点:&lt;br/&gt;客户端网络配置&lt;br/&gt;服务器物理网卡&lt;br/&gt;基础网络路由&lt;br/&gt;ICMP防火墙规则&quot;];</span><br><span class="line">        B -- &quot;YES (网络可达)&quot; --&gt; C&#123;&quot;客户端 nc 或 telnet 端口？&quot;&#125;;</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    subgraph &quot;服务及本地连接检查&quot;</span><br><span class="line">        C -- &quot;YES (连接成功)&quot; --&gt; C_YES[&quot;结论:&lt;br/&gt;网络和端口畅通&lt;br/&gt;问题在应用层&lt;br/&gt;下一步: 检查应用日志或配置&quot;];</span><br><span class="line">        C -- &quot;NO (Connection Refused/Timeout)&quot; --&gt; D&#123;&quot;SSH 登录服务器&lt;br/&gt;检查服务进程 systemctl status 或 ps？&quot;&#125;;</span><br><span class="line">        D -- &quot;NO (未运行)&quot; --&gt; D_NO[&quot;解决:&lt;br/&gt;启动服务并检查启动日志&quot;];</span><br><span class="line">        D -- &quot;YES (Running)&quot; --&gt; E&#123;&quot;服务器本地 netstat 检查监听？&quot;&#125;;</span><br><span class="line">        E --&gt; E1&#123;&quot;监听在 0.0.0.0 或 ::: 的端口？&quot;&#125;;</span><br><span class="line">        E1 -- &quot;NO (仅监听 127.0.0.1)&quot; --&gt; E1_NO[&quot;解决:&lt;br/&gt;修改服务配置使其监听所有接口&lt;br/&gt;然后重启服务&quot;];</span><br><span class="line">        E1 -- &quot;YES (正确监听)&quot; --&gt; E2&#123;&quot;服务器本地 curl localhost 端口是否正常？&quot;&#125;;</span><br><span class="line">        E2 -- &quot;NO (本地访问失败)&quot; --&gt; E2_NO[&quot;排查点:&lt;br/&gt;服务配置错误&lt;br/&gt;应用本身 Bug&lt;br/&gt;下一步: 深入检查服务日志&quot;];</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    subgraph &quot;深度网络包分析 (tcpdump)&quot;</span><br><span class="line">        E2 -- &quot;YES (本地访问正常)&quot; --&gt; F&#123;&quot;在服务器端 tcpdump 抓包&lt;br/&gt;同时客户端发起连接&quot;&#125;;</span><br><span class="line">        F --&gt; F1&#123;&quot;抓到客户端发来的 SYN 包？&quot;&#125;;</span><br><span class="line">        F1 -- &quot;YES (流量已到达服务器网卡)&quot; --&gt; F1_YES[&quot;✅ 流量已到，问题在服务器内部&quot;];</span><br><span class="line">        F1_YES --&gt; F2&#123;&quot;接着看到 SYN-ACK (服务器回应)？&quot;&#125;;</span><br><span class="line">        F1_YES --- F3&#123;&quot;或看到 RST (服务器拒绝)？&quot;&#125;;</span><br><span class="line">        </span><br><span class="line">        F2 -- &quot;YES (握手成功)&quot; --&gt; F2_YES[&quot;结论:&lt;br/&gt;TCP三次握手完成&lt;br/&gt;问题在更上层的应用&lt;br/&gt;下一步: 检查服务日志或应用配置&quot;];</span><br><span class="line">        F2 -- &quot;NO (有SYN无SYN-ACK)&quot; --&gt; F2_NO[&quot;原因:&lt;br/&gt;服务器防火墙静默丢弃(DROP)&lt;br/&gt;下一步: 检查 iptables 或 firewalld 的&lt;br/&gt;INPUT 链规则顺序和默认策略&quot;];</span><br><span class="line">        </span><br><span class="line">        F3 -- &quot;YES&quot; --&gt; F3_YES[&quot;原因:&lt;br/&gt;• 服务器防火墙拒绝(REJECT)&lt;br/&gt;• 服务未启动或崩溃&lt;br/&gt;• 服务监听在错误端口或IP&quot;];</span><br><span class="line"></span><br><span class="line">        F1 -- &quot;NO (流量未到达服务器网卡)&quot; --&gt; F1_NO[&quot;❌ 流量在到达服务器前被阻断&quot;];</span><br><span class="line">        F1_NO --&gt; F1_NO_sub&#123;&quot;在客户端 tcpdump：&lt;br/&gt;客户端是否发出 SYN 包？&quot;&#125;;</span><br><span class="line">        F1_NO_sub -- &quot;NO&quot; --&gt; F1_NO_sub_NO[&quot;排查点:&lt;br/&gt;客户端网络配置&lt;br/&gt;或客户端出口防火墙&quot;];</span><br><span class="line">        F1_NO_sub -- &quot;YES (客户端已发，服务器未收)&quot; --&gt; F1_NO_sub_YES[&quot;高嫌疑点:&lt;br/&gt;中间网络设备(物理防火墙)&lt;br/&gt;或 云平台安全组或网络ACL&quot;];</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    subgraph &quot;云平台与系统级检查&quot;</span><br><span class="line">        F1_NO_sub_YES --&gt; G&#123;&quot;(针对云服务器)&lt;br/&gt;检查云平台控制台&lt;br/&gt;安全组或网络ACL&quot;&#125;;</span><br><span class="line">        G --&gt; G1&#123;&quot;入站规则是否对&lt;br/&gt;客户端IP开放相应端口？&quot;&#125;;</span><br><span class="line">        G1 -- &quot;NO&quot; --&gt; G1_NO[&quot;解决:&lt;br/&gt;添加或修改相应入站规则&quot;];</span><br><span class="line">        G1 -- &quot;YES&quot; --&gt; G1_YES[&quot;下一步:&lt;br/&gt;回头再次检查系统内部防火墙&lt;br/&gt;或核对服务配置或应用日志&quot;];</span><br><span class="line">        </span><br><span class="line">        G1_YES --&gt; H&#123;&quot;(最终检查)&lt;br/&gt;检查 SELinux 或 AppArmor 状态及日志&quot;&#125;;</span><br><span class="line">        H --&gt; H_ACTION[&quot;考虑临时设置为 Permissive 模式以测试&quot;];</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    %% Styling</span><br><span class="line">    style C_YES fill:#d4edda,stroke:#155724</span><br><span class="line">    style F2_YES fill:#d4edda,stroke:#155724</span><br><span class="line">    style F1_NO fill:#f8d7da,stroke:#721c24</span><br><span class="line">    style B_NO fill:#f8d7da,stroke:#721c24</span><br><span class="line">    style D_NO fill:#fff3cd,stroke:#856404</span><br><span class="line">    style E1_NO fill:#fff3cd,stroke:#856404</span><br><span class="line">    style E2_NO fill:#fff3cd,stroke:#856404</span><br><span class="line">    style G1_NO fill:#fff3cd,stroke:#856404</span><br><span class="line">    style F1_NO_sub_YES fill:#f8d7da,stroke:#721c24</span><br></pre></td></tr></table></figure>

<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">客户端无法访问Web服务 (e.g., 80端口)</span><br><span class="line">|<span class="string"></span></span><br><span class="line"><span class="string"></span>|<span class="string">-- 1. 客户端 Ping 服务器 IP？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 继续</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 检查客户端网络/服务器网卡/基础路由/ICMP防火墙</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 2. 客户端 `nc -vz &lt;server_ip&gt; &lt;port&gt;` 或 `telnet &lt;server_ip&gt; &lt;port&gt;`？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- YES (连接成功) -&gt; 问题不在网络连接，转到应用层排查（日志、配置）</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- NO (Connection Refused/Timeout) -&gt; 继续排查</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 3. SSH 登录服务器，检查服务进程 `systemctl status / ps -ef`？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- YES (Running) -&gt; 继续</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 启动服务并检查服务日志</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 4. 服务器本地 `netstat -tulnp </span>|<span class="string"> grep &lt;port&gt;` 或 `ss -tulnp </span>|<span class="string"> grep &lt;port&gt;`？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- 监听在 `0.0.0.0:&lt;port&gt;`？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 继续</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 修改服务配置，使其监听所有接口，重启服务</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- 服务器本地 `curl http://localhost:&lt;port&gt;` 正常？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 继续</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 服务配置问题/应用本身问题，检查服务日志</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 5. 服务器端 `tcpdump -i &lt;interface&gt; port &lt;port&gt;` 抓包，同时客户端发起连接？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- 抓到 `SYN` 包 (客户端发给服务器)？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 好消息，流量已到达网卡！</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- 接着看到 `SYN-ACK` (服务器回应)？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- YES -&gt; TCP三次握手完成，问题在应用层。检查服务日志/应用配置。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- NO (收到SYN但无SYN-ACK) -&gt; 服务器防火墙静默DROP (`iptables -L` 检查 INPUT 链的规则顺序和默认策略)。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- 接着看到 `RST` (服务器拒绝)？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 服务器防火墙 REJECT 或服务未启动/未监听在该端口/监听127.0.0.1。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; **流量根本没到服务器网卡！**</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">        </span>|<span class="string">-- 检查客户端 `tcpdump`：客户端是否发出了 `SYN` 包？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">        </span>|<span class="string">    </span>|<span class="string">-- YES (客户端发出，服务器未收到) -&gt; **云平台安全组/网络ACL** 或 **中间网络设备/硬件防火墙** 阻挡了。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">        </span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 客户端可能网络配置错误，或客户端防火墙阻挡。</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 6. (针对云服务器) 检查云平台控制台安全组/网络ACL入站规则。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- 端口是否对外部IP开放？</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- YES -&gt; 回头检查系统内部防火墙或服务配置/应用日志。</span></span><br><span class="line"><span class="string"></span>|<span class="string">    </span>|<span class="string">-- NO  -&gt; 添加相应入站规则。</span></span><br><span class="line"><span class="string"></span>|</span><br><span class="line">|<span class="string">-- 7. (如果仍有问题) 检查 SELinux 状态及日志，考虑临时禁用以测试。</span></span><br></pre></td></tr></table></figure>

<p>通过这一系列工具的综合运用，特别是 <code>tcpdump</code> 在关键节点的流量分析，能够帮助我们精确地定位网络故障点，无论是发生在客户端、服务器本身、操作系统防火墙、云平台安全组，还是中间的任何网络环节。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Linux%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9B%AE%E5%BD%95%E5%B1%82%E6%AC%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Linux%E7%9A%84%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9B%AE%E5%BD%95%E5%B1%82%E6%AC%A1/" class="post-title-link" itemprop="url">Linux的文件系统目录层次</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:48:27" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/Manage/" itemprop="url" rel="index"><span itemprop="name">Manage</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Linux-文件系统层次结构"><a href="#Linux-文件系统层次结构" class="headerlink" title="Linux 文件系统层次结构"></a>Linux 文件系统层次结构</h3><hr>
<h3 id="一、Linux-文件系统层次结构-FHS-概述"><a href="#一、Linux-文件系统层次结构-FHS-概述" class="headerlink" title="一、Linux 文件系统层次结构 (FHS) 概述"></a>一、Linux 文件系统层次结构 (FHS) 概述</h3><p>Linux 的文件系统是一个单一的、统一的目录树，所有文件和目录都从根目录 <code>/</code> 开始。这与 Windows 系统中每个分区（如 C:, D:）都有自己的目录树不同。即使系统有多个硬盘或分区，它们也会被“挂载”到这个统一的目录树的某个点上，成为其中的一部分。</p>
<p>FHS 的主要目标是：</p>
<ol>
<li><strong>标准化：</strong> 确保不同 Linux 发行版的文件和目录位置保持一致，方便用户和开发人员。</li>
<li><strong>可移植性：</strong> 简化软件在不同 Linux 系统间的移植。</li>
<li><strong>层次性：</strong> 清晰地划分系统文件、程序、用户数据、可变数据等，方便管理和备份。</li>
<li><strong>清晰性：</strong> 目录名称大多具有描述性，使其用途一目了然。</li>
</ol>
<h3 id="二、重要目录及其用途示例"><a href="#二、重要目录及其用途示例" class="headerlink" title="二、重要目录及其用途示例"></a>二、重要目录及其用途示例</h3><p>下面我们将详细介绍 Linux 系统中一些重要的目录及其常见用途。</p>
<h4 id="1-根目录"><a href="#1-根目录" class="headerlink" title="1. / (根目录)"></a>1. <code>/</code> (根目录)</h4><ul>
<li><strong>用途：</strong> 整个文件系统的最顶层，所有其他目录和文件都位于其下。系统启动所需的核心文件位于此目录，或者通过其下的子目录链接。</li>
<li><strong>示例内容：</strong> 常见子目录如 <code>/bin</code>, <code>/etc</code>, <code>/home</code>, <code>/usr</code>, <code>/var</code> 等。</li>
</ul>
<h4 id="2-bin-二进制可执行文件"><a href="#2-bin-二进制可执行文件" class="headerlink" title="2. /bin (二进制可执行文件)"></a>2. <code>/bin</code> (二进制可执行文件)</h4><ul>
<li><strong>用途：</strong> 存放系统启动和运行所需的基本用户命令（二进制可执行文件）。这些命令在单用户模式或系统修复模式下也可用。</li>
<li><strong>示例内容：</strong> <code>ls</code>, <code>cp</code>, <code>mv</code>, <code>rm</code>, <code>cat</code>, <code>mkdir</code>, <code>echo</code>, <code>bash</code> 等。</li>
</ul>
<h4 id="3-sbin-系统二进制可执行文件"><a href="#3-sbin-系统二进制可执行文件" class="headerlink" title="3. /sbin (系统二进制可执行文件)"></a>3. <code>/sbin</code> (系统二进制可执行文件)</h4><ul>
<li><strong>用途：</strong> 存放系统管理员使用的基本系统管理命令（二进制可执行文件）。这些命令通常用于系统维护和管理，例如启动&#x2F;停止服务、配置网络等。</li>
<li><strong>示例内容：</strong> <code>fdisk</code>, <code>reboot</code>, <code>shutdown</code>, <code>ifconfig</code>, <code>ip</code>, <code>mount</code>, <code>fsck</code> 等。</li>
</ul>
<h4 id="4-etc-配置文件"><a href="#4-etc-配置文件" class="headerlink" title="4. /etc (配置文件)"></a>4. <code>/etc</code> (配置文件)</h4><ul>
<li><strong>用途：</strong> 存放系统及大部分应用程序的配置文件。这些文件通常是文本文件，用于控制系统行为和应用程序设置。</li>
<li><strong>示例内容：</strong> <code>/etc/passwd</code> (用户账户信息), <code>/etc/fstab</code> (文件系统挂载表), <code>/etc/hosts</code> (主机名解析), <code>/etc/resolv.conf</code> (DNS 配置), <code>/etc/ssh/sshd_config</code> (SSH 服务配置), <code>/etc/apt/sources.list</code> (软件包源配置), <code>/etc/nginx/nginx.conf</code> (Nginx 配置)等。</li>
</ul>
<h4 id="5-dev-设备文件"><a href="#5-dev-设备文件" class="headerlink" title="5. /dev (设备文件)"></a>5. <code>/dev</code> (设备文件)</h4><ul>
<li><strong>用途：</strong> 存放设备文件（不占用磁盘空间，而是代表物理或虚拟设备）。通过这些文件，用户和程序可以与硬件设备交互。</li>
<li><strong>示例内容：</strong> <code>/dev/sda</code> (第一块SCSI&#x2F;SATA硬盘), <code>/dev/sda1</code> (第一块硬盘的第一个分区), <code>/dev/null</code> (空设备，丢弃所有写入的数据), <code>/dev/zero</code> (零设备，提供无限的零), <code>/dev/tty</code>&#x2F;<code>/dev/pts/0</code> (终端设备)等。</li>
</ul>
<h4 id="6-proc-进程信息伪文件系统"><a href="#6-proc-进程信息伪文件系统" class="headerlink" title="6. /proc (进程信息伪文件系统)"></a>6. <code>/proc</code> (进程信息伪文件系统)</h4><ul>
<li><strong>用途：</strong> 一个虚拟文件系统，不存储在硬盘上，而是由内核动态生成。它提供了关于系统进程、内核信息、内存使用等运行时的信息。</li>
<li><strong>示例内容：</strong> <code>/proc/cpuinfo</code> (CPU 信息), <code>/proc/meminfo</code> (内存信息), <code>/proc/mounts</code> (已挂载的文件系统), <code>/proc/&lt;PID&gt;</code> (每个进程的详细信息目录)等。</li>
</ul>
<h4 id="7-sys-系统信息伪文件系统"><a href="#7-sys-系统信息伪文件系统" class="headerlink" title="7. /sys (系统信息伪文件系统)"></a>7. <code>/sys</code> (系统信息伪文件系统)</h4><ul>
<li><strong>用途：</strong> 另一个虚拟文件系统，自2.6版内核引入。它提供了更结构化的设备信息和内核参数，允许用户通过文件系统接口来配置内核和硬件。</li>
<li><strong>示例内容：</strong> <code>/sys/class/net/eth0/address</code> (eth0 网卡的MAC地址), <code>/sys/block/sda/size</code> (sda 设备的大小)等。</li>
</ul>
<h4 id="8-var-可变数据文件"><a href="#8-var-可变数据文件" class="headerlink" title="8. /var (可变数据文件)"></a>8. <code>/var</code> (可变数据文件)</h4><ul>
<li><strong>用途：</strong> 存放系统运行时会不断变化的文件，例如日志文件、邮件队列、临时文件、网站数据等。</li>
<li><strong>示例内容：</strong><ul>
<li><code>/var/log</code>: 系统和应用程序日志文件（如 <code>/var/log/syslog</code>, <code>/var/log/messages</code>, <code>/var/log/auth.log</code>, <code>/var/log/httpd/access_log</code> 等）。</li>
<li><code>/var/run</code> (或 <code>/run</code>): 运行时数据，如 PID 文件，系统重启后清空。</li>
<li><code>/var/tmp</code>: 较大的临时文件，可以在重启后保留。</li>
<li><code>/var/spool</code>: 打印队列、邮件队列等。</li>
<li><code>/var/www</code>: Web 服务器的默认根目录（例如 Apache 或 Nginx）。</li>
<li><code>/var/lib</code>: 系统和应用程序的状态信息，如数据库文件、包管理器的状态数据等。</li>
</ul>
</li>
</ul>
<h4 id="9-tmp-临时文件"><a href="#9-tmp-临时文件" class="headerlink" title="9. /tmp (临时文件)"></a>9. <code>/tmp</code> (临时文件)</h4><ul>
<li><strong>用途：</strong> 存放临时文件。所有用户都可以在这里创建文件，通常在系统重启时或定期被清理。</li>
<li><strong>示例内容：</strong> 临时下载文件、程序运行时生成的临时文件等。</li>
</ul>
<h4 id="10-usr-Unix-System-Resources"><a href="#10-usr-Unix-System-Resources" class="headerlink" title="10. /usr (Unix System Resources)"></a>10. <code>/usr</code> (Unix System Resources)</h4><ul>
<li><strong>用途：</strong> 存放绝大多数用户命令和应用程序、库文件、文档等。这是一个非常重要的目录，其内容相对静态（不随系统运行而变化），因此可以被多个系统共享（在只读模式下挂载）。</li>
<li><strong>示例内容：</strong><ul>
<li><code>/usr/bin</code>: 非基本用户命令（数量远多于 <code>/bin</code>）。</li>
<li><code>/usr/sbin</code>: 非基本系统管理命令。</li>
<li><code>/usr/local</code>: 本地安装的软件（通常是用户从源码编译安装的）。</li>
<li><code>/usr/lib</code>: 应用程序和程序的库文件。</li>
<li><code>/usr/share</code>: 与体系结构无关的共享数据，如文档、man 页面、图标、默认配置文件模板等。</li>
<li><code>/usr/include</code>: C&#x2F;C++ 编程的头文件。</li>
</ul>
</li>
</ul>
<h4 id="11-home-用户主目录"><a href="#11-home-用户主目录" class="headerlink" title="11. /home (用户主目录)"></a>11. <code>/home</code> (用户主目录)</h4><ul>
<li><strong>用途：</strong> 存放普通用户的主目录。每个用户的个人文件、文档、配置都会存储在其各自的主目录中（如 <code>/home/username</code>）。</li>
<li><strong>示例内容：</strong> <code>/home/user1/documents</code>, <code>/home/user2/pictures</code>, <code>/home/user1/.bashrc</code> (用户Shell配置), <code>/home/user1/.ssh</code> (SSH 密钥)等。</li>
</ul>
<h4 id="12-root-root-用户主目录"><a href="#12-root-root-用户主目录" class="headerlink" title="12. /root (root 用户主目录)"></a>12. <code>/root</code> (root 用户主目录)</h4><ul>
<li><strong>用途：</strong> 系统管理员（root 用户）的主目录，与普通用户的主目录分开存放，以保护其配置和数据。</li>
<li><strong>示例内容：</strong> root 用户的配置文件、脚本等。</li>
</ul>
<h4 id="13-opt-可选的应用程序软件包"><a href="#13-opt-可选的应用程序软件包" class="headerlink" title="13. /opt (可选的应用程序软件包)"></a>13. <code>/opt</code> (可选的应用程序软件包)</h4><ul>
<li><strong>用途：</strong> 存放第三方独立软件包和附加应用程序。通常，每个软件包会有一个独立的子目录，方便管理和卸载。</li>
<li><strong>示例内容：</strong> <code>/opt/google/chrome</code>, <code>/opt/splunk</code>, <code>/opt/jdk</code> 等。</li>
</ul>
<h4 id="14-mnt-临时文件系统挂载点"><a href="#14-mnt-临时文件系统挂载点" class="headerlink" title="14. /mnt (临时文件系统挂载点)"></a>14. <code>/mnt</code> (临时文件系统挂载点)</h4><ul>
<li><strong>用途：</strong> 临时挂载文件系统的地方。例如，挂载一个 USB 驱动器、CD-ROM、网络文件共享或另一个硬盘分区时，通常会挂载到 <code>/mnt</code> 下的某个子目录。</li>
<li><strong>示例内容：</strong> <code>/mnt/usbdisk</code>, <code>/mnt/cdrom</code>, <code>/mnt/nfs_share</code> 等。</li>
</ul>
<h4 id="15-media-可移动媒体设备挂载点"><a href="#15-media-可移动媒体设备挂载点" class="headerlink" title="15. /media (可移动媒体设备挂载点)"></a>15. <code>/media</code> (可移动媒体设备挂载点)</h4><ul>
<li><strong>用途：</strong> 由系统自动管理的，用于挂载可移动媒体设备的目录，例如 USB 闪存驱动器、CD&#x2F;DVD 驱动器、SD 卡等。</li>
<li><strong>示例内容：</strong> 当插入 USB 驱动器时，它可能自动挂载到 <code>/media/username/disk_label</code>。</li>
</ul>
<h4 id="16-boot-引导加载器文件"><a href="#16-boot-引导加载器文件" class="headerlink" title="16. /boot (引导加载器文件)"></a>16. <code>/boot</code> (引导加载器文件)</h4><ul>
<li><strong>用途：</strong> 存放引导加载器（如 GRUB, LILO）所需的文件，以及 Linux 内核镜像文件和 <code>initramfs</code>（初始 ramdisk），这些都是系统启动所必需的。</li>
<li><strong>示例内容：</strong> <code>/boot/grub</code>, <code>/boot/vmlinuz-5.15.0-86-generic</code>, <code>/boot/initrd.img-5.15.0-86-generic</code> 等。</li>
</ul>
<h4 id="17-srv-服务数据"><a href="#17-srv-服务数据" class="headerlink" title="17. /srv (服务数据)"></a>17. <code>/srv</code> (服务数据)</h4><ul>
<li><strong>用途：</strong> 存放特定服务的数据。例如，FTP 服务器的数据目录，Web 服务器的根目录（除了 <code>/var/www</code>），版本控制系统的数据等。</li>
<li><strong>示例内容：</strong> <code>/srv/ftp</code>, <code>/srv/www</code> 等。这个目录通常是空的，需要管理员根据服务需求创建子目录。</li>
</ul>
<hr>
<h3 id="三、FHS-的重要性和运维实践"><a href="#三、FHS-的重要性和运维实践" class="headerlink" title="三、FHS 的重要性和运维实践"></a>三、FHS 的重要性和运维实践</h3><ul>
<li><strong>故障排查：</strong> 当系统出现问题时，FHS 指引你快速定位相关的日志文件（<code>/var/log</code>）、配置文件（<code>/etc</code>）或核心程序（<code>/usr/bin</code>, <code>/sbin</code>）。</li>
<li><strong>安全：</strong> 将静态文件和可变数据分开，更易于实现只读挂载，提高安全性。例如，<code>/usr</code> 可以挂载为只读，而 <code>/var</code> 必须是可写的。</li>
<li><strong>备份与恢复：</strong> 根据 FHS，可以更容易地规划备份策略。例如，只备份用户数据 <code>/home</code> 和重要的配置文件 <code>/etc</code> 及 <code>/var/log</code> 等，而系统程序文件可以在系统重装后重新安装。</li>
<li><strong>多盘管理：</strong> 如果系统有多个硬盘，可以将例如 <code>/home</code>, <code>/var</code>, <code>/opt</code> 等目录单独挂载到不同的分区或硬盘上，以优化性能、隔离数据或在系统盘空间不足时进行扩容。</li>
<li><strong>标准化部署：</strong> 无论是手动部署还是自动化脚本，遵循 FHS 能够确保应用程序和服务的安装位置符合预期，便于管理和维护。</li>
</ul>
<p>理解并遵循 FHS 是每一位 Linux 运维工程师的基本功，它能帮助你更好地管理和维护 Linux 系统。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Local%20%E5%AD%98%E5%82%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Local%20%E5%AD%98%E5%82%A8/" class="post-title-link" itemprop="url">Local 存储</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:48:36" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">存储</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Local-存储"><a href="#Local-存储" class="headerlink" title="Local 存储"></a>Local 存储</h1><p>前面我们有通过 <code>hostPath</code> 或者 <code>emptyDir</code> 的方式来持久化我们的数据，但是显然我们还需要更加可靠的存储来保存应用的持久化数据，这样容器在重建后，依然可以使用之前的数据。但是存储资源和 CPU 资源以及内存资源有很大不同，为了屏蔽底层的技术实现细节，让用户更加方便的使用，Kubernetes 便引入了 <code>PV</code> 和 <code>PVC</code> 两个重要的资源对象来实现对存储的管理。</p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><code>PV</code> 的全称是：<code>PersistentVolume</code>（持久化卷），是对底层共享存储的一种抽象，PV 由管理员进行创建和配置，它和具体的底层的共享存储技术的实现方式有关，比如 <code>Ceph</code>、<code>GlusterFS</code>、<code>NFS</code>、<code>hostPath</code> 等，都是通过插件机制完成与共享存储的对接。</p>
<p><code>PVC</code> 的全称是：<code>PersistentVolumeClaim</code>（持久化卷声明），PVC 是用户存储的一种声明，PVC 和 Pod 比较类似，Pod 消耗的是节点，PVC 消耗的是 PV 资源，Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。</p>
<p>但是通过 PVC 请求到一定的存储空间也很有可能不足以满足应用对于存储设备的各种需求，而且不同的应用程序对于存储性能的要求可能也不尽相同，比如读写速度、并发性能等，为了解决这一问题，Kubernetes 又为我们引入了一个新的资源对象：<code>StorageClass</code>，通过 <code>StorageClass</code> 的定义，管理员可以将存储资源定义为某种类型的资源，比如快速存储、慢速存储等，用户根据 StorageClass 的描述就可以非常直观的知道各种存储资源的具体特性了，这样就可以根据应用的特性去申请合适的存储资源了，此外 <code>StorageClass</code> 还可以为我们自动生成 PV，免去了每次手动创建的麻烦。</p>
<h2 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h2><p>我们上面提到了 PV 是对底层存储技术的一种抽象，PV 一般都是由管理员来创建和配置的，我们首先来创建一个 <code>hostPath</code> 类型的 <code>PersistentVolume</code>。Kubernetes 支持 hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟附带网络的存储，但是需要注意的是在生产集群中，我们不会使用 hostPath，集群管理员会提供网络存储资源，比如 NFS 共享卷或 Ceph 存储卷，集群管理员还可以使用 <code>StorageClasses</code> 来设置动态提供存储。因为 Pod 并不是始终固定在某个节点上面的，所以要使用 hostPath 的话我们就需要将 Pod 固定在某个节点上，这样显然就大大降低了应用的容错性。</p>
<p>比如我们这里将测试的应用固定在节点 node1 上面，首先在该节点上面创建一个 <code>/data/k8s/test/hostpath</code> 的目录，然后在该目录中创建一个 <code>index.html</code> 的文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;Hello from Kubernetes hostpath storage&#x27; &gt; /data/k8s/test/hostpath/index.html</span><br></pre></td></tr></table></figure>



<p>然后接下来创建一个 hostPath 类型的 PV 资源对象：（pv-hostpath.yaml）</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-hostpath</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">&#x27;/data/k8s/test/hostpath&#x27;</span></span><br></pre></td></tr></table></figure>



<p>配置文件中指定了该卷位于集群节点上的 <code>/data/k8s/test/hostpath</code> 目录，还指定了 10G 大小的空间和 <code>ReadWriteOnce</code> 的访问模式，这意味着该卷可以在单个节点上以读写方式挂载，另外还定义了名称为 <code>manual</code> 的 <code>StorageClass</code>，该名称用来将 <code>PersistentVolumeClaim</code> 请求绑定到该 <code>PersistentVolum</code>。下面是关于 PV 的这些配置属性的一些说明：</p>
<ul>
<li><p>Capacity（存储能力）：一般来说，一个 PV 对象都要指定一个存储能力，通过 PV 的 <code>capacity</code> 属性来设置的，目前只支持存储空间的设置，就是我们这里的 <code>storage=10Gi</code>，不过未来可能会加入 <code>IOPS</code>、吞吐量等指标的配置。</p>
</li>
<li><p>AccessModes（访问模式）：用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：</p>
<ul>
<li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载</li>
<li>ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载</li>
<li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载</li>
</ul>
<p>注意</p>
<p>一些 PV 可能支持多种访问模式，但是在挂载的时候只能使用一种访问模式，多种访问模式是不会生效的。</p>
<p>下图是一些常用的 Volume 插件支持的访问模式： <img data-src="https://mudutestmenu.mudu.tv/upload/l1mhvl.jpg" alt="pv access modes"></p>
</li>
</ul>
<p>直接创建上面的资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f pv-hostpath.yaml</span><br><span class="line">persistentvolume/pv-hostpath created</span><br></pre></td></tr></table></figure>



<p>创建完成后查看 PersistentVolume 的信息，输出结果显示该 <code>PersistentVolume</code> 的状态（STATUS） 为 <code>Available</code>。 这意味着它还没有被绑定给 <code>PersistentVolumeClaim</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pv pv-hostpath</span><br><span class="line">NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">pv-hostpath   10Gi       RWO            Retain           Available           manual                  58s</span><br></pre></td></tr></table></figure>



<p>其中有一项 <code>RECLAIM POLICY</code> 的配置，同样我们可以通过 PV 的 <code>persistentVolumeReclaimPolicy</code>（回收策略）属性来进行配置，目前 PV 支持的策略有三种：</p>
<ul>
<li>Retain（保留）：保留数据，需要管理员手工清理数据</li>
<li>Recycle（回收）：清除 PV 中的数据，效果相当于执行 <code>rm -rf /thevoluem/*</code></li>
<li>Delete（删除）：与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务，比如 ASW EBS。</li>
</ul>
<p>不过需要注意的是，目前只有 <code>NFS</code> 和 <code>HostPath</code> 两种类型支持回收策略，当然一般来说还是设置为 <code>Retain</code> 这种策略保险一点。</p>
<p>注意</p>
<p><code>Recycle</code> 策略会通过运行一个 busybox 容器来执行数据删除命令，默认定义的 busybox 镜像是：<code>gcr.io/google_containers/busybox:latest</code>，并且 <code>imagePullPolicy: Always</code>，如果需要调整配置，需要增加<code>kube-controller-manager</code> 启动参数：<code>--pv-recycler-pod-template-filepath-hostpath</code> 来进行配置。</p>
<p>关于 PV 的状态，实际上描述的是 PV 的生命周期的某个阶段，一个 PV 的生命周期中，可能会处于 4 种不同的阶段：</p>
<ul>
<li>Available（可用）：表示可用状态，还未被任何 PVC 绑定</li>
<li>Bound（已绑定）：表示 PV 已经被 PVC 绑定</li>
<li>Released（已释放）：PVC 被删除，但是资源还未被集群重新声明</li>
<li>Failed（失败）： 表示该 PV 的自动回收失败</li>
</ul>
<p>现在我们创建完成了 PV，如果我们需要使用这个 PV 的话，就需要创建一个对应的 PVC 来和他进行绑定了，就类似于我们的服务是通过 Pod 来运行的，而不是 Node，只是 Pod 跑在 Node 上而已。</p>
<p>现在我们来创建一个 <code>PersistentVolumeClaim</code>，Pod 使用 PVC 来请求物理存储，我们这里创建的 PVC 请求至少 3G 容量的卷，该卷至少可以为一个节点提供读写访问，下面是 PVC 的配置文件：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pvc-hostpath.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-hostpath</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br></pre></td></tr></table></figure>



<p>同样我们可以直接创建这个 PVC 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl create -f pvc-hostpath.yaml</span><br><span class="line">persistentvolumeclaim/pvc-hostpath created</span><br></pre></td></tr></table></figure>



<p>创建 PVC 之后，Kubernetes 就会去查找满足我们声明要求的 PV，比如 <code>storageClassName</code>、<code>accessModes</code> 以及容量这些是否满足要求，如果满足要求就会将 PV 和 PVC 绑定在一起。</p>
<p>注意</p>
<p>需要注意的是目前 PV 和 PVC 之间是一对一绑定的关系，也就是说一个 PV 只能被一个 PVC 绑定。</p>
<p>我们现在再次查看 PV 的信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pv -l type=local</span><br><span class="line">NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE</span><br><span class="line">pv-hostpath   10Gi       RWO            Retain           Bound    default/pvc-hostpath   manual                  81m</span><br></pre></td></tr></table></figure>



<p>现在输出的 STATUS 为 <code>Bound</code>，查看 PVC 的信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc pvc-hostpath</span><br><span class="line">NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-hostpath   Bound    pv-hostpath   10Gi       RWO            manual         6m47s</span><br></pre></td></tr></table></figure>



<p>输出结果表明该 PVC 绑定了到了上面我们创建的 <code>pv-hostpath</code> 这个 PV 上面了，我们这里虽然声明的 3G 的容量，但是由于 PV 里面是 10G，所以显然也是满足要求的。</p>
<p>PVC 准备好过后，接下来我们就可以来创建 Pod 了，该 Pod 使用上面我们声明的 PVC 作为存储卷：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pv-hostpath-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-hostpath-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pv-hostpath</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">pvc-hostpath</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">kubernetes.io/hostname:</span> <span class="string">node1</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">task-pv-container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&#x27;/usr/share/nginx/html&#x27;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">pv-hostpath</span></span><br></pre></td></tr></table></figure>



<p>这里需要注意的是，由于我们创建的 PV 真正的存储在节点 node1 上面，所以我们这里必须把 Pod 固定在这个节点下面，另外可以注意到 Pod 的配置文件指定了 <code>PersistentVolumeClaim</code>，但没有指定 <code>PersistentVolume</code>，对 Pod 而言，<code>PVC</code> 就是一个存储卷。直接创建这个 Pod 对象即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl create -f pv-hostpath-pod.yaml</span><br><span class="line">pod/pv-hostpath-pod created</span><br><span class="line">➜ kubectl get pod pv-hostpath-pod</span><br><span class="line">NAME              READY   STATUS    RESTARTS   AGE</span><br><span class="line">pv-hostpath-pod   1/1     Running   0          105s</span><br></pre></td></tr></table></figure>



<p>运行成功后，我们可以打开一个 shell 访问 Pod 中的容器：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it pv-hostpath-pod -- /bin/bash</span><br></pre></td></tr></table></figure>



<p>在 shell 中，我们可以验证 nginx 的数据 是否正在从 hostPath 卷提供 index.html 文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@pv-hostpath-pod:/# apt-get update</span><br><span class="line">root@pv-hostpath-pod:/# apt-get install curl -y</span><br><span class="line">root@pv-hostpath-pod:/# curl localhost</span><br><span class="line">Hello from Kubernetes hostpath storage</span><br></pre></td></tr></table></figure>



<p>我们可以看到输出结果是我们前面写到 hostPath 卷种的 index.html 文件中的内容，同样我们可以把 Pod 删除，然后再次重建再测试一次，可以发现内容还是我们在 hostPath 种设置的内容。</p>
<p>我们在持久化容器数据的时候使用 PV&#x2F;PVC 有什么好处呢？比如我们这里之前直接在 Pod 下面也可以使用 hostPath 来持久化数据，为什么还要费劲去创建 PV、PVC 对象来引用呢？PVC 和 PV 的设计，其实跟<code>“面向对象”</code>的思想完全一致，PVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则由 PV 负责完成。这样做的好处是，作为应用开发者，我们只需要跟 PVC 这个“接口”打交道，而不必关心具体的实现是 hostPath、NFS 还是 Ceph。毕竟这些存储相关的知识太专业了，应该交给专业的人去做，这样对于我们的 Pod 来说就不用管具体的细节了，你只需要给我一个可用的 PVC 即可了，这样是不是就完全屏蔽了细节和解耦了啊，所以我们更应该使用 PV、PVC 这种方式。</p>
<h2 id="Local-PV"><a href="#Local-PV" class="headerlink" title="Local PV"></a>Local PV</h2><p>上面我们创建了后端是 hostPath 类型的 PV 资源对象，我们也提到了，使用 hostPath 有一个局限性就是，我们的 Pod 不能随便漂移，需要固定到一个节点上，因为一旦漂移到其他节点上去了宿主机上面就没有对应的数据了，所以我们在使用 hostPath 的时候都会搭配 nodeSelector 来进行使用。但是使用 hostPath 明显也有一些好处的，因为 PV 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多，所以对于一些对磁盘 IO 要求比较高的应用比如 etcd 就非常实用了。不过呢，相比于正常的 PV 来说，使用了 hostPath 的这些节点一旦宕机数据就可能丢失，所以这就要求使用 hostPath 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。</p>
<p>所以在 hostPath 的基础上，Kubernetes 依靠 PV、PVC 实现了一个新的特性，这个特性的名字叫作：<code>Local Persistent Volume</code>，也就是我们说的 <code>Local PV</code>。</p>
<p>其实 <code>Local PV</code> 实现的功能就非常类似于 <code>hostPath</code> 加上 <code>nodeAffinity</code>，比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了，那么，我只需要再给这个 Pod 加上一个 <code>nodeAffinity=nodeA</code>，不就可以使用这个 Volume 了吗？理论上确实是可行的，但是事实上，我们绝不应该把一个宿主机上的目录当作 PV 来使用，因为本地目录的存储行为是完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。所以，一般来说 <code>Local PV</code> 对应的存储介质是一块额外挂载在宿主机的磁盘或者块设备，我们可以认为就是<code>“一个 PV 一块盘”</code>。</p>
<p>另外一个 <code>Local PV</code> 和普通的 PV 有一个很大的不同在于 <code>Local PV</code> 可以保证 Pod 始终能够被正确地调度到它所请求的 <code>Local PV</code> 所在的节点上面，对于普通的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后再持久化节点上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载，但是对于 <code>Local PV</code> 来说，节点上可供使用的磁盘必须是提前准备好的，因为它们在不同节点上的挂载情况可能完全不同，甚至有的节点可以没这种磁盘，所以，这时候，调度器就必须能够知道所有节点与 <code>Local PV</code> 对应的磁盘的关联关系，然后根据这个信息来调度 Pod，实际上就是在调度的时候考虑 Volume 的分布。</p>
<p>接下来我们来测试下 <code>Local PV</code> 的使用，当然按照上面我们的分析我们应该给宿主机挂载并格式化一个可用的磁盘，我们这里就暂时将 node1 节点上的 <code>/data/k8s/localpv</code> 这个目录看成是挂载的一个独立的磁盘。现在我们来声明一个 <code>Local PV</code> 类型的 PV，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pv-local.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">local-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/k8s/localpv</span> <span class="comment"># node1节点上的目录</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">              <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">              <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">node1</span></span><br></pre></td></tr></table></figure>



<p>和前面我们定义的 PV 不同，我们这里定义了一个 <code>local</code> 字段，表明它是一个 <code>Local PV</code>，而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：<code>/data/k8s/localpv</code>，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node1 节点上。所以，在这个 PV 的定义里，添加了一个节点亲和性 <code>nodeAffinity</code> 字段指定 node1 这个节点。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。</p>
<p>直接创建上面的资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f pv-local.yaml</span><br><span class="line">persistentvolume/pv-local created</span><br><span class="line">➜ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS  CLAIM      STORAGECLASS      REASON   AGE</span><br><span class="line">pv-local  5Gi        RWO            Delete           Available          local-storage              24s</span><br></pre></td></tr></table></figure>



<p>可以看到，这个 PV 创建后，进入了 <code>Available</code>（可用）状态。这个时候如果按照前面提到的，我们要使用这个 <code>Local PV</code> 的话就需要去创建一个 PVC 和他进行绑定：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pvc-local.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-local</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">local-storage</span></span><br></pre></td></tr></table></figure>



<p>同样要注意声明的这些属性需要和上面的 PV 对应，直接创建这个资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f pvc-local.yaml</span><br><span class="line">persistentvolumeclaim/pvc-local created</span><br><span class="line">➜ kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">pvc-local      Bound    pv-local      5Gi        RWO            local-storage   38s</span><br></pre></td></tr></table></figure>



<p>可以看到现在 PVC 和 PV 已经处于 <code>Bound</code> 绑定状态了。但实际上这是不符合我们的需求的，比如现在我们的 Pod 声明使用这个 pvc-local，并且我们也明确规定，这个 Pod 只能运行在 node2 这个节点上，如果按照上面我们这里的操作，这个 pvc-local 是不是就和我们这里的 pv-local 这个 <code>Local PV</code> 绑定在一起了，但是这个 PV 的存储卷又在 node1 这个节点上，显然就会出现冲突了，那么这个 Pod 的调度肯定就会失败了，所以我们在使用 <code>Local PV</code> 的时候，必须想办法延迟这个<code>“绑定”</code>操作。</p>
<p>要怎么来实现这个延迟绑定呢？我们可以通过创建 <code>StorageClass</code> 来指定这个动作，在 StorageClass 种有一个 <code>volumeBindingMode=WaitForFirstConsumer</code> 的属性，就是告诉 Kubernetes 在发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但不要现在就立刻执行绑定操作（即：设置 PVC 的 VolumeName 字段），而是要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。</p>
<p>所以我们需要创建对应的 <code>StorageClass</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># local-storageclass.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/no-provisioner</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br></pre></td></tr></table></figure>



<p>这个 <code>StorageClass</code> 的名字，叫作 local-storage，也就是我们在 PV 中声明的，需要注意的是，在它的 <code>provisioner</code> 字段，我们指定的是 <code>no-provisioner</code>。这是因为我们这里是手动创建的 PV，所以不需要动态来生成 PV，另外这个 StorageClass 还定义了一个 <code>volumeBindingMode=WaitForFirstConsumer</code> 的属性，它是 <code>Local PV</code> 里一个非常重要的特性，即：<strong>延迟绑定</strong>。通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。</p>
<p>现在我们来创建这个 StorageClass 资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f local-storageclass.yaml</span><br><span class="line">storageclass.storage.k8s.io/local-storage created</span><br></pre></td></tr></table></figure>



<p>现在我们重新删除上面声明的 PVC 对象，重新创建：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl delete -f pvc-local.yaml</span><br><span class="line">persistentvolumeclaim &quot;pvc-local&quot; deleted</span><br><span class="line">➜ kubectl create -f pvc-local.yaml</span><br><span class="line">persistentvolumeclaim/pvc-local created</span><br><span class="line">➜ kubectl get pvc</span><br><span class="line">NAME           STATUS    VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">pvc-local      Pending                                           local-storage   3s</span><br></pre></td></tr></table></figure>



<p>我们可以发现这个时候，集群中即使已经存在了一个可以与 PVC 匹配的 PV 了，但这个 PVC 依然处于 <code>Pending</code> 状态，也就是等待绑定的状态，这就是因为上面我们配置的是延迟绑定，需要在真正的 Pod 使用的时候才会来做绑定。</p>
<p>同样我们声明一个 Pod 来使用这里的 pvc-local 这个 PVC，资源对象如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pv-local-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-local-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example-pv-local</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">pvc-local</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example-pv-local</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">example-pv-local</span></span><br></pre></td></tr></table></figure>



<p>直接创建这个 Pod：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f pv-local-pod.yaml</span><br><span class="line">pod/pv-local-pod created</span><br></pre></td></tr></table></figure>



<p>创建完成后我们这个时候去查看前面我们声明的 PVC，会立刻变成 <code>Bound</code> 状态，与前面定义的 PV 绑定在了一起：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">pvc-local      Bound    pv-local      5Gi        RWO            local-storage   4m59s</span><br></pre></td></tr></table></figure>



<p>这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl exec -it pv-local-pod /bin/sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span> /usr/share/nginx/html</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">echo</span> <span class="string">&quot;Hello from Kubernetes local pv storage&quot;</span> &gt; test.txt</span></span><br><span class="line"><span class="meta prompt_">#</span></span><br></pre></td></tr></table></figure>



<p>然后，登录到 node1 这台机器上，查看一下它的 <code>/data/k8s/localpv</code> 目录下的内容，你就可以看到刚刚创建的这个文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node1节点上</span></span><br><span class="line">➜ ls /data/k8s/localpv</span><br><span class="line">test.txt</span><br><span class="line">➜ cat /data/k8s/localpv/test.txt</span><br><span class="line">Hello from Kubernetes local pv storage</span><br></pre></td></tr></table></figure>



<p>如果重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl delete -f pv-local-pod.yaml</span><br><span class="line">➜ kubectl apply -f pv-local-pod.yaml</span><br><span class="line">➜ kubectl exec -it pv-local-pod /bin/sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">ls</span> /usr/share/nginx/html</span></span><br><span class="line">test.txt</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> /usr/share/nginx/html/test.txt</span></span><br><span class="line">Hello from Kubernetes local pv storage</span><br><span class="line"><span class="meta prompt_">#</span></span><br></pre></td></tr></table></figure>



<p>到这里就说明基于本地存储的 Volume 是完全可以提供容器持久化存储功能的，对于 StatefulSet 这样的有状态的资源对象，也完全可以通过声明 Local 类型的 PV 和 PVC，来管理应用的存储状态。</p>
<p>需要注意的是，我们上面手动创建 PV 的方式，即静态的 PV 管理方式，在删除 PV 时需要按如下流程执行操作：</p>
<ul>
<li>删除使用这个 PV 的 Pod</li>
<li>从宿主机移除本地磁盘</li>
<li>删除 PVC</li>
<li>删除 PV</li>
</ul>
<p>如果不按照这个流程的话，这个 PV 的删除就会失败。</p>
<p><a target="_blank" rel="noopener" href="https://docs.youdianzhishi.com/k8s/scheduler/qos/">
</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/LocalDNS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/LocalDNS/" class="post-title-link" itemprop="url">LocalDNS</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:48:43" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">网络</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DNS-优化"><a href="#DNS-优化" class="headerlink" title="DNS 优化"></a>DNS 优化</h1><p>前面我们讲解了在 Kubernetes 中我们可以使用 CoreDNS 来进行集群的域名解析，但是如果在集群规模较大并发较高的情况下我们仍然需要对 DNS 进行优化，典型的就是大家比较熟悉的 CoreDNS 会出现超时 5s 的情况。</p>
<h2 id="超时原因"><a href="#超时原因" class="headerlink" title="超时原因"></a>超时原因</h2><p>在 iptables 模式下（默认情况下），每个服务的 kube-proxy 在主机网络名称空间的 nat 表中创建一些 iptables 规则。</p>
<p>比如在集群中具有两个 DNS 服务器实例的 kube-dns 服务，其相关规则大致如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(1) -A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES</span><br><span class="line">&lt;...&gt;</span><br><span class="line">(2) -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment &quot;kube-system/kube-dns:dns cluster IP&quot; -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU</span><br><span class="line">&lt;...&gt;</span><br><span class="line">(3) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment &quot;kube-system/kube-dns:dns&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-LLLB6FGXBLX6PZF7</span><br><span class="line">(4) -A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment &quot;kube-system/kube-dns:dns&quot; -j KUBE-SEP-LRVEW52VMYCOUSMZ</span><br><span class="line">&lt;...&gt;</span><br><span class="line">(5) -A KUBE-SEP-LLLB6FGXBLX6PZF7 -p udp -m comment --comment &quot;kube-system/kube-dns:dns&quot; -m udp -j DNAT --to-destination 10.32.0.6:53</span><br><span class="line">&lt;...&gt;</span><br><span class="line">(6) -A KUBE-SEP-LRVEW52VMYCOUSMZ -p udp -m comment --comment &quot;kube-system/kube-dns:dns&quot; -m udp -j DNAT --to-destination 10.32.0.7:53</span><br></pre></td></tr></table></figure>



<p>我们知道每个 Pod 的 <code>/etc/resolv.conf</code> 文件中都有填充的 <code>nameserver 10.96.0.10</code> 这个条目。所以来自 Pod 的 DNS 查找请求将发送到 <code>10.96.0.10</code>，这是 kube-dns 服务的 ClusterIP 地址。</p>
<p>由于 <code>(1)</code> 请求进入 <code>KUBE-SERVICE</code> 链，然后匹配规则 <code>(2)</code>，最后根据 <code>(3)</code> 的 random 随机模式，跳转到 (5) 或 (6) 条目，将请求 UDP 数据包的目标 IP 地址修改为 DNS 服务器的<code>实际</code> IP 地址，这是通过 <code>DNAT</code> 完成的。其中 <code>10.32.0.6</code> 和 <code>10.32.0.7</code> 是我们集群中 CoreDNS 的两个 Pod 副本的 IP 地址。</p>
<h3 id="内核中的-DNAT"><a href="#内核中的-DNAT" class="headerlink" title="内核中的 DNAT"></a>内核中的 DNAT</h3><p><code>DNAT</code> 的主要职责是同时更改传出数据包的目的地，响应数据包的源，并确保对所有后续数据包进行相同的修改。后者严重依赖于连接跟踪机制，也称为 <code>conntrack</code>，它被实现为内核模块。<code>conntrack</code> 会跟踪系统中正在进行的网络连接。</p>
<p><code>conntrack</code> 中的每个连接都由两个元组表示，一个元组用于原始请求（IP_CT_DIR_ORIGINAL），另一个元组用于答复（IP_CT_DIR_REPLY）。对于 UDP，每个元组都由源 IP 地址，源端口以及目标 IP 地址和目标端口组成，答复元组包含存储在 src 字段中的目标的真实地址。</p>
<p>例如，如果 IP 地址为 <code>10.40.0.17</code> 的 Pod 向 kube-dns 的 ClusterIP 发送一个请求，该请求被转换为 <code>10.32.0.6</code>，则将创建以下元组：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原始：src = 10.40.0.17 dst = 10.96.0.10 sport = 53378 dport = 53</span><br><span class="line">回复：src = 10.32.0.6 dst = 10.40.0.17 sport = 53 dport = 53378</span><br></pre></td></tr></table></figure>



<p>通过这些条目内核可以相应地修改任何相关数据包的目的地和源地址，而无需再次遍历 DNAT 规则，此外，它将知道如何修改回复以及应将回复发送给谁。创建 <code>conntrack</code> 条目后，将首先对其进行确认，然后如果没有已确认的 <code>conntrack</code> 条目具有相同的原始元组或回复元组，则内核将尝试确认该条目。<code>conntrack</code> 创建和 DNAT 的简化流程如下所示：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/iz848m.png" alt="conntrack"></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>DNS 客户端 (glibc 或 musl libc) 会并发请求 A 和 AAAA 记录，跟 DNS Server 通信自然会先 connect (建立 fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议，connect 时并不会创建 <code>conntrack</code> 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，这时它们源 Port 相同，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们分别创建 conntrack 表项，而集群内请求 CoreDNS 都是访问的 CLUSTER-IP，报文最终会被 DNAT 成一个具体的 Pod IP，当两个包被 DNAT 成同一个 IP，最终它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 DNS 的 Pod 副本只有一个实例的情况就很容易发生，现象就是 DNS 请求超时，客户端默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 DNS 请求有 5s 的延时。</p>
<blockquote>
<p>具体原因可以参考 weave works 总结的文章 <a target="_blank" rel="noopener" href="https://www.weave.works/blog/racy-conntrack-and-dns-lookup-timeouts">Racy conntrack and DNS lookup timeouts</a>。</p>
</blockquote>
<ul>
<li>只有多个线程或进程，并发从同一个 socket 发送相同五元组的 UDP 报文时，才有一定概率会发生</li>
<li>glibc、musl（alpine linux 的 libc 库）都使用 <code>parallel query</code>, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃</li>
<li>由于 ipvs 也使用了 conntrack, 使用 kube-proxy 的 ipvs 模式，并不能避免这个问题</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>要彻底解决这个问题最好当然是内核上去 FIX 掉这个 BUG，除了这种方法之外我们还可以使用其他方法来进行规避，我们可以避免相同五元组 DNS 请求的并发。</p>
<p>在 <code>resolv.conf</code> 中就有两个相关的参数可以进行配置：</p>
<ul>
<li><code>single-request-reopen</code>：发送 A 类型请求和 AAAA 类型请求使用不同的源端口，这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突。</li>
<li><code>single-request</code>：避免并发，改为串行发送 A 类型和 AAAA 类型请求。没有了并发，从而也避免了冲突。</li>
</ul>
<p>要给容器的 <code>resolv.conf</code> 加上 options 参数，有几个办法：</p>
<ul>
<li><ol>
<li>在容器的 <code>ENTRYPOINT</code> 或者 <code>CMD</code> 脚本中，执行 <code>/bin/echo &#39;options single-request-reopen&#39; &gt;&gt; /etc/resolv.conf</code></li>
</ol>
</li>
<li><ol>
<li>在 Pod 的 postStart hook 中添加：</li>
</ol>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">lifecycle:</span></span><br><span class="line">  <span class="attr">postStart:</span></span><br><span class="line">    <span class="attr">exec:</span></span><br><span class="line">      <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;/bin/echo &#x27;options single-request-reopen&#x27; &gt;&gt; /etc/resolv.conf&quot;</span></span><br></pre></td></tr></table></figure>



<ul>
<li><ol>
<li>使用 <code>template.spec.dnsConfig</code> 配置:</li>
</ol>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">template:</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">dnsConfig:</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">single-request-reopen</span></span><br></pre></td></tr></table></figure>



<ul>
<li><ol>
<li>使用 ConfigMap 覆盖 Pod 里面的 <code>/etc/resolv.conf</code>：</li>
</ol>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># configmap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">resolv.conf:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    nameserver 1.2.3.4</span></span><br><span class="line"><span class="string">    search default.svc.cluster.local svc.cluster.local cluster.local</span></span><br><span class="line"><span class="string">    options ndots:5 single-request-reopen timeout:1</span></span><br><span class="line"><span class="string"></span><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resolvconf</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># Pod Spec</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">resolv-conf</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/resolv.conf</span></span><br><span class="line">      <span class="attr">subPath:</span> <span class="string">resolv.conf</span> <span class="comment"># 在某个目录下面挂载一个文件（保证不覆盖当前目录）需要使用subPath -&gt; 不支持热更新</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">resolv-conf</span></span><br><span class="line">    <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">resolvconf</span></span><br><span class="line">      <span class="attr">items:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">resolv.conf</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">resolv.conf</span></span><br></pre></td></tr></table></figure>



<p>上面的方法在一定程度上可以解决 DNS 超时的问题，但更好的方式是<strong>使用本地 DNS 缓存</strong>，容器的 DNS 请求都发往本地的 DNS 缓存服务，也就不需要走 DNAT，当然也不会发生 <code>conntrack</code> 冲突了，而且还可以有效提升 CoreDNS 的性能瓶颈。</p>
<h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>这里我们使用一个简单的 golang 程序来测试下使用本地 DNS 缓存的前后性能。代码如下所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main.go</span></span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;context&quot;</span></span><br><span class="line">    <span class="string">&quot;flag&quot;</span></span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;net&quot;</span></span><br><span class="line">    <span class="string">&quot;sync/atomic&quot;</span></span><br><span class="line">    <span class="string">&quot;time&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> host <span class="type">string</span></span><br><span class="line"><span class="keyword">var</span> connections <span class="type">int</span></span><br><span class="line"><span class="keyword">var</span> duration <span class="type">int64</span></span><br><span class="line"><span class="keyword">var</span> limit <span class="type">int64</span></span><br><span class="line"><span class="keyword">var</span> timeoutCount <span class="type">int64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    flag.StringVar(&amp;host, <span class="string">&quot;host&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;Resolve host&quot;</span>)</span><br><span class="line">    flag.IntVar(&amp;connections, <span class="string">&quot;c&quot;</span>, <span class="number">100</span>, <span class="string">&quot;Connections&quot;</span>)</span><br><span class="line">    flag.Int64Var(&amp;duration, <span class="string">&quot;d&quot;</span>, <span class="number">0</span>, <span class="string">&quot;Duration(s)&quot;</span>)</span><br><span class="line">    flag.Int64Var(&amp;limit, <span class="string">&quot;l&quot;</span>, <span class="number">0</span>, <span class="string">&quot;Limit(ms)&quot;</span>)</span><br><span class="line">    flag.Parse()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> count <span class="type">int64</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> errCount <span class="type">int64</span> = <span class="number">0</span></span><br><span class="line">    pool := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;, connections)</span><br><span class="line">    exit := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="type">bool</span>)</span><br><span class="line">    <span class="keyword">var</span> (</span><br><span class="line">        min <span class="type">int64</span> = <span class="number">0</span></span><br><span class="line">        max <span class="type">int64</span> = <span class="number">0</span></span><br><span class="line">        sum <span class="type">int64</span> = <span class="number">0</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">        time.Sleep(time.Second * time.Duration(duration))</span><br><span class="line">        exit &lt;- <span class="literal">true</span></span><br><span class="line">    &#125;()</span><br><span class="line"></span><br><span class="line">endD:</span><br><span class="line">    <span class="keyword">for</span> &#123;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> pool &lt;- <span class="literal">nil</span>:</span><br><span class="line">            <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">                <span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line">                    &lt;-pool</span><br><span class="line">                &#125;()</span><br><span class="line">                resolver := &amp;net.Resolver&#123;&#125;</span><br><span class="line">                now := time.Now()</span><br><span class="line">                _, err := resolver.LookupIPAddr(context.Background(), host)</span><br><span class="line">                use := time.Since(now).Nanoseconds() / <span class="type">int64</span>(time.Millisecond)</span><br><span class="line">                <span class="keyword">if</span> min == <span class="number">0</span> || use &lt; min &#123;</span><br><span class="line">                    min = use</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> use &gt; max &#123;</span><br><span class="line">                    max = use</span><br><span class="line">                &#125;</span><br><span class="line">                sum += use</span><br><span class="line">                <span class="keyword">if</span> limit &gt; <span class="number">0</span> &amp;&amp; use &gt;= limit &#123;</span><br><span class="line">                    timeoutCount++</span><br><span class="line">                &#125;</span><br><span class="line">                atomic.AddInt64(&amp;count, <span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">                    fmt.Println(err.Error())</span><br><span class="line">                    atomic.AddInt64(&amp;errCount, <span class="number">1</span>)</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;()</span><br><span class="line">        <span class="keyword">case</span> &lt;-exit:</span><br><span class="line">            <span class="keyword">break</span> endD</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Printf(<span class="string">&quot;request count：%d\nerror count：%d\n&quot;</span>, count, errCount)</span><br><span class="line">    fmt.Printf(<span class="string">&quot;request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\n&quot;</span>, min, max, sum/count, timeoutCount)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>首先配置好 golang 环境，然后直接构建上面的测试应用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go build -o testdns .</span><br></pre></td></tr></table></figure>



<p>构建完成后生成一个 testdns 的二进制文件，然后我们将这个二进制文件拷贝到任意一个 Pod 中去进行测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl cp testdns svc-demo-546b7bcdcf-6xsnr:/root -n default</span><br></pre></td></tr></table></figure>



<p>拷贝完成后进入这个测试的 Pod 中去：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it svc-demo-546b7bcdcf-6xsnr -- /bin/bash</span></span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:/# cd /root</span><br></pre></td></tr></table></figure>



<p>然后我们执行 testdns 程序来进行压力测试，比如执行 200 个并发，持续 30 秒：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">对 nginx-service.default 这个地址进行解析</span></span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">request count：12533</span><br><span class="line">error count：5</span><br><span class="line">request time：min(5ms) max(16871ms) avg(425ms) timeout(475n)</span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">request count：10058</span><br><span class="line">error count：3</span><br><span class="line">request time：min(4ms) max(12347ms) avg(540ms) timeout(487n)</span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">lookup nginx-service.default on 10.96.0.10:53: no such host</span><br><span class="line">request count：12242</span><br><span class="line">error count：2</span><br><span class="line">request time：min(3ms) max(12206ms) avg(478ms) timeout(644n)</span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：11008</span><br><span class="line">error count：0</span><br><span class="line">request time：min(3ms) max(11110ms) avg(496ms) timeout(478n)</span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：9141</span><br><span class="line">error count：0</span><br><span class="line">request time：min(4ms) max(11198ms) avg(607ms) timeout(332n)</span><br><span class="line">root@svc-demo-546b7bcdcf-6xsnr:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：9126</span><br><span class="line">error count：0</span><br><span class="line">request time：min(4ms) max(11554ms) avg(613ms) timeout(197n)</span><br></pre></td></tr></table></figure>



<p>我们可以看到大部分平均耗时都是在 500ms 左右，这个性能是非常差的，而且还有部分解析失败的条目。接下来我们就来尝试使用 <code>NodeLocal DNSCache</code> 来提升 DNS 的性能和可靠性。</p>
<h2 id="NodeLocal-DNSCache"><a href="#NodeLocal-DNSCache" class="headerlink" title="NodeLocal DNSCache"></a>NodeLocal DNSCache</h2><p><code>NodeLocal DNSCache</code> 通过在集群节点上运行一个 DaemonSet 来提高集群 DNS 性能和可靠性。处于 ClusterFirst 的 DNS 模式下的 Pod 可以连接到 kube-dns 的 serviceIP 进行 DNS 查询，通过 kube-proxy 组件添加的 iptables 规则将其转换为 CoreDNS 端点。通过在每个集群节点上运行 DNS 缓存，<code>NodeLocal DNSCache</code> 可以缩短 DNS 查找的延迟时间、使 DNS 查找时间更加一致，以及减少发送到 kube-dns 的 DNS 查询次数。</p>
<p>在集群中运行 <code>NodeLocal DNSCache</code> 有如下几个好处：</p>
<ul>
<li>如果本地没有 CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须到另一个节点进行解析，使用 <code>NodeLocal DNSCache</code> 后，拥有本地缓存将有助于改善延迟</li>
<li>跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满 conntrack 表（上面提到的 5s 超时问题就是这个原因造成的）</li>
<li>从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP，TCP conntrack 条目将在连接关闭时被删除，而 UDP 条目必须超时(默认 <code>nfconntrackudp_timeout</code> 是 30 秒)</li>
<li>将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时）</li>
</ul>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/upm1o7.png" alt="NodeLocal"></p>
<p>要安装 <code>NodeLocal DNSCache</code> 也非常简单，直接获取官方的资源清单即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml</span><br></pre></td></tr></table></figure>



<p>该资源清单文件中包含几个变量值得注意，其中：</p>
<ul>
<li><code>__PILLAR__DNS__SERVER__</code> ：表示 kube-dns 这个 Service 的 ClusterIP，可以通过命令 <code>kubectl get svc -n kube-system | grep kube-dns | awk&#39;&#123; print $3 &#125;&#39;</code> 获取（我们这里就是 <code>10.96.0.10</code>）</li>
<li><code>__PILLAR__LOCAL__DNS__</code>：表示 DNSCache 本地的 IP，默认为 <code>169.254.20.10</code></li>
<li><code>__PILLAR__DNS__DOMAIN__</code>：表示集群域，默认就是 <code>cluster.local</code></li>
</ul>
<p>另外还有两个参数 <code>__PILLAR__CLUSTER__DNS__</code> 和 <code>__PILLAR__UPSTREAM__SERVERS__</code>，这两个参数会通过镜像 1.15.16 版本去进行自动配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的 <code>Upstream Server</code> 配置。直接执行如下所示的命令即可安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sed <span class="string">&#x27;s/k8s.gcr.io\/dns/cnych/g</span></span></span><br><span class="line">s/__PILLAR__DNS__SERVER__/10.96.0.10/g</span><br><span class="line">s/__PILLAR__LOCAL__DNS__/169.254.20.10/g</span><br><span class="line">s/__PILLAR__DNS__DOMAIN__/cluster.local/g&#x27; nodelocaldns.yaml |</span><br><span class="line">kubectl apply -f -</span><br></pre></td></tr></table></figure>



<p>可以通过如下命令来查看对应的 Pod 是否已经启动成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods -n kube-system -l k8s-app=node-local-dns</span></span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">node-local-dns-4wclp   1/1     Running   0          5m43s</span><br><span class="line">node-local-dns-gxq57   1/1     Running   0          5m43s</span><br><span class="line">node-local-dns-v7gtz   1/1     Running   0          5m43s</span><br></pre></td></tr></table></figure>



<p>需要注意的是这里使用 DaemonSet 部署 node-local-dns 使用了 <code>hostNetwork=true</code>，会占用宿主机的 8080 端口，所以需要保证该端口未被占用。</p>
<p>但是到这里还没有完，如果 kube-proxy 组件使用的是 ipvs 模式的话我们还需要修改 kubelet 的 <code>--cluster-dns</code> 参数，将其指向 <code>169.254.20.10</code>，Daemonset 会在每个节点创建一个网卡来绑这个 IP，Pod 向本节点这个 IP 发 DNS 请求，缓存没有命中的时候才会再代理到上游集群 DNS 进行查询。iptables 模式下 Pod 还是向原来的集群 DNS 请求，节点上有这个 IP 监听，会被本机拦截，再请求集群上游 DNS，所以不需要更改 <code>--cluster-dns</code> 参数。</p>
<blockquote>
<p>如果担心线上环境修改 <code>--cluster-dns</code> 参数会产生影响，我们也可以直接在新部署的 Pod 中通过 dnsConfig 配置使用新的 localdns 的地址来进行解析。</p>
</blockquote>
<p>由于我这里使用的是 kubeadm 安装的 1.19 版本的集群，所以我们只需要替换节点上 <code>/var/lib/kubelet/config.yaml</code> 文件中的 clusterDNS 这个参数值，然后重启即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#x27;s/10.96.0.10/169.254.20.10/g&#x27; /var/lib/kubelet/config.yaml</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>



<p>待 node-local-dns 安装配置完成后，我们可以部署一个新的 Pod 来验证下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test-node-local-dns.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-node-local-dns</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">local-dns</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;sleep 60m&#x27;</span>]</span><br></pre></td></tr></table></figure>



<p>直接部署：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f test-node-local-dns.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it test-node-local-dns /bin/sh</span></span><br><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 169.254.20.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>



<p>我们可以看到 nameserver 已经变成 <code>169.254.20.10</code> 了，当然对于之前的历史 Pod 要想使用 node-local-dns 则需要重建。</p>
<p>接下来我们重建前面压力测试 DNS 的 Pod，重新将 testdns 二进制文件拷贝到 Pod 中去：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">拷贝到重建的 Pod 中</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">cp</span> testdns svc-demo-546b7bcdcf-b5mkt:/root</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it svc-demo-546b7bcdcf-b5mkt -- /bin/bash</span></span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:/# cat /etc/resolv.conf</span><br><span class="line">nameserver 169.254.20.10  # 可以看到 nameserver 已经更改</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:/# cd /root</span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:~# ls</span><br><span class="line">testdns</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重新执行压力测试</span></span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：16297</span><br><span class="line">error count：0</span><br><span class="line">request time：min(2ms) max(5270ms) avg(357ms) timeout(8n)</span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：15982</span><br><span class="line">error count：0</span><br><span class="line">request time：min(2ms) max(5360ms) avg(373ms) timeout(54n)</span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：25631</span><br><span class="line">error count：0</span><br><span class="line">request time：min(3ms) max(958ms) avg(232ms) timeout(0n)</span><br><span class="line">root@svc-demo-546b7bcdcf-b5mkt:~# ./testdns -host nginx-service.default -c 200 -d 30 -l 5000</span><br><span class="line">request count：23388</span><br><span class="line">error count：0</span><br><span class="line">request time：min(6ms) max(1130ms) avg(253ms) timeout(0n)</span><br></pre></td></tr></table></figure>



<p>从上面的结果可以看到无论是最大解析时间还是平均解析时间都比之前默认的 CoreDNS 提示了不少的效率，所以我们还是非常推荐在线上环境部署 <code>NodeLocal DNSCache</code> 来提升 DNS 的性能和可靠性的，唯一的缺点就是由于 LocalDNS 使用的是 DaemonSet 模式部署，所以如果需要更新镜像则可能会中断服务（不过可以使用一些第三方的增强组件来实现原地升级解决这个问题，比如 <a target="_blank" rel="noopener" href="https://openkruise.io/">openkruise</a>）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/NAT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/NAT/" class="post-title-link" itemprop="url">NAT</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:50:08" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/" itemprop="url" rel="index"><span itemprop="name">WEB</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/Http/" itemprop="url" rel="index"><span itemprop="name">Http</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是网络地址转换（NAT）？"><a href="#什么是网络地址转换（NAT）？" class="headerlink" title="什么是网络地址转换（NAT）？"></a>什么是网络地址转换（NAT）？</h3><p><strong>NAT (Network Address Translation)<strong>，即</strong>网络地址转换</strong>，是一种在 IP 网络中改变数据包的 IP 地址信息的<strong>技术</strong>。它的主要目的是解决 IPv4 地址枯竭的问题，同时也能提供一定程度的网络安全隔离。</p>
<p>简单来说，NAT 允许一个拥有<strong>私有 IP 地址</strong>的网络（如家庭网络或企业内部网络）连接到使用<strong>公共 IP 地址</strong>的外部网络（如互联网），而无需为内部的每台设备都分配一个唯一的公共 IP 地址。</p>
<p><strong>核心概念：</strong></p>
<ul>
<li><strong>私有 IP 地址：</strong> 在私有网络内部使用的 IP 地址，它们不能直接在公共互联网上路由。RFC 1918 定义了以下私有 IP 地址范围：<ul>
<li><code>10.0.0.0</code> 到 <code>10.255.255.255</code> (10&#x2F;8)</li>
<li><code>172.16.0.0</code> 到 <code>172.31.255.255</code> (172.16&#x2F;12)</li>
<li><code>192.168.0.0</code> 到 <code>192.168.255.255</code> (192.168&#x2F;16)</li>
</ul>
</li>
<li><strong>公共 IP 地址：</strong> 在公共互联网上可寻址的全球唯一的 IP 地址。</li>
<li><strong>NAT 设备：</strong> 通常是路由器或防火墙，负责执行 IP 地址的转换。</li>
</ul>
<h3 id="NAT-的工作原理："><a href="#NAT-的工作原理：" class="headerlink" title="NAT 的工作原理："></a>NAT 的工作原理：</h3><p>NAT 的工作原理是当数据包在私有网络和公共网络之间传输时，修改其源 IP 地址或目的 IP 地址。具体的工作方式取决于 NAT 的类型。</p>
<p>让我们以最常见的<strong>端口地址转换 (PAT)<strong>，也称为</strong>网络地址端口转换 (NAPT)</strong> 或**多对一 NAT (Many-to-One NAT)**为例来描述其工作原理。这是家庭和小型办公室路由器最常用的 NAT 类型。</p>
<p><strong>场景描述：</strong></p>
<p>假设你家里有两个设备（PC1 和 PC2），它们都连接到同一个路由器，路由器有一个公共 IP 地址。</p>
<ul>
<li><strong>PC1：</strong> 私有 IP <code>192.168.1.10</code>，源端口 <code>1000</code></li>
<li><strong>PC2：</strong> 私有 IP <code>192.168.1.11</code>，源端口 <code>2000</code></li>
<li><strong>路由器：</strong> 私有接口 IP <code>192.168.1.1</code>，公共接口 IP <code>203.0.113.50</code> (你的公共 IP)</li>
<li><strong>外部服务器：</strong> IP <code>198.51.100.20</code>，端口 <code>80</code> (Web 服务器)</li>
</ul>
<hr>
<h4 id="1-私有网络到公共网络-出站流量-："><a href="#1-私有网络到公共网络-出站流量-：" class="headerlink" title="1. 私有网络到公共网络 (出站流量)："></a><strong>1. 私有网络到公共网络 (出站流量)：</strong></h4><ol>
<li><p><strong>PC1 发送请求：</strong></p>
<ul>
<li>PC1 想要访问外部服务器 <code>198.51.100.20</code> 的网页。</li>
<li>它生成一个 IP 数据包：<ul>
<li><strong>源 IP：</strong> <code>192.168.1.10</code></li>
<li><strong>源端口：</strong> <code>1000</code></li>
<li><strong>目的 IP：</strong> <code>198.51.100.20</code></li>
<li><strong>目的端口：</strong> <code>80</code></li>
</ul>
</li>
<li>这个数据包被发送到路由器。</li>
</ul>
</li>
<li><p><strong>路由器进行 NAT (地址和端口转换)：</strong></p>
<ul>
<li>路由器收到数据包后，检查其 NAT 表（也称为转换表或映射表）。</li>
<li>因为这是来自内部的连接请求第一次经过，路由器会：<ul>
<li><strong>记录：</strong> 在 NAT 表中创建一个新条目，记录 <code>192.168.1.10:1000</code> 将被转换为 <code>203.0.113.50:端口X</code>。</li>
<li><strong>选择公共端口：</strong> 路由器会选择一个自己公共 IP 地址上未被占用的端口（例如 <code>50000</code>）来代表 PC1 的请求。</li>
<li><strong>修改数据包头部：</strong><ul>
<li><strong>源 IP：</strong> 从 <code>192.168.1.10</code> 修改为 <code>203.0.113.50</code> (路由器的公共 IP)。</li>
<li><strong>源端口：</strong> 从 <code>1000</code> 修改为 <code>50000</code>。</li>
</ul>
</li>
</ul>
</li>
<li>转换后的数据包：<ul>
<li><strong>源 IP：</strong> <code>203.0.113.50</code></li>
<li><strong>源端口：</strong> <code>50000</code></li>
<li><strong>目的 IP：</strong> <code>198.51.100.20</code></li>
<li><strong>目的端口：</strong> <code>80</code></li>
</ul>
</li>
<li>路由器将这个新的数据包发送到公共互联网。</li>
</ul>
</li>
<li><p><strong>PC2 发送类似请求：</strong></p>
<ul>
<li>如果 PC2 同时访问外部服务器：<ul>
<li>数据包：源 IP <code>192.168.1.11</code>，源端口 <code>2000</code>。</li>
<li>路由器将它转换为：源 IP <code>203.0.113.50</code>，源端口 <code>50001</code> (例如)。</li>
</ul>
</li>
<li>路由器会为每个内部连接使用其公共 IP 上的不同源端口，以此来区分不同的内部主机。</li>
</ul>
</li>
</ol>
<h4 id="2-公共网络到私有网络-入站响应-："><a href="#2-公共网络到私有网络-入站响应-：" class="headerlink" title="2. 公共网络到私有网络 (入站响应)："></a><strong>2. 公共网络到私有网络 (入站响应)：</strong></h4><ol>
<li><p><strong>外部服务器发送响应：</strong></p>
<ul>
<li>外部服务器处理完 PC1 的请求后，发送响应数据包。</li>
<li>响应数据包的目的地是它收到的请求的源 IP 和源端口：<ul>
<li><strong>源 IP：</strong> <code>198.51.100.20</code></li>
<li><strong>源端口：</strong> <code>80</code></li>
<li><strong>目的 IP：</strong> <code>203.0.113.50</code> (路由器的公共 IP)</li>
<li><strong>目的端口：</strong> <code>50000</code> (路由器为 PC1 分配的端口)</li>
</ul>
</li>
<li>这个数据包被发送到互联网，最终到达你的路由器。</li>
</ul>
</li>
<li><p><strong>路由器进行反向 NAT (反向地址和端口转换)：</strong></p>
<ul>
<li>路由器收到响应数据包。</li>
<li>它根据响应数据包的<strong>目的端口 (<code>50000</code>)</strong> 以及<strong>源 IP (<code>198.51.100.20</code>)</strong> 到其 NAT 表中查找对应的内部映射。</li>
<li>它找到之前为 PC1 (<code>192.168.1.10:1000</code>) 创建的条目。</li>
<li><strong>修改数据包头部：</strong><ul>
<li><strong>目的 IP：</strong> 从 <code>203.0.113.50</code> 修改为 <code>192.168.1.10</code>。</li>
<li><strong>目的端口：</strong> 从 <code>50000</code> 修改为 <code>1000</code>。</li>
</ul>
</li>
<li>转换后的数据包：<ul>
<li><strong>源 IP：</strong> <code>198.51.100.20</code></li>
<li><strong>源端口：</strong> <code>80</code></li>
<li><strong>目的 IP：</strong> <code>192.168.1.10</code></li>
<li><strong>目的端口：</strong> <code>1000</code></li>
</ul>
</li>
<li>路由器将这个数据包发送到私有网络中的 PC1。</li>
</ul>
</li>
<li><p><strong>PC1 接收响应：</strong></p>
<ul>
<li>PC1 收到响应，其源和目的 IP&#x2F;端口都“看起来”是正常的，因为它不知道中间发生了 NAT 转换。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="NAT-的主要类型："><a href="#NAT-的主要类型：" class="headerlink" title="NAT 的主要类型："></a>NAT 的主要类型：</h3><p>虽然上述描述的是最常见的 PAT，但 NAT 还有其他几种类型，各有其应用场景：</p>
<ol>
<li><p><strong>静态 NAT (Static NAT &#x2F; One-to-One NAT)：</strong></p>
<ul>
<li><strong>原理：</strong> 将<strong>一个内部私有 IP 地址</strong>永久映射到<strong>一个外部公共 IP 地址</strong>。</li>
<li><strong>用途：</strong> 当内部服务器（如 Web 服务器、FTP 服务器）需要从外部互联网访问时使用。每个内部服务器都需要一个专用的公共 IP 地址。</li>
<li><strong>特点：</strong> 外部设备可以直接通过公共 IP 地址访问内部映射的服务器。</li>
</ul>
</li>
<li><p><strong>动态 NAT (Dynamic NAT)：</strong></p>
<ul>
<li><strong>原理：</strong> 维护一个<strong>公共 IP 地址池</strong>。当内部设备需要访问外部网络时，动态地从这个池中分配一个公共 IP 地址进行一对一转换。当连接结束或超时时，该公共 IP 地址被释放回池中。</li>
<li><strong>用途：</strong> 当有多个内部设备，但公共 IP 地址数量有限（但比内部设备数量多）时。</li>
<li><strong>特点：</strong> 仍然是“一对一”的转换，但映射是动态的。</li>
</ul>
</li>
<li><p><strong>端口地址转换 (PAT &#x2F; NAT Overload &#x2F; NAPT)：</strong></p>
<ul>
<li><strong>原理：</strong> 这是最常见的类型。允许<strong>多个私有 IP 地址</strong>共享<strong>同一个公共 IP 地址</strong>。通过使用不同的<strong>源端口号</strong>来区分不同的内部连接。</li>
<li><strong>用途：</strong> 家庭路由器、小型办公室网络，是解决 IPv4 地址枯竭的最有效措施。</li>
<li><strong>特点：</strong> 高效利用公共 IP 地址，安全性较高（外部无法直接发起连接到内部）。</li>
</ul>
</li>
</ol>
<h3 id="NAT-的优点："><a href="#NAT-的优点：" class="headerlink" title="NAT 的优点："></a>NAT 的优点：</h3><ul>
<li><strong>缓解 IPv4 地址枯竭：</strong> 允许多个设备共享少数公共 IP 地址，极大地扩展了 IPv4 地址的使用寿命。</li>
<li><strong>提高安全性：</strong> 默认情况下，内部网络（私有 IP）对外部网络是不可见的，外部设备无法直接发起与内部设备的连接，提供了一层基本的防火墙功能。</li>
<li><strong>网络隐藏性：</strong> 隐藏了内部网络的结构和拓扑细节。</li>
<li><strong>灵活性：</strong> 内部网络可以自由使用私有 IP 地址，无需担心与外部网络的冲突。</li>
</ul>
<h3 id="NAT-的缺点："><a href="#NAT-的缺点：" class="headerlink" title="NAT 的缺点："></a>NAT 的缺点：</h3><ul>
<li><strong>增加了复杂性：</strong> 对网络管理员维护和排除故障带来一定的复杂性。</li>
<li><strong>点对点应用限制：</strong> 某些点对点应用（如 VoIP、在线游戏、P2P 文件共享）可能难以在 NAT 后面工作，因为它们可能需要直接的端到端通信。这通常需要<strong>端口转发 (Port Forwarding)</strong> 来解决。</li>
<li><strong>端到端连接中断：</strong> 从理论上打破了 IP 协议的端到端透明性（数据包的源和目的 IP 地址不再保持不变），这可能对某些高级网络协议和安全机制（如 IPSec）造成影响。</li>
<li><strong>调试困难：</strong> 在日志或错误报告中，所有来自内部私有网络的流量都显示为来自同一个公共 IP 地址，这使得追踪具体是哪个内部设备导致的问题变得困难。</li>
</ul>
<p>尽管存在缺点，但在 IPv4 环境下，NAT 仍然是一种不可或缺且广泛使用的技术。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/NFS%20%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/NFS%20%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8/" class="post-title-link" itemprop="url">NFS 共享存储</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:50:24" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">存储</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>10 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="NFS-共享存储"><a href="#NFS-共享存储" class="headerlink" title="NFS 共享存储"></a>NFS 共享存储</h1><p>前面我们学习了 hostPath 与 Local PV 两种本地存储方式，但是平时我们的应用更多的是无状态服务，可能会同时发布在不同的节点上，这个时候本地存储就不适用了，往往就需要使用到共享存储了，比如最简单常用的网络共享存储 NFS，本节课我们就来介绍下如何在 Kubernetes 下面使用 NFS 共享存储。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>我们这里为了演示方便，先使用相对简单的 NFS 这种存储资源，接下来我们在节点 <code>192.168.31.31</code> 上来安装 NFS 服务，数据目录：<code>/var/lib/k8s/data/</code></p>
<p>关闭防火墙</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl stop firewalld.service</span><br><span class="line">➜ systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>



<p>安装配置 nfs</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ yum -y install nfs-utils rpcbind</span><br></pre></td></tr></table></figure>



<p>共享目录设置权限：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ mkdir -p /var/lib/k8s/data</span><br><span class="line">➜ chmod 755 /var/lib/k8s/data/</span><br></pre></td></tr></table></figure>



<p>配置 nfs，nfs 的默认配置文件在 <code>/etc/exports</code> 文件下，在该文件中添加下面的配置信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ vi /etc/exports</span><br><span class="line">/var/lib/k8s/data  *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure>



<p>配置说明：</p>
<ul>
<li><code>/var/lib/k8s/data</code>：是共享的数据目录</li>
<li>*：表示任何人都有权限连接，当然也可以是一个网段，一个 IP，也可以是域名</li>
<li>rw：读写的权限</li>
<li>sync：表示文件同时写入硬盘和内存</li>
<li>no_root_squash：当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份</li>
</ul>
<p>当然 nfs 的配置还有很多，感兴趣的同学可以在网上去查找一下。</p>
<p>启动服务 nfs 需要向 rpc 注册，rpc 一旦重启了，注册的文件都会丢失，向他注册的服务都需要重启 注意启动顺序，先启动 rpcbind</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl start rpcbind.service</span><br><span class="line">➜ systemctl enable rpcbind</span><br><span class="line">➜ systemctl status rpcbind</span><br><span class="line">● rpcbind.service - RPC bind service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; disabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Tue 2018-07-10 20:57:29 CST; 1min 54s ago</span><br><span class="line">  Process: 17696 ExecStart=/sbin/rpcbind -w $RPCBIND_ARGS (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 17697 (rpcbind)</span><br><span class="line">    Tasks: 1</span><br><span class="line">   Memory: 1.1M</span><br><span class="line">   CGroup: /system.slice/rpcbind.service</span><br><span class="line">           └─17697 /sbin/rpcbind -w</span><br><span class="line"></span><br><span class="line">Jul 10 20:57:29 master systemd[1]: Starting RPC bind service...</span><br><span class="line">Jul 10 20:57:29 master systemd[1]: Started RPC bind service.</span><br></pre></td></tr></table></figure>



<p>看到上面的 Started 证明启动成功了。</p>
<p>然后启动 nfs 服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl start nfs.service</span><br><span class="line">➜ systemctl enable nfs</span><br><span class="line">➜ systemctl status nfs</span><br><span class="line">● nfs-server.service - NFS server and services</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /run/systemd/generator/nfs-server.service.d</span><br><span class="line">           └─order-with-mounts.conf</span><br><span class="line">   Active: active (exited) since Tue 2018-07-10 21:35:37 CST; 14s ago</span><br><span class="line"> Main PID: 32067 (code=exited, status=0/SUCCESS)</span><br><span class="line">   CGroup: /system.slice/nfs-server.service</span><br><span class="line"></span><br><span class="line">Jul 10 21:35:37 master systemd[1]: Starting NFS server and services...</span><br><span class="line">Jul 10 21:35:37 master systemd[1]: Started NFS server and services.</span><br></pre></td></tr></table></figure>



<p>同样看到 Started 则证明 NFS Server 启动成功了。</p>
<p>另外我们还可以通过下面的命令确认下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ rpcinfo -p|grep nfs</span><br><span class="line">    100003    3   tcp   2049  nfs</span><br><span class="line">    100003    4   tcp   2049  nfs</span><br><span class="line">    100227    3   tcp   2049  nfs_acl</span><br><span class="line">    100003    3   udp   2049  nfs</span><br><span class="line">    100003    4   udp   2049  nfs</span><br><span class="line">    100227    3   udp   2049  nfs_acl</span><br></pre></td></tr></table></figure>



<p>查看具体目录挂载权限：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ cat /var/lib/nfs/etab</span><br><span class="line">/var/lib/k8s/data       *(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)</span><br></pre></td></tr></table></figure>



<p>到这里我们就把 nfs server 给安装成功了，然后就是前往节点安装 nfs 的客户端来验证，安装 nfs 当前也需要先关闭防火墙：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl stop firewalld.service</span><br><span class="line">➜ systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>



<p>然后安装 nfs</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ yum -y install nfs-utils rpcbind</span><br></pre></td></tr></table></figure>



<p>安装完成后，和上面的方法一样，先启动 rpc、然后启动 nfs：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl start rpcbind.service</span><br><span class="line">➜ systemctl enable rpcbind.service</span><br><span class="line">➜ systemctl start nfs.service</span><br><span class="line">➜ systemctl enable nfs.service</span><br></pre></td></tr></table></figure>



<p>挂载数据目录 客户端启动完成后，我们在客户端来挂载下 nfs 测试下，首先检查下 nfs 是否有共享目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ showmount -e 192.168.31.31</span><br><span class="line">Export list for 192.168.31.31:</span><br><span class="line">/var/lib/k8s/data *</span><br></pre></td></tr></table></figure>



<p>然后我们在客户端上新建目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ mkdir -p /root/course/kubeadm/data</span><br></pre></td></tr></table></figure>



<p>将 nfs 共享目录挂载到上面的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ mount -t nfs 192.168.31.31:/var/lib/k8s/data /root/course/kubeadm/data</span><br></pre></td></tr></table></figure>



<p>挂载成功后，在客户端上面的目录中新建一个文件，然后我们观察下 nfs 服务端的共享目录下面是否也会出现该文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ touch /root/course/kubeadm/data/test.txt</span><br></pre></td></tr></table></figure>



<p>然后在 nfs 服务端查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ ls -ls /var/lib/k8s/data/</span><br><span class="line">total 4</span><br><span class="line">4 -rw-r--r--. 1 root root 4 Jul 10 21:50 test.txt</span><br></pre></td></tr></table></figure>



<p>如果上面出现了 test.txt 的文件，那么证明我们的 nfs 挂载成功了。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>前面我们已经了解到了 PV、PVC、StorgeClass 的使用，那么我们的 NFS 又应该如何在 Kubernetes 中使用呢？</p>
<p>同样创建一个如下所示 nfs 类型的 PV 资源对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nfs-volume.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/var/lib/k8s/data/</span> <span class="comment"># 指定nfs的挂载点</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.31</span><span class="number">.31</span> <span class="comment"># 指定nfs服务地址</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">manual</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>



<p>我们知道用户真正使用的是 PVC，而要使用 PVC 的前提就是必须要先和某个符合条件的 PV 进行一一绑定，比如存储容器、访问模式，以及 PV 和 PVC 的 storageClassName 字段必须一样，这样才能够进行绑定，当 PVC 和 PV 绑定成功后就可以直接使用这个 PVC 对象了：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nfs-pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-volumes</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">nfs-pvc</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs</span></span><br><span class="line">          <span class="attr">subPath:</span> <span class="string">test-volumes</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">&#x27;/usr/share/nginx/html&#x27;</span></span><br></pre></td></tr></table></figure>



<p>直接创建上面的资源对象即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f volume.yaml</span><br><span class="line">➜ kubectl apply -f pod.yaml</span><br><span class="line">➜ kubectl get pv nfs-pv</span><br><span class="line">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE</span><br><span class="line">nfs-pv   1Gi        RWO            Retain           Bound    default/nfs-pvc   manual                  119s</span><br><span class="line">➜ kubectl get pvc nfs-pvc</span><br><span class="line">NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">nfs-pvc   Bound    nfs-pv   1Gi        RWO            manual         2m5s</span><br><span class="line">➜ kubectl get pods test-volumes -o wide</span><br><span class="line">NAME           READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">test-volumes   1/1     Running   0          2m30s   10.244.2.174   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>



<p>由于我们这里 PV 中的数据为空，所以挂载后会将 nginx 容器中的 <code>/usr/share/nginx/html</code> 目录覆盖，那么访问应用的时候就没有内容了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ curl http://10.244.2.174</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.21.5&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>



<p>我们可以在 PV 目录中添加一些内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 nfs 服务器上面执行</span></span><br><span class="line">➜ echo &quot;nfs pv content&quot; &gt; /var/lib/k8s/data/test-volumes/index.html</span><br><span class="line">➜ curl http://10.244.2.174</span><br><span class="line">nfs pv content</span><br></pre></td></tr></table></figure>



<p>然后重新访问就有数据了，而且当我们的 Pod 应用挂掉或者被删掉重新启动后数据还是存在的，因为数据已经持久化了。</p>
<p>上面的示例中需要我们手动去创建 PV 来和 PVC 进行绑定，有的场景下面需要自动创建 PV，这个时候就需要使用到 StorageClass 了，并且需要一个对应的 provisioner 来自动创建 PV，比如这里我们使用的 NFS 存储，则可以使用 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-subdir-external-provisioner</a> 这个 Provisioner，它使用现有的和已配置的 NFS 服务器来支持通过 PVC 动态配置 PV，持久卷配置为 <code>$&#123;namespace&#125;-$&#123;pvcName&#125;-$&#123;pvName&#125;</code>，首先我们使用 Helm Chart 来安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/</span><br><span class="line">➜ helm upgrade --install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=192.168.31.31 --set nfs.path=/var/lib/k8s/data --set image.repository=cnych/nfs-subdir-external-provisioner --set storageClass.defaultClass=true -n kube-system</span><br></pre></td></tr></table></figure>



<p>上面的命令会在 <code>kube-system</code> 命名空间下安装 <code>nfs-subdir-external-provisioner</code>，并且会创建一个名为 <code>nfs-client</code> 默认的 StorageClass：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get sc</span><br><span class="line">NAME                   PROVISIONER                                     RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">local-storage          kubernetes.io/no-provisioner                    Delete          WaitForFirstConsumer   false                  2d20h</span><br><span class="line">➜ nfs-client (default)   cluster.local/nfs-subdir-external-provisioner   Delete          Immediate              true                   38d</span><br><span class="line">➜ kubectl get sc nfs-client -o yaml</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  ......</span><br><span class="line">  name: nfs-client</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &quot;true&quot;</span><br><span class="line">provisioner: cluster.local/nfs-subdir-external-provisioner</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">volumeBindingMode: Immediate</span><br><span class="line">allowVolumeExpansion: true</span><br></pre></td></tr></table></figure>



<p>这样当以后我们创建的 PVC 中如果没有指定具体的 <code>StorageClass</code> 的时候，则会使用上面的 SC 自动创建一个 PV。比如我们创建一个如下所示的 PVC：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nfs-sc-pvc</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-sc-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># storageClassName: nfs-client  # 不指定则使用默认的 SC</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>



<p>直接创建上面的 PVC 资源对象后就会自动创建一个 PV 与其进行绑定：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc nfs-sc-pvc</span><br><span class="line">NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">nfs-sc-pvc   Bound    pvc-ed8e2fb7-897d-465f-8735-81d52c91d074   1Gi        RWO            nfs-client     15s</span><br></pre></td></tr></table></figure>



<p>对应自动创建的 PV 如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pv pvc-ed8e2fb7-897d-465f-8735-81d52c91d074 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    pv.kubernetes.io/provisioned-by: cluster.local/nfs-subdir-external-provisioner</span><br><span class="line">  creationTimestamp: &quot;2022-02-13T09:44:13Z&quot;</span><br><span class="line">  finalizers:</span><br><span class="line">  - kubernetes.io/pv-protection</span><br><span class="line">  name: pvc-ed8e2fb7-897d-465f-8735-81d52c91d074</span><br><span class="line">  resourceVersion: &quot;3954045&quot;</span><br><span class="line">  uid: 6d66e6ea-888b-4bc0-bab0-9aca3a536cb5</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">  claimRef:</span><br><span class="line">    apiVersion: v1</span><br><span class="line">    kind: PersistentVolumeClaim</span><br><span class="line">    name: nfs-sc-pvc</span><br><span class="line">    namespace: default</span><br><span class="line">    resourceVersion: &quot;3954040&quot;</span><br><span class="line">    uid: ed8e2fb7-897d-465f-8735-81d52c91d074</span><br><span class="line">  nfs:</span><br><span class="line">    path: /var/lib/k8s/data/default-nfs-sc-pvc-pvc-ed8e2fb7-897d-465f-8735-81d52c91d074</span><br><span class="line">    server: 192.168.31.31</span><br><span class="line">  persistentVolumeReclaimPolicy: Delete</span><br><span class="line">  storageClassName: nfs-client</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">status:</span><br><span class="line">  phase: Bound</span><br></pre></td></tr></table></figure>



<p>挂载的 nfs 目录为 <code>/var/lib/k8s/data/default-nfs-sc-pvc-pvc-ed8e2fb7-897d-465f-8735-81d52c91d074</code>，和上面的 <code>$&#123;namespace&#125;-$&#123;pvcName&#125;-$&#123;pvName&#125;</code> 规范一致的。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>我们只是在 volumes 中指定了我们上面创建的 PVC 对象，当这个 Pod 被创建之后， kubelet 就会把这个 PVC 对应的这个 NFS 类型的 Volume（PV）挂载到这个 Pod 容器中的目录中去。前面我们也提到了这样的话对于普通用户来说完全就不用关心后面的具体存储在 NFS 还是 Ceph 或者其他了，只需要直接使用 PVC 就可以了，因为真正的存储是需要很多相关的专业知识的，这样就完全职责分离解耦了。</p>
<p>普通用户直接使用 PVC 没有问题，但是也会出现一个问题，那就是当普通用户创建一个 PVC 对象的时候，这个时候系统里面并没有合适的 PV 来和它进行绑定，因为 PV 大多数情况下是管理员给我们创建的，这个时候启动 Pod 肯定就会失败了，如果现在管理员如果去创建一个对应的 PV 的话，PVC 和 PV 当然就可以绑定了，然后 Pod 也会自动的启动成功，这是因为在 Kubernetes 中有一个专门处理持久化存储的控制器 Volume Controller，这个控制器下面有很多个控制循环，其中一个就是用于 PV 和 PVC 绑定的 <code>PersistentVolumeController</code>。</p>
<p><code>PersistentVolumeController</code> 会不断地循环去查看每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与未绑定的 PVC 进行绑定，这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态。而所谓将一个 PV 与 PVC 进行<strong>绑定</strong>，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 <code>spec.volumeName</code> 字段上。</p>
<p>PV 和 PVC 绑定上了，那么又是如何将容器里面的数据进行持久化的呢，我们知道 Docker 的 Volume 挂载其实就是<strong>将一个宿主机上的目录和一个容器里的目录绑定挂载在了一起</strong>，具有持久化功能当然就是指的宿主机上面的这个目录了，当容器被删除或者在其他节点上重建出来以后，这个目录里面的内容依然存在，所以一般情况下实现持久化是需要一个远程存储的，比如 NFS、Ceph 或者云厂商提供的磁盘等等。所以接下来需要做的就是持久化宿主机目录这个过程。</p>
<p>当 Pod 被调度到一个节点上后，节点上的 kubelet 组件就会为这个 Pod 创建它的 Volume 目录，默认情况下 kubelet 为 Volume 创建的目录在 kubelet 工作目录下面：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure>



<p>比如上面我们创建的 Pod 对应的 Volume 目录完整路径为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/kubelet/pods/d4fcdb11-baf7-43d9-8d7d-3ede24118e08/volumes/kubernetes.io~nfs/nfs-pv</span><br></pre></td></tr></table></figure>



<p>!!! info “提示” 要获取 Pod 的唯一标识 uid，可通过命令 <code>kubectl get pod pod名 -o jsonpath=&#123;.metadata.uid&#125;</code> 获取。</p>
<p>然后就需要根据我们的 Volume 类型来决定需要做什么操作了，假如后端存储使用的 Ceph RBD，那么 kubelet 就需要先将 Ceph 提供的 RBD 挂载到 Pod 所在的宿主机上面，这个阶段在 Kubernetes 中被称为 <strong>Attach 阶段</strong>。Attach 阶段完成后，为了能够使用这个块设备，kubelet 还要进行第二个操作，即：<strong>格式化</strong>这个块设备，然后将它<strong>挂载</strong>到宿主机指定的挂载点上。这个挂载点，也就是上面我们提到的 Volume 的宿主机的目录。将块设备格式化并挂载到 Volume 宿主机目录的操作，在 Kubernetes 中被称为 <strong>Mount 阶段</strong>。但是对于我们这里使用的 NFS 就更加简单了， 因为 NFS 存储并没有一个设备需要挂载到宿主机上面，所以这个时候 kubelet 就会直接进入第二个 <code>Mount</code> 阶段，相当于直接在宿主机上面执行如下的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount -t nfs 192.168.31.31:/var/lib/k8s/data/ /var/lib/kubelet/pods/d4fcdb11-baf7-43d9-8d7d-3ede24118e08/volumes/kubernetes.io~nfs/nfs-pv</span><br></pre></td></tr></table></figure>



<p>同样可以在测试的 Pod 所在节点查看 Volume 的挂载信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ findmnt /var/lib/kubelet/pods/d4fcdb11-baf7-43d9-8d7d-3ede24118e08/volumes/kubernetes.io~nfs/nfs-pv</span><br><span class="line">TARGET                                                                               SOURCE                 FSTYPE OPTIONS</span><br><span class="line">/var/lib/kubelet/pods/d4fcdb11-baf7-43d9-8d7d-3ede24118e08/volumes/kubernetes.io~nfs/nfs-pv</span><br><span class="line">                                                                                     192.168.31.31:/var/lib/k8s/data/ nfs4   rw,relatime,</span><br></pre></td></tr></table></figure>



<p>我们可以看到这个 Volume 被挂载到了 NFS（192.168.31.31:&#x2F;var&#x2F;lib&#x2F;k8s&#x2F;data&#x2F;）下面，以后我们在这个目录里写入的所有文件，都会被保存在远程 NFS 服务器上。</p>
<p>这样在经过了上面的阶段过后，我们就得到了一个持久化的宿主机上面的 Volume 目录了，接下来 kubelet 只需要把这个 Volume 目录挂载到容器中对应的目录即可，这样就可以为 Pod 里的容器挂载这个持久化的 Volume 了，这一步其实也就相当于执行了如下所示的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">docker 或者 nerdctl</span></span><br><span class="line">docker run -v /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;:/&lt;容器内的目标目录&gt; 我的镜像 ...</span><br></pre></td></tr></table></figure>



<p>整个存储的架构可以用下图来说明： <img data-src="https://mudutestmenu.mudu.tv/upload/3ov1bv.png" alt="存储架构"></p>
<ul>
<li>PV Controller：负责 PV&#x2F;PVC 的绑定，并根据需求进行数据卷的 Provision&#x2F;Delete 操作</li>
<li>AD Controller：负责存储设备的 Attach&#x2F;Detach 操作，将设备挂载到目标节点</li>
<li>Volume Manager：管理卷的 Mount&#x2F;Unmount 操作、卷设备的格式化等操作</li>
<li>Volume Plugin：扩展各种存储类型的卷管理能力，实现第三方存储的各种操作能力和 Kubernetes 存储系统结合</li>
</ul>
<p>我们上面使用的 NFS 就属于 <code>In-Tree</code> 这种方式，<code>In-Tree</code> 就是在 Kubernetes 源码内部实现的，和 Kubernetes 一起发布、管理的，但是更新迭代慢、灵活性比较差，另外一种方式 <code>Out-Of-Tree</code> 是独立于 Kubernetes 的，目前主要有 <code>CSI</code> 和 <code>FlexVolume</code> 两种机制，开发者可以根据自己的存储类型实现不同的存储插件接入到 Kubernetes 中去，其中 <code>CSI</code> 是现在也是以后主流的方式，接下来我们会主要介绍 <code>CSI</code> 这种存储插件的使用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Longhorn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Longhorn/" class="post-title-link" itemprop="url">Longhorn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:48:54" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E5%AD%98%E5%82%A8/" itemprop="url" rel="index"><span itemprop="name">存储</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>32k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>29 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Longhorn"><a href="#Longhorn" class="headerlink" title="Longhorn"></a>Longhorn</h1><p>前面我们学习了本地存储、NFS 共享存储，除了这些存储类型之外，还有一个块存储，同样为 Kubernetes 提供块存储的方案有很多，比如 Ceph RBD，今天我们为大家介绍的是 Rancher 开源的一款 Kubernetes 的云原生分布式块存储方案 - Longhorn。</p>
<p>使用 Longhorn，可以：</p>
<ul>
<li>使用 Longhorn 卷作为 Kubernetes 集群中分布式有状态应用程序的持久存储</li>
<li>将你的块存储分区为 Longhorn 卷，以便你可以在有或没有云提供商的情况下使用 Kubernetes 卷</li>
<li>跨多个节点和数据中心复制块存储以提高可用性</li>
<li>将备份数据存储在 NFS 或 AWS S3 等外部存储中</li>
<li>创建跨集群灾难恢复卷，以便可以从第二个 Kubernetes 集群中的备份中快速恢复主 Kubernetes 集群中的数据</li>
<li>调度一个卷的快照，并将备份调度到 NFS 或 S3 兼容的二级存储</li>
<li>从备份还原卷</li>
<li>不中断持久卷的情况下升级 Longhorn</li>
</ul>
<p>Longhorn 还带有独立的 UI，可以使用 Helm、kubectl 或 Rancher 应用程序目录进行安装。</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>Longhorn 为每个卷创建一个专用的存储控制器，并在多个节点上存储的多个副本之间同步复制该卷。Longhorn 在整体上分为两层：<strong>数据平面和控制平面</strong>，Longhorn Engine 是存储控制器，对应数据平面，Longhorn Manager 对应控制平面。</p>
<p>Longhorn Manager 会以 DaemonSet 的形式在 Longhorn 集群中的每个节点上运行，它负责在 Kubernetes 集群中创建和管理卷，并处理来自 UI 或 Kubernetes 卷插件的 API 调用，它是遵循 Kubernetes 控制器模式。</p>
<p>Longhorn Manager 通过与 Kubernetes APIServer 通信来创建新的 Longhorn volume CRD，然后 Longhorn Manager 会一直 Watch APIServer 的响应，当它看到发现创建了一个新的 Longhorn volume CRD 时，Longhorn Manager 就会去创建一个新的对应卷。当 Longhorn Manager 被要求创建一个卷时，它会在卷所连接的节点上创建一个 Longhorn Engine 实例，并在每个将放置副本的节点上创建一个副本，副本应放置在不同的主机上以确保最大可用性。副本的多条数据路径确保了 Longhorn 卷的高可用性，即使某个副本或引擎出现问题，也不会影响所有副本或 Pod 对卷的访问。</p>
<p>Longhorn Engine 始终与使用 Longhorn 卷的 Pod 在同一节点中运行，它在存储在多个节点上的多个副本之间同步复制卷。</p>
<p>如下图所示，描述了 Longhorn 卷、Longhorn Engine、副本实例和磁盘之间的读&#x2F;写数据流:</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/recuan.png" alt="卷、Longhorn Engine、副本实例和磁盘之间的读/写数据流"></p>
<ul>
<li>上图中有 3 个 Longhorn 卷实例</li>
<li>每个卷都有一个专用控制器，称为 Longhorn Engine，并作为 Linux 进程运行</li>
<li>每个 Longhorn 卷有两个副本，每个副本也是一个 Linux 进程</li>
<li>图中的箭头表示卷、控制器实例、副本实例和磁盘之间的读&#x2F;写数据流</li>
<li>通过为每个卷创建单独的 Longhorn Engine，如果一个控制器发生故障，其他卷的功能不会受到影响</li>
</ul>
<blockquote>
<p>注意: 图中的 Engine 并非是单独的一个 Pod，而是每一个 Volume 会对应一个 golang exec 出来的 Linux 进程</p>
</blockquote>
<p>在 Longhorn 中，每个 Engine 只需要服务一个卷，简化了存储控制器的设计，由于控制器软件的故障域与单个卷隔离，因此控制器崩溃只会影响一个卷。由于 Longhorn Engine 足够简单和轻便，因此我们可以创建多达 100000 个独立的 Engine，Kubernetes 去调度这些独立的 Engine，从一组共享的磁盘中提取资源，并与 Longhorn 合作形成一个弹性的分布式块存储系统。</p>
<p>因为每个卷都有自己的控制器，所以每个卷的控制器和副本实例也可以升级，而不会导致 IO 操作明显中断。Longhorn 可以创建一个长时间运行的 job 任务来协调所有卷的升级，而不会中断系统的运行。</p>
<p>Longhorn 是通过 CSI 驱动在 Kubernetes 中管理的，CSI 驱动通过调用 Longhorn 来创建卷，为 Kubernetes 工作负载创建持久性数据，CSI 插件可以让我们创建、删除、附加、分离、挂载卷，并对卷进行快照操作，Kubernetes 集群内部使用 CSI 接口与 Longhorn CSI 驱动进行通信，而 Longhorn CSI 驱动是通过使用 Longhorn API 与 Longhorn Manager 进行通信。</p>
<p>此外 Longhorn 还提供一个 UI 界面程序，通过 Longhorn API 与 Longhorn Manager 进行交互，通过 Longhorn UI 可以管理快照、备份、节点和磁盘等，此外，集群工作节点的空间使用情况还可以通过 Longhorn UI 查看。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>要在 Kubernetes 集群上安装 Longhorn，需要集群的每个节点都必须满足以下要求：</p>
<ul>
<li>与 Kubernetes 兼容的容器运行时（Docker v1.13+、containerd v1.3.7+ 等）</li>
<li>Kubernetes v1.18+</li>
<li>安装 <code>open-iscsi</code>，并且 <code>iscsid</code> 守护程序在所有节点上运行，这是必要的，因为 Longhorn 依赖主机上的 <code>iscsiadm</code> 为 Kubernetes 提供持久卷</li>
<li>RWX 支持需要每个节点上都安装 NFSv4 客户端</li>
<li>宿主机文件系统支持 <code>file extents</code> 功能来存储数据，目前我们支持：ext4 与 XFS</li>
<li>bash、curl、findmnt、grep、awk、blkid、lsblk 等工具必须安装</li>
<li><code>Mount propagation</code> 必须启用，它允许将一个容器挂载的卷与同一 pod 中的其他容器共享，甚至可以与同一节点上的其他 pod 共享</li>
</ul>
<p>Longhorn workloads 必须能够以 root 身份运行才能正确部署和操作 Longhorn。</p>
<h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><p>为了验证这些环境要求，Longhorn 官方提供了一个脚本来帮助我们进行检查，执行该脚本需要在本地安装 <code>jq</code> 工具，执行下面的命令即可运行脚本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜ curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.2.3/scripts/environment_check.sh | bash</span><br><span class="line">daemonset.apps/longhorn-environment-check created</span><br><span class="line">waiting for pods to become ready (0/2)</span><br><span class="line">waiting for pods to become ready (0/2)</span><br><span class="line">all pods ready (2/2)</span><br><span class="line"></span><br><span class="line">  MountPropagation is enabled!</span><br><span class="line"></span><br><span class="line">cleaning up...</span><br><span class="line">daemonset.apps &quot;longhorn-environment-check&quot; deleted</span><br><span class="line">clean up complete</span><br></pre></td></tr></table></figure>



<p>如果没有检查通过会给出相关的提示信息。</p>
<p>要安装 <code>open-iscsi</code>，可以直接使用下面的命令即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">apt-get install open-iscsi  <span class="comment"># Debian 和 Ubuntu 系统命令</span></span></span><br><span class="line">➜ yum install -y iscsi-initiator-utils</span><br></pre></td></tr></table></figure>



<p>Longhorn 官方还为我们还提供了一个 iscsi 安装程序，可以更轻松地自动安装 <code>open-iscsi</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.2.3/deploy/prerequisite/longhorn-iscsi-installation.yaml</span><br></pre></td></tr></table></figure>



<p>部署完成后，运行以下命令来检查安装程序的 pod 状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pod | grep longhorn-iscsi-installation</span><br><span class="line">longhorn-iscsi-installation-49hd7   1/1     Running   0          21m</span><br><span class="line">longhorn-iscsi-installation-pzb7r   1/1     Running   0          39m</span><br></pre></td></tr></table></figure>



<p>也可以通过以下命令查看日志，查看安装结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl logs longhorn-iscsi-installation-pzb7r -c iscsi-installation</span><br><span class="line">...</span><br><span class="line">Installed:</span><br><span class="line">  iscsi-initiator-utils.x86_64 0:6.2.0.874-7.amzn2</span><br><span class="line"></span><br><span class="line">Dependency Installed:</span><br><span class="line">  iscsi-initiator-utils-iscsiuio.x86_64 0:6.2.0.874-7.amzn2</span><br><span class="line"></span><br><span class="line">Complete!</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/iscsid.service to /usr/lib/systemd/system/iscsid.service.</span><br><span class="line">iscsi install successfully</span><br></pre></td></tr></table></figure>



<p>同样要安装 NFSv4 客户端，可以直接使用下面的命令一键安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">apt-get install nfs-common  <span class="comment">#  Debian 和 Ubuntu 系统命令</span></span></span><br><span class="line">➜ yum install nfs-utils</span><br></pre></td></tr></table></figure>



<p>同样 Longhorn 官方也提供了一个 nfs 客户端安装程序，可以更轻松地自动安装 nfs-client：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.2.3/deploy/prerequisite/longhorn-nfs-installation.yaml</span><br></pre></td></tr></table></figure>



<p>部署完成后，运行以下命令来检查安装程序的 pod 状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pod | grep longhorn-nfs-installation</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">longhorn-nfs-installation-t2v9v   1/1     Running   0          143m</span><br><span class="line">longhorn-nfs-installation-7nphm   1/1     Running   0          143m</span><br></pre></td></tr></table></figure>



<p>也可以通过以下命令查看日志，查看安装结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl logs longhorn-nfs-installation-t2v9v -c nfs-installation</span><br><span class="line">...</span><br><span class="line">nfs install successfully</span><br></pre></td></tr></table></figure>



<p>相关依赖环境准备好过后就可以开始安装 Longhorn 了。</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>官方支持使用 Rancher Catalog 应用、kubectl 与 helm 三种方式来进行安装，同样这里我们选择使用 helm 进行安装。</p>
<p>首先添加 longhorn 的 chart 仓库：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ helm repo add longhorn https://charts.longhorn.io</span><br><span class="line">➜ helm repo update</span><br></pre></td></tr></table></figure>



<p>然后可以根据自己的实际场景定制 values 文件，可以通过下面的命令获取默认的 values 文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ curl -Lo values.yaml https://raw.githubusercontent.com/longhorn/charts/master/charts/longhorn/values.yaml</span><br></pre></td></tr></table></figure>



<p>然后可以修改 values 文件中的配置，longhorn 推荐单独挂盘作为存储使用，这里作为测试直接使用默认的 <code>/var/lib/longhorn</code> 目录。</p>
<p>如下所示默认配置的示例片段：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">defaultSettings:</span></span><br><span class="line">  <span class="attr">backupTarget:</span> <span class="string">s3://backupbucket@us-east-1/backupstore</span></span><br><span class="line">  <span class="attr">backupTargetCredentialSecret:</span> <span class="string">minio-secret</span></span><br><span class="line">  <span class="attr">createDefaultDiskLabeledNodes:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">defaultDataPath:</span> <span class="string">/var/lib/longhorn-example/</span></span><br><span class="line">  <span class="attr">replicaSoftAntiAffinity:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">storageOverProvisioningPercentage:</span> <span class="number">600</span></span><br><span class="line">  <span class="attr">storageMinimalAvailablePercentage:</span> <span class="number">15</span></span><br><span class="line">  <span class="attr">upgradeChecker:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">defaultReplicaCount:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">defaultDataLocality:</span> <span class="string">disabled</span></span><br><span class="line">  <span class="attr">guaranteedEngineCPU:</span></span><br><span class="line">  <span class="attr">defaultLonghornStaticStorageClass:</span> <span class="string">longhorn-static-example</span></span><br><span class="line">  <span class="attr">backupstorePollInterval:</span> <span class="number">500</span></span><br><span class="line">  <span class="attr">taintToleration:</span> <span class="string">key1=value1:NoSchedule;</span> <span class="string">key2:NoExecute</span></span><br><span class="line">  <span class="attr">systemManagedComponentsNodeSelector:</span> <span class="string">&#x27;label-key1:label-value1&#x27;</span></span><br><span class="line">  <span class="attr">priority-class:</span> <span class="string">high-priority</span></span><br><span class="line">  <span class="attr">autoSalvage:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">disableSchedulingOnCordonedNode:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">replicaZoneSoftAntiAffinity:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">volumeAttachmentRecoveryPolicy:</span> <span class="string">never</span></span><br><span class="line">  <span class="attr">nodeDownPodDeletionPolicy:</span> <span class="string">do-nothing</span></span><br><span class="line">  <span class="attr">mkfsExt4Parameters:</span> <span class="string">-O</span> <span class="string">^64bit,^metadata_csum</span></span><br><span class="line">  <span class="attr">guaranteed-engine-manager-cpu:</span> <span class="number">15</span></span><br><span class="line">  <span class="attr">guaranteed-replica-manager-cpu:</span> <span class="number">15</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ingress:</span> <span class="comment"># 开启ingress</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">ingressClassName:</span> <span class="string">nginx</span> <span class="comment"># 配置 ingressclass</span></span><br><span class="line">  <span class="attr">host:</span> <span class="string">longhorn.k8s.local</span></span><br><span class="line">  <span class="attr">annotations:</span> <span class="comment"># 添加annotations</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/proxy-body-size:</span> <span class="string">10000m</span></span><br><span class="line"><span class="attr">enablePSP:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">persistence:</span></span><br><span class="line">  <span class="attr">defaultClass:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">defaultFsType:</span> <span class="string">ext4</span></span><br><span class="line">  <span class="attr">defaultClassReplicaCount:</span> <span class="number">2</span> <span class="comment"># 配置成节点数</span></span><br></pre></td></tr></table></figure>



<p>然后执行下面的命令一键安装 Longhorn：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ helm upgrade --install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace -f values.yaml</span><br><span class="line">NAME: longhorn</span><br><span class="line">LAST DEPLOYED: Sun Feb 20 16:14:05 2022</span><br><span class="line">NAMESPACE: longhorn-system</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">NOTES:</span><br><span class="line">Longhorn is now installed on the cluster!</span><br><span class="line"></span><br><span class="line">Please wait a few minutes for other Longhorn components such as CSI deployments, Engine Images, and Instance Managers to be initialized.</span><br><span class="line"></span><br><span class="line">Visit our documentation at https://longhorn.io/docs/</span><br></pre></td></tr></table></figure>



<p>部署后可以查看 Pod 的运行状态来确保安装正确：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -n longhorn-system</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">csi-attacher-5f46994f7-fqntq                1/1     Running   0          33s</span><br><span class="line">csi-attacher-5f46994f7-ltxg8                1/1     Running   0          36m</span><br><span class="line">csi-attacher-5f46994f7-vw75d                1/1     Running   0          36m</span><br><span class="line">csi-provisioner-6ccbfbf86f-bvc99            1/1     Running   0          33s</span><br><span class="line">csi-provisioner-6ccbfbf86f-k46hn            1/1     Running   0          36m</span><br><span class="line">csi-provisioner-6ccbfbf86f-lxm8h            1/1     Running   0          36m</span><br><span class="line">csi-resizer-6dd8bd4c97-52gmm                1/1     Running   0          35m</span><br><span class="line">csi-resizer-6dd8bd4c97-9btj6                1/1     Running   0          3s</span><br><span class="line">csi-resizer-6dd8bd4c97-fdjmp                1/1     Running   0          35m</span><br><span class="line">csi-snapshotter-86f65d8bc-5mjk2             1/1     Running   0          33s</span><br><span class="line">csi-snapshotter-86f65d8bc-5rrfs             1/1     Running   0          35m</span><br><span class="line">csi-snapshotter-86f65d8bc-bg6nv             1/1     Running   0          35m</span><br><span class="line">engine-image-ei-fa2dfbf0-jrb2d              1/1     Running   0          36m</span><br><span class="line">engine-image-ei-fa2dfbf0-m5799              1/1     Running   0          36m</span><br><span class="line">instance-manager-e-051171e6                 1/1     Running   0          36m</span><br><span class="line">instance-manager-e-db94b4b7                 1/1     Running   0          24m</span><br><span class="line">instance-manager-r-dd84ad5c                 1/1     Running   0          36m</span><br><span class="line">instance-manager-r-f5eefb8a                 1/1     Running   0          24m</span><br><span class="line">longhorn-csi-plugin-mljt2                   2/2     Running   0          35m</span><br><span class="line">longhorn-csi-plugin-rfzcj                   2/2     Running   0          24m</span><br><span class="line">longhorn-driver-deployer-6db849975f-dh4p4   1/1     Running   0          58m</span><br><span class="line">longhorn-manager-bxks6                      1/1     Running   0          24m</span><br><span class="line">longhorn-manager-tj58k                      1/1     Running   0          2m50s</span><br><span class="line">longhorn-ui-6f547c964-k56xr                 1/1     Running   0          58m</span><br></pre></td></tr></table></figure>



<p>上面是部署完成后运行的 Pod，这里可以对这些工作负载做一个简单的说明：</p>
<ul>
<li><code>csi-attacher-xxx</code>、<code>csi-provisioner-xxx</code>、<code>csi-resizer-xxx</code>、<code>csi-snapshotter-xxx</code> 是 csi 原生的组件</li>
<li><code>longhorn-manager-xxx</code> 是运行在每个节点上的 Longhorn Manager，是一个控制器，也为 Longhorn UI 或者 CSI 插件提供 API，主要功能是通过修改 Kubernetes CRD 来触发控制循环，比如 volume attach&#x2F;detach 操作</li>
<li><code>longhorn-ui-xxx</code> 提供 Longhorn UI 服务，提供一个可视化的控制页面</li>
<li>Longhorn Engine 数据平面，提供两种工作模式：Engine Mode（<code>instance-manager-e-xxx</code> 的 Pod）、Replica Mode（<code>instance-manager-r-xxx</code> 的 Pod），Replica 负责实际数据的写入，每个副本包含数据的完整副本，Engine 连接到副本实现 volume 的数据平面，任何写操作都会同步到所有副本，读操作从任意一个副本读取数据</li>
</ul>
<p>由于上面安装的时候我们添加了 Ingress 支持，所以可以通过配置的域名去访问 Longhorn UI：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get ingress  -n longhorn-system</span><br><span class="line">NAME               CLASS   HOSTS                ADDRESS         PORTS   AGE</span><br><span class="line">longhorn-ingress   nginx   longhorn.k8s.local   192.168.31.31   80      4m11s</span><br></pre></td></tr></table></figure>



<p>这里我们使用的 ingress-nginx 这个控制器，安装完成后在浏览器中直接访问 <code>http://longhorn.k8s.local</code> 即可，Longhorn UI 界面中展示了当前存储系统的状态。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/uozcf3.png" alt="Longhorn UI"></p>
<p>关于存储的几种状态：</p>
<ul>
<li><code>Schedulable</code>: 可用于 Longhorn 卷调度的实际空间(actual space)</li>
<li><code>Reserved</code>: 为其他应用程序和系统保留的空间(space reserved)</li>
<li><code>Used</code>: Longhorn、系统和其他应用程序已使用的实际空间(space reserved)</li>
<li><code>Disabled</code>: 不允许调度 Longhorn 卷的磁盘&#x2F;节点的总空间</li>
</ul>
<p>在 Node 页面，Longhorn 会显示每个节点的空间分配、调度和使用信息：</p>
<p><img data-src="https://picdn.youdianzhishi.com/images/20220222150810.png" alt="Node 页面"></p>
<ul>
<li>Size 列：Longhorn 卷可以使用的最大实际可用空间，它等于节点的总磁盘空间减去保留空间。</li>
<li>Allocated 列：左边的数字是**卷调度(volume scheduling)**已使用的大小，并不代表该空间已被用于 Longhorn 卷数据存储。正确的数字是卷调度的 max 大小，它是 Size 乘以 Storage Over Provisioning Percentage 的结果，因此，这两个数字之间的差异（我们称之为可分配空间 allocable space）决定了卷副本是否可以调度到这个节点。</li>
<li>Used 列：左边部分表示该节点当前使用的空间，整个条形表示节点的总空间。</li>
</ul>
<p>此外还会创建一个默认的 StorageClass 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get sc longhorn</span><br><span class="line">NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   91m</span><br><span class="line">➜ kubectl get sc longhorn -o yaml</span><br><span class="line">allowVolumeExpansion: true</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    ......</span><br><span class="line">    storageclass.kubernetes.io/is-default-class: &quot;true&quot;</span><br><span class="line">  creationTimestamp: &quot;2022-02-20T09:32:51Z&quot;</span><br><span class="line">  ......</span><br><span class="line">  name: longhorn</span><br><span class="line">  resourceVersion: &quot;4524911&quot;</span><br><span class="line">  uid: 6066e858-e7ab-4dab-95db-7ff829e6e01b</span><br><span class="line">parameters:</span><br><span class="line">  fromBackup: &quot;&quot;</span><br><span class="line">  fsType: ext4</span><br><span class="line">  numberOfReplicas: &quot;3&quot;</span><br><span class="line">  staleReplicaTimeout: &quot;30&quot;</span><br><span class="line">provisioner: driver.longhorn.io</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">volumeBindingMode: Immediate</span><br></pre></td></tr></table></figure>



<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>下面我们来测试使用 longhorn 提供一个存储卷，由于提供了默认的 StorageClass，所以直接创建 PVC 即可，创建一个如下所示的 PVC：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>



<p>然后部署一个 mysql 应用来使用上面的 PVC 进行数据持久化：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">mysql</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">mysql:5.6</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">password</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">3306</span></span><br><span class="line">              <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/var/lib/mysql</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">mysql-pvc</span></span><br></pre></td></tr></table></figure>



<p>直接创建上面的资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-pvc</span><br><span class="line">NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-pvc   Bound    pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307   1Gi        RWO            longhorn       8s</span><br><span class="line">➜ kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS      AGE</span><br><span class="line">mysql-6879698bd4-r8cxz   1/1     Running   0             3m10s</span><br><span class="line">➜ kubectl exec -it mysql-6879698bd4-r8cxz -- mysql -uroot -ppassword</span><br><span class="line">Warning: Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.6.51 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">create database longhorn;</span></span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span></span><br></pre></td></tr></table></figure>



<p>应用启动成功后我们可以去节点上查看数据来验证是否成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ ls /var/lib/longhorn/</span><br><span class="line">engine-binaries  longhorn-disk.cfg  replicas</span><br><span class="line">➜ ls /var/lib/longhorn/replicas/</span><br><span class="line">pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307-c40376c5</span><br><span class="line">➜ ls /var/lib/longhorn/replicas/pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307-c40376c5</span><br><span class="line">revision.counter  volume-head-000.img  volume-head-000.img.meta  volume.meta</span><br></pre></td></tr></table></figure>



<p>需要注意的是 longhorn 是分布式块存储，与分布式文件系统不同，不能超过 pv 设置的存储大小（上例中为 1G）。我们在数据库中创建了一个名为 <code>longhorn</code> 的数据库，然后我们重建 Pod 再次查看数据是否依然存在：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS      AGE</span><br><span class="line">mysql-6879698bd4-s8tfv   1/1     Running   0             6s</span><br><span class="line">➜ kubectl exec -it mysql-6879698bd4-s8tfv -- mysql -uroot -ppassword</span><br><span class="line">Warning: Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1</span><br><span class="line">Server version: 5.6.51 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">show databases;</span></span><br><span class="line">+---------------------+</span><br><span class="line">| Database            |</span><br><span class="line">+---------------------+</span><br><span class="line">| information_schema  |</span><br><span class="line">| longhorn            |</span><br><span class="line">| #mysql50#lost+found |</span><br><span class="line">| mysql               |</span><br><span class="line">| performance_schema  |</span><br><span class="line">+---------------------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt;</span></span><br></pre></td></tr></table></figure>



<p>可以看到前面创建的数据库依然存在，证明我们的数据持久化成功了。在 Longhorn UI 界面中也可以看到数据卷的信息：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/vffmth.png" alt="volume"></p>
<h2 id="备份恢复"><a href="#备份恢复" class="headerlink" title="备份恢复"></a>备份恢复</h2><p>Longhorn 提供了备份恢复功能，要使用这个功能我们需要给卷创建一个 <code>snapshot</code> 快照，快照是 Kubernetes Volume 在任何指定时间点的状态。</p>
<p>在 Longhorn UI 的 Volume 页面中点击要创建快照的卷，进入卷的详细信息页面，点击下方的 <code>Take Snapshot</code> 按钮即可创建快照了，创建快照后，将在卷头(Volume Head)之前的快照列表中可以看到它，比如这里我们会前面测试使用的 <code>mysql</code> 卷创建一个快照：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/w0805c.png" alt="创建快照"></p>
<p>同样在节点的数据目录下面也可以看到创建的快照数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ tree /var/lib/longhorn/replicas/pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307-fbf72396/</span><br><span class="line">/var/lib/longhorn/replicas/pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307-fbf72396/</span><br><span class="line">├── revision.counter</span><br><span class="line">├── volume-head-002.img</span><br><span class="line">├── volume-head-002.img.meta</span><br><span class="line">├── volume.meta</span><br><span class="line">├── volume-snap-3b1f877b-24ba-44ec-808e-ab8d4b15f8dd.img</span><br><span class="line">├── volume-snap-3b1f877b-24ba-44ec-808e-ab8d4b15f8dd.img.meta</span><br><span class="line">├── volume-snap-5d403e8e-65e8-46d1-aa54-70aa3280dac4.img</span><br><span class="line">└── volume-snap-5d403e8e-65e8-46d1-aa54-70aa3280dac4.img.meta</span><br><span class="line"></span><br><span class="line">0 directories, 8 files</span><br></pre></td></tr></table></figure>



<p>其中的 <code>volume-snap-xxx</code> 后面的数据和页面上的快照名称是一致的，比如页面中我们刚刚创建的快照名称为 <code>3b1f877b-24ba-44ec-808e-ab8d4b15f8dd</code>，其中的 <code>img</code> 文件是镜像文件，而 <code>img.meta</code> 是保存当前快照的元信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ cat volume-snap-3b1f877b-24ba-44ec-808e-ab8d4b15f8dd.img.meta</span><br><span class="line">&#123;&quot;Name&quot;:&quot;volume-head-001.img&quot;,&quot;Parent&quot;:&quot;volume-snap-5d403e8e-65e8-46d1-aa54-70aa3280dac4.img&quot;,&quot;Removed&quot;:false,&quot;UserCreated&quot;:true,&quot;Created&quot;:&quot;2022-02-22T07:36:48Z&quot;,&quot;Labels&quot;:null&#125;</span><br></pre></td></tr></table></figure>



<p>元信息里面包含父级的文件镜像，这其实表明快照是增量的快照。</p>
<p>此外除了手动创建快照之外，从 Longhorn UI 上还可以进行周期性快照和备份，同样在卷的详细页面可以进行配置，在 <code>Recurring Jobs Schedule</code> 区域点击 <code>Add</code> 按钮即可创建一个定时的快照。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/dmvziu.png" alt="定时快照"></p>
<p>创建任务的时候可以选择任务类型是备份(backup)或快照(snapshot)，任务的时间以 CRON 表达式的形式进行配置，还可以配置要保留的备份或快照数量以及标签。</p>
<p>为了避免当卷长时间没有新数据时，<code>recurring jobs</code> 可能会用相同的备份和空快照覆盖旧的备份&#x2F;快照的问题，Longhorn 执行以下操作：</p>
<ul>
<li><code>Recurring backup job</code> 仅在自上次备份以来卷有新数据时才进行新备份</li>
<li><code>Recurring snapshot job</code> 仅在卷头(volume head)中有新数据时才拍摄新快照</li>
</ul>
<p>此外我们还可以通过使用 Kubernetes 的 StorageClass 来配置定时快照，可以通过 StorageClass 的 <code>recurringJobs</code> 参数配置定时备份和快照，<code>recurringJobs</code> 字段应遵循以下 JSON 格式：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">longhorn</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">driver.longhorn.io</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">numberOfReplicas:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line">  <span class="attr">staleReplicaTimeout:</span> <span class="string">&quot;30&quot;</span></span><br><span class="line">  <span class="attr">fromBackup:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">recurringJobs:</span> <span class="string">&#x27;[</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;name&quot;:&quot;snap&quot;,</span></span><br><span class="line"><span class="string">      &quot;task&quot;:&quot;snapshot&quot;,</span></span><br><span class="line"><span class="string">      &quot;cron&quot;:&quot;*/1 * * * *&quot;,</span></span><br><span class="line"><span class="string">      &quot;retain&quot;:1</span></span><br><span class="line"><span class="string">    &#125;,</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">      &quot;name&quot;:&quot;backup&quot;,</span></span><br><span class="line"><span class="string">      &quot;task&quot;:&quot;backup&quot;,</span></span><br><span class="line"><span class="string">      &quot;cron&quot;:&quot;*/2 * * * *&quot;,</span></span><br><span class="line"><span class="string">      &quot;retain&quot;:1</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">  ]&#x27;</span></span><br></pre></td></tr></table></figure>



<p>应为每个 recurring job 指定以下参数：</p>
<ul>
<li>name：认为的名称，不要在一个 <code>recurringJobs</code> 中使用重复的名称，并且 name 的长度不能超过 8 个字符</li>
<li>task：任务的类型，它仅支持 snapshot 或 backup</li>
<li>cron：Cron 表达式，指定任务的执行时间</li>
<li>retain：Longhorn 将为一项任务保留多少快照&#x2F;备份，不少于 1</li>
</ul>
<p>使用这个 StorageClass 创建的任何卷都将自动配置上这些 <code>recurring jobs</code>。</p>
<p>要备份卷就需要在 Longhorn 中配置一个备份目标，可以是一个 NFS 服务或者 S3 兼容的对象存储服务，用于存储 Longhorn 卷的备份数据，备份目标可以在 <code>Settings/General/BackupTarget</code> 中配置，我们这里使用 Helm Chart 安装的，最好的方式是去定制 values 文件中的 <code>defaultSettings.backupTarget</code>，当然也可以直接去通过 Longhorn UI 进行配置，比如这里我们先配置备份目标为 nfs 服务，<code>Backup Target</code> 值设置为 <code>nfs://192.168.31.31:/var/lib/k8s/data</code>（要确保目录存在），<code>Backup Target Credential Secret</code> 留空即可，然后拉到最下面点击 <code>Save</code>：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/tckkaa.png" alt="backup 配置"></p>
<p>备份目标配置后，就可以开始备份了，同样导航到 Longhorn UI 的 Volume 页面，选择要备份的卷，点击 <code>Create Backup</code>，然后添加合适的标签点击 OK 即可。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/xydv6j.png" alt="创建备份"></p>
<p>备份完成后导航到 Backup 页面就可以看到对应的备份数据了：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/myguu3.png" alt="备份数据"></p>
<p>这些备份的数据也会对应一个 <code>backupvolumes</code> crd 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get backupvolumes -n longhorn-system</span><br><span class="line">NAME                                       CREATEDAT              LASTBACKUPNAME            LASTBACKUPAT           LASTSYNCEDAT</span><br><span class="line">pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307   2022-02-22T09:23:24Z   backup-8ae4af9c49534859   2022-02-22T09:23:24Z   2022-02-22T09:41:09Z</span><br></pre></td></tr></table></figure>



<p>然后我们去到 NFS 服务器上查看会在挂载目录下面创建一个 <code>backupstore</code> 目录，下面会保留我们备份的数据：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">➜ tree /var/lib/k8s/data/backupstore</span><br><span class="line">/var/lib/k8s/data/backupstore</span><br><span class="line">└── volumes</span><br><span class="line">    └── 5e</span><br><span class="line">        └── b6</span><br><span class="line">            └── pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307</span><br><span class="line">                ├── backups</span><br><span class="line">                │   └── backup_backup-8ae4af9c49534859.cfg</span><br><span class="line">                ├── blocks</span><br><span class="line">                │   ├── 02</span><br><span class="line">                │   │   └── 2e</span><br><span class="line">                │   │       └── 022eefc6526cd3d8fc3a9f9a4ba253a910c61a1c430a807403f60a2f233fa210.blk</span><br><span class="line">                ......</span><br><span class="line">                │   └── f7</span><br><span class="line">                │       └── e3</span><br><span class="line">                │           └── f7e3ae1f83e10da4ece5142abac1fafc0d0917370f7418874c151a66a18bfa15.blk</span><br><span class="line">                └── volume.cfg</span><br><span class="line"></span><br><span class="line">51 directories, 25 files</span><br></pre></td></tr></table></figure>



<p>同样这个时候我们也可以去快照列表选择要备份的快照：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/lezyhw.png" alt="备份快照"></p>
<p>有了备份数据后要想要恢复数据，只需要选择对应的备份数据，点击 <code>Restore Latest Backup</code> 恢复数据即可：</p>
<p><img data-src="https://picdn.youdianzhishi.com/images/20220222173315.png" alt="恢复数据"></p>
<h2 id="ReadWriteMany"><a href="#ReadWriteMany" class="headerlink" title="ReadWriteMany"></a>ReadWriteMany</h2><p>Longhorn 可以通过 <code>NFSv4</code> 服务器暴露 Longhorn 卷，原生支持 RWX 工作负载，使用的 RWX 卷 会在 longhorn-system 命名空间下面创建一个 <code>share-manager-&lt;volume-name&gt;</code> 的 Pod，该 Pod 负责通过在 Pod 内运行的 NFSv4 服务器暴露 Longhorn 卷。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/h74y3g.png" alt="RWX"></p>
<p>要能够使用 RWX 卷，每个客户端节点都需要安装 <code>NFSv4</code> 客户端，对于 Ubuntu，可以通过以下方式安装 NFSv4 客户端：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ apt install nfs-common</span><br></pre></td></tr></table></figure>



<p>对于基于 RPM 的发行版，可以通过以下方式安装 NFSv4 客户端：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ yum install nfs-utils</span><br></pre></td></tr></table></figure>



<p>现在我们来创建一个如下所示的 PVC 对象，访问模式配置为 <code>ReadWriteMany</code>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># html-vol.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>



<p>直接创建上面的资源对象就会动态创建一个 PV 与之绑定：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc html</span><br><span class="line">NAME   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">html   Bound    pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15   1Gi        RWX            longhorn       15s</span><br><span class="line">➜ kubectl get pv pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM          STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15   1Gi        RWX            Delete           Bound    default/html   longhorn                63s</span><br></pre></td></tr></table></figure>



<p>然后创建一个如下所示的名为 writer 的 Deployment 资源对象，使用上面创建的 PVC 来持久化数据：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># html-writer.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">writer</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">writer</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">writer</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">content</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">alpine:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/html</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span></span><br><span class="line">              <span class="string">date</span> <span class="string">&gt;&gt;</span> <span class="string">/html/index.html;</span></span><br><span class="line">              <span class="string">sleep</span> <span class="number">5</span><span class="string">;</span></span><br><span class="line">              <span class="string">done</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">html</span></span><br></pre></td></tr></table></figure>



<p>部署后上面创建的 Longhorn 的卷就变成 <code>Attached</code> 状态了：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/dgetd6.png" alt="Attached"></p>
<p>并且这个时候会自动启动一个 <code>share-manager</code> 的 Pod，通过该 Pod 内运行的 NFSv4 服务器来暴露 Longhorn 卷：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -n longhorn-system -l longhorn.io/component=share-manager</span><br><span class="line">NAME                                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">share-manager-pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15   1/1     Running   0          2m16s</span><br><span class="line">➜ kubectl logs -f share-manager-pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15 -n longhorn-system</span><br><span class="line">time=&quot;2022-02-22T10:07:42Z&quot; level=info msg=&quot;starting RLIMIT_NOFILE rlimit.Cur 1048576, rlimit.Max 1048576&quot;</span><br><span class="line">time=&quot;2022-02-22T10:07:42Z&quot; level=info msg=&quot;ending RLIMIT_NOFILE rlimit.Cur 1048576, rlimit.Max 1048576&quot;</span><br><span class="line">time=&quot;2022-02-22T10:07:42Z&quot; level=debug msg=&quot;volume pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15 device /dev/longhorn/pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15 contains filesystem of format &quot; encrypted=false volume=pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15</span><br><span class="line">I0222 10:07:42.432630       1 mount_linux.go:425] Disk &quot;/dev/longhorn/pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15&quot; appears to be unformatted, attempting to format as type: &quot;ext4&quot; with options: [-F -m0 /dev/longhorn/pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15]</span><br><span class="line">I0222 10:07:42.981928       1 mount_linux.go:435] Disk successfully formatted (mkfs): ext4 - /dev/longhorn/pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15 /export/pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15</span><br><span class="line">time=&quot;2022-02-22T10:07:43Z&quot; level=info msg=&quot;starting nfs server, volume is ready for export&quot; encrypted=false volume=pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15</span><br><span class="line">time=&quot;2022-02-22T10:07:43Z&quot; level=info msg=&quot;Running NFS server!&quot;</span><br><span class="line">time=&quot;2022-02-22T10:07:43Z&quot; level=info msg=&quot;starting health check for volume&quot; encrypted=false volume=pvc-a03c5f7d-d4ca-43e9-aa4a-fb3b5eb5cf15</span><br></pre></td></tr></table></figure>



<p>然后我们再创建一个如下所示的 Deployment：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># html-reader.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">reader</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">reader</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">reader</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx:stable-alpine</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">          <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">            <span class="attr">claimName:</span> <span class="string">html</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">reader</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">reader</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>



<p>上面的 reader Pods 可以引用 writer Pod 相同的 PVC，是因为上面我们创建的 PV 和 PVC 是 <code>ReadWriteMany</code> 访问模式，直接创建上面的资源对象，我们可以通过 NodePort 来访问应用：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -l app=reader</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">reader-b54c4749d-4bjxf   1/1     Running   0          11s</span><br><span class="line">reader-b54c4749d-5thwz   1/1     Running   0          4m11s</span><br><span class="line">reader-b54c4749d-drcfk   1/1     Running   0          5m35s</span><br><span class="line">➜ kubectl get svc reader</span><br><span class="line">NAME     TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">reader   NodePort   10.101.54.19   &lt;none&gt;        80:31800/TCP   84s</span><br><span class="line">➜ curl http://192.168.31.31:31800</span><br><span class="line">......</span><br><span class="line">Tue Feb 22 10:18:39 UTC 2022</span><br><span class="line">Tue Feb 22 10:18:44 UTC 2022</span><br><span class="line">Tue Feb 22 10:18:49 UTC 2022</span><br><span class="line">Tue Feb 22 10:18:54 UTC 2022</span><br><span class="line">Tue Feb 22 10:18:59 UTC 2022</span><br><span class="line">......</span><br></pre></td></tr></table></figure>



<p>现在我们尝试从一个 reader Pod 中去产生一些数据，然后再去访问应用验证数据是否正确：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl exec reader-b54c4749d-4bjxf-- /bin/sh -c &quot;echo longhorn rwx access mode &gt;&gt; /usr/share/nginx/html/index.html&quot;</span><br><span class="line">➜ curl http://192.168.31.31:31800</span><br><span class="line">......</span><br><span class="line">Tue Feb 22 10:23:49 UTC 2022</span><br><span class="line">longhorn rwx access mode</span><br></pre></td></tr></table></figure>



<p>这里我们就验证了在 Longhorn 中使用 <code>ReadWriteMany</code> 访问模式的 Volume 卷。</p>
<h2 id="CSI-卷管理"><a href="#CSI-卷管理" class="headerlink" title="CSI 卷管理"></a>CSI 卷管理</h2><p>上面我们提到了通过 Longhorn UI 可以对卷进行快照、备份恢复等功能，此外我们还可以通过 Kubernetes 来实现对卷的管理，比如可以在集群上启用 CSI 快照和克隆支持。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/npw3k8.png" alt="卷管理"></p>
<h3 id="CSI-卷快照"><a href="#CSI-卷快照" class="headerlink" title="CSI 卷快照"></a>CSI 卷快照</h3><p>Kubernetes 从 1.12 版本开始引入了存储卷快照功能，在 1.17 版本进入 Beta 版本，和 PV、PVC 两个资源对象类似，Kubernetes 提供了 <code>VolumeSnapshotContent</code>、<code>VolumeSnapshot</code>、<code>VolumeSnapshotClass</code> 三个资源对象用于卷快照管理。</p>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p><code>VolumeSnapshotContent</code> 是基于某个 PV 创建的快照，类似于 PV 的资源概念；<code>VolumeSnapshot</code> 是用户对卷快照的请求，类似于持久化声明 PVC 的概念；<code>VolumeSnapshotClass</code> 对象可以来设置快照的特性，屏蔽 <code>VolumeSnapshotContent</code> 的细节，为 <code>VolumeSnapshot</code> 绑定提供动态管理，就像 <code>StorageClass</code> 的“类”概念。</p>
<p>卷快照能力为 Kubernetes 用户提供了一种标准的方式来在指定时间点复制卷的内容，并且不需要创建全新的卷，比如数据库管理员可以在执行编辑或删除之类的修改之前对数据库执行备份。</p>
<p>但是在使用该功能时，需要注意以下几点：</p>
<ul>
<li><code>VolumeSnapshot</code>、<code>VolumeSnapshotContent</code> 和 <code>VolumeSnapshotClass</code> 资源对象是 CRDs， 不属于核心 API。</li>
<li><code>VolumeSnapshot</code> 支持仅可用于 CSI 驱动。</li>
<li>作为 <code>VolumeSnapshot</code> 部署过程的一部分，Kubernetes 团队提供了一个部署于控制平面的快照控制器，并且提供了一个叫做 <code>csi-snapshotter</code> 的 Sidecar 容器，和 CSI 驱动程序一起部署，快照控制器会去监听 <code>VolumeSnapshot</code> 和 <code>VolumeSnapshotContent</code> 对象，并且负责创建和删除 <code>VolumeSnapshotContent</code> 对象。 <code>csi-snapshotter</code> 监听 <code>VolumeSnapshotContent</code> 对象，并且触发针对 CSI 端点的 <code>CreateSnapshot</code> 和 <code>DeleteSnapshot</code> 的操作，完成快照的创建或删除。</li>
<li>CSI 驱动可能实现，也可能没有实现卷快照功能，CSI 驱动可能会使用 <code>csi-snapshotter</code> 来提供对卷快照的支持，详见 <a target="_blank" rel="noopener" href="https://kubernetes-csi.github.io/docs/external-snapshotter.html">CSI 驱动程序文档</a>。</li>
</ul>
<p><code>VolumeSnapshotContents</code> 和 <code>VolumeSnapshots</code> 的生命周期包括资源供应、资源绑定、对使用 PVC 的保护机制和资源删除等各个阶段，这两个对象会遵循这些生命周期。</p>
<p><strong>资源供应</strong>：与 PV 的资源供应类似，<code>VolumeSnapshotContent</code> 也可以以静态或动态两种方式供应资源。</p>
<ul>
<li>静态供应：集群管理员会预先创建好一组 <code>VolumeSnapshotContent</code> 资源，类似于手动创建 PV</li>
<li>动态供应：基于 <code>VolumeSnapshotClass</code> 资源，当用户创建 <code>VolumeSnapshot</code> 申请时自动创建 <code>VolumeSnapshotContent</code>，类似于 <code>StorageClass</code> 动态创建 PV</li>
</ul>
<p><strong>资源绑定</strong>：快照控制器负责将 <code>VolumeSnapshot</code> 与一个合适的 <code>VolumeSnapshotContent</code> 进行绑定，包括静态和动态供应两种情况，<code>VolumeSnapshot</code> 和 <code>VolumeSnapshotContent</code> 之间也是一对一进行绑定的，不会存在一对多的情况。</p>
<p><strong>对使用中的 PVC 的保护机制</strong>：当存储快照 <code>VolumeSnapshot</code> 正在被创建且还未完成时，相关的 PVC 将会被标记为<code>正被使用中</code>，如果用户对 PVC 进行删除操作，系统不会立即删除 PVC，以避免快照还未做完造成数据丢失，删除操作会延迟到 <code>VolumeSnapshot</code> 创建完成（<code>readyToUse</code> 状态）或被终止（<code>aborted</code> 状态）的情况下完成。</p>
<p><strong>资源删除</strong>：对 <code>VolumeSnapshot</code> 发起删除操作时，对与其绑定的后端 <code>VolumeSnapshotContent</code> 的删除操作将基于删除策略 <code>DeletionPolicy</code> 的设置来决定，可以配置的删除策略有：</p>
<ul>
<li><code>Delete</code>：自动删除 <code>VolumeSnapshotContent</code> 资源对象和快照的内容。</li>
<li><code>Retain</code>：<code>VolumeSnapshotContent</code> 资源对象和快照的内容都将保留，需要手动清理。</li>
</ul>
<p>我们这里的 Longhorn 系统在部署完成后创建了 3 个 <code>csi-snapshotter</code> 的 Pod：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -n longhorn-system</span><br><span class="line">NAME                                                     READY   STATUS      RESTARTS       AGE</span><br><span class="line">csi-snapshotter-86f65d8bc-9t7dd                          1/1     Running     5 (126m ago)   2d17h</span><br><span class="line">csi-snapshotter-86f65d8bc-d6xbj                          1/1     Running     5 (126m ago)   2d17h</span><br><span class="line">csi-snapshotter-86f65d8bc-dncwv                          1/1     Running     5 (126m ago)   2d17h</span><br><span class="line">......</span><br></pre></td></tr></table></figure>



<p>这其实是启动的 3 个副本，同一时间只有一个 Pod 提供服务，通过 <code>leader-election</code> 来实现的选主高可用，比如当前这里提供服务的是 <code>csi-snapshotter-86f65d8bc-dncwv</code>，我们可以查看对应的日志信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl logs -f csi-snapshotter-86f65d8bc-dncwv -n longhorn-system</span><br><span class="line">......</span><br><span class="line">E0223 04:36:33.570567       1 reflector.go:127] github.com/kubernetes-csi/external-snapshotter/client/v3/informers/externalversions/factory.go:117: Failed to watch *v1beta1.VolumeSnapshotClass: failed to list *v1beta1.VolumeSnapshotClass: the server could not find the requested resource (get volumesnapshotclasses.snapshot.storage.k8s.io)</span><br><span class="line">E0223 04:37:03.773447       1 reflector.go:127] github.com/kubernetes-csi/external-snapshotter/client/v3/informers/externalversions/factory.go:117: Failed to watch *v1beta1.VolumeSnapshotContent: failed to list *v1beta1.VolumeSnapshotContent: the server could not find the requested resource (get volumesnapshotcontents.snapshot.storage.k8s.io)</span><br></pre></td></tr></table></figure>



<p>可以看到提示没有 <code>VolumeSnapshotClass</code> 和 <code>VolumeSnapshotContent</code> 资源，这是因为这两个资源都是 CRDs，并不是 Kubernetes 内置的资源对象，而我们在安装 Longhorn 的时候也没有安装这两个 CRDs，所以找不到，要通过 CSI 来实现卷快照功能自然就需要先安装 CRDs，我们可以从 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-csi/external-snapshotter">external-snapshotter</a> 项目中来获取：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ git clone https://github.com/kubernetes-csi/external-snapshotter</span><br><span class="line">➜ cd external-snapshotter &amp;&amp; git checkout v5.0.1</span><br><span class="line">➜ kubectl kustomize client/config/crd | kubectl create -f -</span><br></pre></td></tr></table></figure>



<p>上面的命令会安装上面提到的 3 个 Snapshot CRDs:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get crd |grep snapshot</span><br><span class="line">volumesnapshotclasses.snapshot.storage.k8s.io    2022-02-23T05:31:34Z</span><br><span class="line">volumesnapshotcontents.snapshot.storage.k8s.io   2022-02-23T05:31:34Z</span><br><span class="line">volumesnapshots.snapshot.storage.k8s.io          2022-02-23T05:31:34Z</span><br></pre></td></tr></table></figure>



<p>安装完成后再去查看上面的 <code>csi-snapshotter</code> 相关的 Pod 日志就正常了。CRDs 安装完成后还不够，我们还需要一个快照控制器来监听 <code>VolumeSnapshot</code> 和 <code>VolumeSnapshotContent</code> 对象，同样 <code>external-snapshotter</code> 项目中也提供了一个 <code>Common Snapshot Controller</code>，执行下面的命令一键安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改 deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml 镜像地址为 cnych/csi-snapshot-controller:v5.0.0，默认为 gcr 镜像</span></span><br><span class="line">➜ kubectl -n kube-system kustomize deploy/kubernetes/snapshot-controller | kubectl create -f -</span><br></pre></td></tr></table></figure>



<p>这里我们将快照控制器安装到了 <code>kube-system</code> 命名空间下，启动两个副本，同样同一时间只有一个 Pod 提供服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -n kube-system -l app=snapshot-controller</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">snapshot-controller-677b65dc6c-288w9   1/1     Running   0          3m22s</span><br><span class="line">snapshot-controller-677b65dc6c-zgdcm   1/1     Running   0          39s</span><br></pre></td></tr></table></figure>



<p>到这里就将使用 CSI 来配置快照的环境准备好了。</p>
<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><p>下面我们仍然以前面的 <code>mysql-pvc</code> 这个卷为例来说明下如何使用卷快照功能：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-pvc</span><br><span class="line">NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-pvc   Bound    pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307   1Gi        RWO            longhorn       2d18h</span><br></pre></td></tr></table></figure>



<p>要创建 <code>mysql-pvc</code> 的快照申请，首先需要创建一个 <code>VolumeSnapshot</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># snapshot-mysql.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">snapshot.storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VolumeSnapshot</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-snapshot-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumeSnapshotClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">source:</span></span><br><span class="line">    <span class="attr">persistentVolumeClaimName:</span> <span class="string">mysql-pvc</span></span><br><span class="line">    <span class="comment"># volumeSnapshotContentName: test-content</span></span><br></pre></td></tr></table></figure>



<p>其中就两个主要配置参数：</p>
<ul>
<li><code>volumeSnapshotClassName</code>：指定 <code>VolumeSnapshotClass</code> 的名称，这样就可以动态创建一个对应的 <code>VolumeSnapshotContent</code> 与之绑定，如果没有指定该参数，则属于静态方式，需要手动创建 <code>VolumeSnapshotContent</code>。</li>
<li><code>persistentVolumeClaimName</code>：指定数据来源的 PVC 名称。</li>
<li><code>volumeSnapshotContentName</code>：如果是申请静态存储快照，则需要通过该参数来指定一个 <code>VolumeSnapshotContent</code>。</li>
</ul>
<p>上面我们指定了一个存储快照类 longhorn，当然需要创建这个对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># snapshotclass.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">snapshot.storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VolumeSnapshotClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="comment"># annotations:  # 如果要指定成默认的快照类</span></span><br><span class="line">  <span class="comment">#   snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot;</span></span><br><span class="line"><span class="attr">driver:</span> <span class="string">driver.longhorn.io</span></span><br><span class="line"><span class="attr">deletionPolicy:</span> <span class="string">Delete</span></span><br></pre></td></tr></table></figure>



<p>每个 <code>VolumeSnapshotClass</code> 都包含 driver、deletionPolicy 和 parameters 字段，在需要动态配置属于该类的 <code>VolumeSnapshot</code> 时使用。</p>
<ul>
<li><code>driver</code>：表示 CSI 存储插件驱动的名称，这里我们使用的是 Longhorn 插件，名为 <code>driver.longhorn.io</code></li>
<li><code>deletionPolicy</code>：删除策略，可以设置为 Delete 或 Retain，如果删除策略是 Delete，那么底层的存储快照会和 <code>VolumeSnapshotContent</code> 对象一起删除，如果删除策略是 Retain，那么底层快照和 <code>VolumeSnapshotContent</code> 对象都会被保留。</li>
<li><code>parameters</code>：存储插件需要配置的参数，有 CSI 驱动提供具体的配置参数。</li>
</ul>
<p>如果想将当前快照类设置成默认的则需要添加 <code>snapshot.storage.kubernetes.io/is-default-class: &quot;true&quot;</code> 这样的 annotations。</p>
<p>现在我们直接创建上面的两个资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f snapshotclass.yaml</span><br><span class="line">volumesnapshotclass.snapshot.storage.k8s.io/longhorn created</span><br><span class="line">➜ kubectl apply -f snapshot-mysql.yaml</span><br><span class="line">volumesnapshot.snapshot.storage.k8s.io/mysql-snapshot-demo created</span><br><span class="line">➜ kubectl get volumesnapshotclass</span><br><span class="line">NAME       DRIVER               DELETIONPOLICY   AGE</span><br><span class="line">longhorn   driver.longhorn.io   Delete           43s</span><br><span class="line">➜ kubectl get volumesnapshot</span><br><span class="line">NAME                  READYTOUSE   SOURCEPVC   SOURCESNAPSHOTCONTENT   RESTORESIZE   SNAPSHOTCLASS   SNAPSHOTCONTENT                                    CREATIONTIME   AGE</span><br><span class="line">mysql-snapshot-demo   true         mysql-pvc                           1Gi           longhorn        snapcontent-1119649a-d4f2-447f-a21a-e527f202e43e   43s            43s</span><br></pre></td></tr></table></figure>



<p>这个时候会动态为我们创建一个 <code>VolumeSnapshotContent</code> 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get volumesnapshotcontent</span><br><span class="line">NAME                                               READYTOUSE   RESTORESIZE   DELETIONPOLICY   DRIVER               VOLUMESNAPSHOTCLASS   VOLUMESNAPSHOT        VOLUMESNAPSHOTNAMESPACE   AGE</span><br><span class="line">snapcontent-1119649a-d4f2-447f-a21a-e527f202e43e   true         1073741824    Delete           driver.longhorn.io   longhorn              mysql-snapshot-demo   default                   97s</span><br></pre></td></tr></table></figure>



<p>自动创建的 <code>VolumeSnapshotContent</code> 对象内容如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">snapshot.storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">VolumeSnapshotContent</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">snapcontent-1119649a-d4f2-447f-a21a-e527f202e43e</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">deletionPolicy:</span> <span class="string">Delete</span></span><br><span class="line">  <span class="attr">driver:</span> <span class="string">driver.longhorn.io</span></span><br><span class="line">  <span class="attr">source:</span></span><br><span class="line">    <span class="attr">volumeHandle:</span> <span class="string">pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307</span></span><br><span class="line">  <span class="attr">volumeSnapshotClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">volumeSnapshotRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">snapshot.storage.k8s.io/v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">VolumeSnapshot</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mysql-snapshot-demo</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">resourceVersion:</span> <span class="string">&#x27;4967456&#x27;</span></span><br><span class="line">    <span class="attr">uid:</span> <span class="string">1119649a-d4f2-447f-a21a-e527f202e43e</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">creationTime:</span> <span class="number">1645597546000000000</span></span><br><span class="line">  <span class="attr">readyToUse:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">restoreSize:</span> <span class="number">1073741824</span></span><br><span class="line">  <span class="attr">snapshotHandle:</span> <span class="string">bs://pvc-ec17a7e4-7bb4-4456-9380-353db3ed4307/backup-f5f28fd624a148ed</span></span><br></pre></td></tr></table></figure>



<p>其中的 <code>source.volumeHandle</code> 字段的值是在后端存储上创建并由 CSI 驱动在创建存储卷期间返回的 Volume 的唯一标识符，在动态供应模式下需要该字段，指定的是快照的来源 Volume 信息，<code>volumeSnapshotRef</code> 下面就是和关联的 <code>VolumeSnapshot</code> 对象的相关信息。当然这个时候我们在 Longhorn UI 界面上也可以看到上面我们创建的这个快照了，快照名称为 <code>snapshot-1119649a-d4f2-447f-a21a-e527f202e43e</code>，后面的 ID 与上面的 <code>VolumeSnapshotContent</code> 名称保持一致：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/ylhyeq.png" alt="快照"></p>
<p>并且也会进行一次对应的 Backup 操作，备份的信息通过 <code>snapshotHandle</code> 进行指定的，格式为 <code>bs://backup-&lt;volume&gt;/backup-&lt;name&gt;</code>：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/hzrj02.png" alt="备份"></p>
<p>这样我们就完成了通过 CSI 实现卷的快照管理功能。</p>
<h3 id="基于快照创建新的-PVC（恢复）"><a href="#基于快照创建新的-PVC（恢复）" class="headerlink" title="基于快照创建新的 PVC（恢复）"></a>基于快照创建新的 PVC（恢复）</h3><p>Kubernetes 对基于快照创建存储卷在 1.17 版本更新到了 Beta 版本，要启用该特性，就需要在 kube-apiserver、kube-controller-manager 和 kubelet 的 Feature Gate 中启用 <code>--feature-gates=...,VolumeSnapshotDataSource</code>（我们这里是 1.22 版本默认已经启用了），然后就可以基于某个快照创建一个新的 PVC 存储卷了，比如现在我们来基于上面创建的 <code>mysql-snapshot-demo</code> 这个对象来创建一个新的 PVC：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># restore-mysql.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-restore-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">dataSource:</span></span><br><span class="line">    <span class="attr">apiGroup:</span> <span class="string">snapshot.storage.k8s.io</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">VolumeSnapshot</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mysql-snapshot-demo</span></span><br></pre></td></tr></table></figure>



<p>上面的 PVC 对象和我们平时声明的方式基本一致，唯一不同的是通过一个 <code>dataSource</code> 字段配置了基于名为 <code>mysql-snapshot-demo</code> 的存储快照进行创建，创建上面的资源对象后同样会自动创建一个 PV 与之绑定：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-restore-pvc</span><br><span class="line">NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-restore-pvc   Bound    pvc-e4ddd985-31a8-4570-b393-dcedec3b0d95   1Gi        RWO            longhorn       17s</span><br></pre></td></tr></table></figure>



<p>在 Longhorn UI 中去查看该卷，可以看到该卷的实际大小并不为 0，这是因为我们是从快照中创建过来的，相当于从上面的快照中恢复的数据：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/2r6car.png" alt="恢复卷"></p>
<h3 id="卷克隆"><a href="#卷克隆" class="headerlink" title="卷克隆"></a>卷克隆</h3><p>除了基于快照创建新的 PVC 对象之外，CSI 类型的存储还支持存储的克隆功能，可以基于已经存在的 PVC 克隆一个新的 PVC，实现方式也是通过在 <code>dataSource</code> 字段中来设置源 PVC 来实现。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/ujmfb8.png" alt="clone"></p>
<p>克隆一个 PVC 其实就是对已存在的存储卷创建一个副本，唯一的区别是，系统在为克隆 PVC 提供后端存储资源时，不是新建一个空的 PV，而是复制一个与原 PVC 绑定 PV 完全一样的 PV。</p>
<p>从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时， 增加了指定一个现有 PVC 作为数据源的能力，源 PVC 必须是 bound 状态且可用的。</p>
<p>用户在使用该功能时，需要注意以下事项：</p>
<ul>
<li>克隆仅适用于 CSI 驱动</li>
<li>克隆仅适用于动态供应</li>
<li>克隆功能取决于具体的 CSI 驱动是否实现该功能</li>
<li>要求目标 PVC 和源 PVC 必须处于同一个命名空间</li>
<li>只支持在相同的 StorageClass 中（可以使用默认的）</li>
<li>两个存储卷的存储模式（VolumeMode）要一致</li>
</ul>
<p>同样我们来对前面的 <code>mysql-pvc</code> 这个存储卷进行克隆操作，对应的 PVC 声明如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-clone-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span> <span class="comment"># 必须大于或等于源的值</span></span><br><span class="line">  <span class="attr">dataSource:</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mysql-pvc</span></span><br></pre></td></tr></table></figure>



<p>该 PVC 和源 PVC 声明一样的配置，唯一不同的是通过 <code>dataSource</code> 指定了源 PVC 的名称，直接创建这个资源对象，结果是 <code>mysql-clone-pvc</code> 这个新的 PVC 与源 <code>mysql-pvc</code> 拥有相同的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-clone-pvc</span><br><span class="line">NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-clone-pvc   Bound    pvc-58eab5f0-a386-435c-91f4-0c26f7935695   1Gi        RWO            longhorn       31s</span><br></pre></td></tr></table></figure>



<p>在 Longhorn UI 页面中也可以看到对应的卷：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/zhlgqn.png" alt="克隆"></p>
<p>一旦新的 PVC 可用，被克隆的 PVC 就可以像其他 PVC 一样被使用了，也可以对其进行克隆、快照、删除等操作。</p>
<h3 id="卷动态扩容"><a href="#卷动态扩容" class="headerlink" title="卷动态扩容"></a>卷动态扩容</h3><p>我们知道对于存储来说扩容是一个非常终于的需求，对于 Kubernetes 中的卷动态扩容同样也是需要的基本功能，PV 要做扩容操作是需要底层存储支持该操作才能实现，Longhorn 底层是支持卷扩容操作的，但是要求扩展的卷必须处于 <code>detached</code> 状态才能操作，有两种方法可以扩容 Longhorn 卷：修改 PVC 和使用 Longhorn UI。</p>
<p>通过 Longhorn UI 操作比较简单，直接在页面中选择要扩容的卷，在操作中选择 <code>Expand Volume</code> 进行操作即可：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/i1a8be.png" alt="扩容"></p>
<p>要通过 PVC 来进行扩容首先需要 PVC 由 Longhorn StorageClass 进行动态供应，并且在 StorageClass 中 <code>allowVolumeExpansion</code> 属性设置为 true，建议使用这种方法，因为 PVC 和 PV 会自动更新，并且在扩容后都会保持一致。比如上面使用的 mysql-clone-pvc 这个卷（处于 <code>detached</code> 状态）使用的 longhorn 这个 StorageClass 中就已经配置了 <code>allowVolumeExpansion: true</code>，然后直接修改 mysql-pvc 这个卷下面的 <code>spec.resources.requests.storage</code> 值即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-clone-pvc</span><br><span class="line">NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-clone-pvc   Bound    pvc-58eab5f0-a386-435c-91f4-0c26f7935695   1Gi        RWO            longhorn       40m</span><br><span class="line">➜ kubectl patch pvc mysql-clone-pvc -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;resources&quot;:&#123;&quot;requests&quot;:&#123;&quot;storage&quot;:&quot;2Gi&quot;&#125;&#125;&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>



<p>修改后可以查看该 PVC 的 events 信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl describe pvc mysql-clone-pvc</span><br><span class="line">......</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                  Age                From                                                                                      Message</span><br><span class="line">  ----     ------                  ----               ----                                                                                      -------</span><br><span class="line">  ......</span><br><span class="line">  Normal   Resizing                14s                external-resizer driver.longhorn.io                                                       External resizer is resizing volume pvc-58eab5f0-a386-435c-91f4-0c26f7935695</span><br><span class="line">  Warning  ExternalExpanding       14s                volume_expand                                                                             Ignoring the PVC: didn&#x27;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC.</span><br><span class="line">  Normal   VolumeResizeSuccessful  2s                 external-resizer driver.longhorn.io                                                       Resize volume succeeded</span><br></pre></td></tr></table></figure>



<p>可以看到通过 <code>external-resizer</code> 组件实现了 Resize 操作，查看 PVC 和 PV 的大小验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pvc mysql-clone-pvc</span><br><span class="line">NAME              STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">mysql-clone-pvc   Bound    pvc-58eab5f0-a386-435c-91f4-0c26f7935695   2Gi        RWO            longhorn       43m</span><br><span class="line">➜ kubectl get pv pvc-58eab5f0-a386-435c-91f4-0c26f7935695</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   REASON   AGE</span><br><span class="line">pvc-58eab5f0-a386-435c-91f4-0c26f7935695   2Gi        RWO            Delete           Bound    default/mysql-clone-pvc   longhorn                43m</span><br></pre></td></tr></table></figure>



<p>可以看到 PVC 和 PV 中的容量都变成了 2Gi，证明扩容成功了，通过 Longhorn UI 也可以查看到卷扩容成功了：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/3p9bma.png" alt="扩容"></p>
<p><a target="_blank" rel="noopener" href="https://docs.youdianzhishi.com/k8s/storage/plugin/">
</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">六一</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">273</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/huiaz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;huiaz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">六一</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">25:34</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
