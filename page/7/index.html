<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-bounce.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hui&#39;s Blog">
<meta property="og:url" content="http://example.com/page/7/index.html">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="六一">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Hui's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hui's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Code Life</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/HTTP%20%E7%8A%B6%E6%80%81%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/HTTP%20%E7%8A%B6%E6%80%81%E7%A0%81/" class="post-title-link" itemprop="url">HTTP 状态码</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:39:29" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/" itemprop="url" rel="index"><span itemprop="name">WEB</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/Http/" itemprop="url" rel="index"><span itemprop="name">Http</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="HTTP-状态码"><a href="#HTTP-状态码" class="headerlink" title="HTTP 状态码"></a>HTTP 状态码</h3><p>HTTP 状态码是服务器对请求的响应。它们是三位数字，分为五个类别，指示请求是否已成功处理，或者是否存在错误。理解这些状态码对于调试网络问题和理解Web应用程序的行为至关重要。</p>
<p>以下是常见的 HTTP 状态码及其简要说明：</p>
<hr>
<h3 id="1xx-信息响应-Informational-Responses"><a href="#1xx-信息响应-Informational-Responses" class="headerlink" title="1xx - 信息响应 (Informational Responses)"></a><strong>1xx - 信息响应 (Informational Responses)</strong></h3><p>指示请求已被接收，继续处理。这些是临时响应，不带任何内容。</p>
<ul>
<li><strong><code>100 Continue</code></strong>: 客户端应继续其请求。通常用于客户端发送一个大型请求体到服务器之前，先发送头部，服务器如果允许，则返回 <code>100 Continue</code>，然后客户端再发送请求体。</li>
</ul>
<hr>
<h3 id="2xx-成功响应-Successful-Responses"><a href="#2xx-成功响应-Successful-Responses" class="headerlink" title="2xx - 成功响应 (Successful Responses)"></a><strong>2xx - 成功响应 (Successful Responses)</strong></h3><p>指示请求已被成功接收、理解和接受。</p>
<ul>
<li><strong><code>200 OK</code></strong>: 请求已成功。这是最常见的状态码，表示请求的一切正常，服务器已返回所请求的数据。</li>
<li><strong><code>201 Created</code></strong>: 请求已成功，并因此创建了一个新的资源。这通常是 PUT 或 POST 请求的响应。响应体中通常包含新创建资源的URI。</li>
<li><strong><code>202 Accepted</code></strong>: 请求已被接受进行处理，但处理尚未完成。请求可能最终被执行，也可能不被执行，因为在实际处理发生时可能不允许。这通常用于异步操作。</li>
<li><strong><code>204 No Content</code></strong>: 服务器成功处理了请求，但没有返回任何内容。通常用于 PUT、DELETE 或 POST 请求，表示操作成功但无需更新客户端页面。</li>
<li><strong><code>206 Partial Content</code></strong>: 服务器成功处理了部分 GET 请求。这通常用于范围请求，例如下载大文件时进行断点续传。</li>
</ul>
<hr>
<h3 id="3xx-重定向-Redirection"><a href="#3xx-重定向-Redirection" class="headerlink" title="3xx - 重定向 (Redirection)"></a><strong>3xx - 重定向 (Redirection)</strong></h3><p>指示客户端需要采取进一步的操作才能完成请求。</p>
<ul>
<li><strong><code>301 Moved Permanently</code></strong>: 所请求的资源已永久移动到新位置。客户端应使用新位置进行未来的请求，并更新其书签&#x2F;缓存。</li>
<li><strong><code>302 Found</code> (Formerly “Moved Temporarily”)</strong>: 所请求的资源暂时位于不同的 URI。客户端应继续使用原来的 URI 进行未来的请求。现代浏览器通常会缓存这个重定向，但不如 301 那么强。</li>
<li><strong><code>303 See Other</code></strong>: 参见其他。服务器告诉客户端，它应该使用 GET 请求在另一个 URI 获取资源。通常在 POST 请求成功后使用，以防止用户刷新页面导致重复提交。</li>
<li><strong><code>304 Not Modified</code></strong>: 客户端的缓存副本是最新的。服务器告诉客户端，它上次请求的资源自上次请求以来没有被修改。客户端可以使用缓存的副本，无需重新下载。常常与 <code>If-Modified-Since</code> 或 <code>If-None-Match</code> 头一起使用。</li>
<li><strong><code>307 Temporary Redirect</code></strong>: 临时重定向。与 <code>302 Found</code> 类似，但更强调客户端不应改变原始请求方法（GET 仍为 GET，POST 仍为 POST）。</li>
</ul>
<hr>
<h3 id="4xx-客户端错误-Client-Errors"><a href="#4xx-客户端错误-Client-Errors" class="headerlink" title="4xx - 客户端错误 (Client Errors)"></a><strong>4xx - 客户端错误 (Client Errors)</strong></h3><p>指示客户端看起来有错误，例如请求语法不正确，请求无效。</p>
<ul>
<li><strong><code>400 Bad Request</code></strong>: 服务器无法理解请求，因为语法无效。</li>
<li><strong><code>401 Unauthorized</code></strong>: 请求需要用户身份验证。当客户端尝试访问受保护的资源但未提供或提供错误的身份验证凭据时返回。</li>
<li><strong><code>403 Forbidden</code></strong>: 服务器理解请求，但拒绝执行。与 <code>401 Unauthorized</code> 不同的是，这种情况下即使提供了身份验证凭据也无济于事，因为客户端没有访问资源的权限。</li>
<li><strong><code>404 Not Found</code></strong>: 服务器找不到所请求的资源。这是最常见的错误响应，表示请求的URL不存在。</li>
<li><strong><code>405 Method Not Allowed</code></strong>: 请求中指定的方法（如 GET、POST、PUT）不被允许用于请求的资源。</li>
<li><strong><code>406 Not Acceptable</code></strong>: 服务器无法根据请求中给定的“Accept”头部（如 <code>Accept-Charset</code>, <code>Accept-Encoding</code>, <code>Accept-Language</code> 等）生成响应。</li>
<li><strong><code>408 Request Timeout</code></strong>: 服务器等待客户端请求的时间过长。</li>
<li><strong><code>409 Conflict</code></strong>: 请求与服务器上的当前状态冲突。例如，编辑时资源版本冲突。</li>
<li><strong><code>410 Gone</code></strong>: 所请求的资源再也不可用，并且不知道其新位置。比 <code>404 Not Found</code> 更永久，客户端不应重新尝试。</li>
<li><strong><code>413 Payload Too Large</code></strong>: 请求体过大，服务器无法处理。</li>
<li><strong><code>415 Unsupported Media Type</code></strong>: 请求的实体类型（<code>Content-Type</code>）服务器无法处理。</li>
<li><strong><code>429 Too Many Requests</code></strong>: 在给定的时间内发送了过多的请求（限速）。</li>
<li><strong><code>451 Unavailable For Legal Reasons</code></strong>: 由于法律原因，请求的资源不可用（例如，内容被审查或限制）。</li>
</ul>
<hr>
<h3 id="5xx-服务器错误-Server-Errors"><a href="#5xx-服务器错误-Server-Errors" class="headerlink" title="5xx - 服务器错误 (Server Errors)"></a><strong>5xx - 服务器错误 (Server Errors)</strong></h3><p>指示服务器在处理有效请求时遇到了一个错误。</p>
<ul>
<li><strong><code>500 Internal Server Error</code></strong>: 服务器遇到了一个不寻常的情况，阻止它完成请求。这是一个通用的错误消息，表明服务器端发生了问题，但没有更具体的错误代码可用。</li>
<li><strong><code>501 Not Implemented</code></strong>: 服务器不支持请求的功能，并且无法识别它。例如，请求的方法（如 PATCH）服务器还不支持。</li>
<li><strong><code>502 Bad Gateway</code></strong>: 作为网关或代理的服务器从上游服务器接收到无效响应。</li>
<li><strong><code>503 Service Unavailable</code></strong>: 服务器目前无法处理请求，通常是因为它过载或停机维护。这通常是临时的。</li>
<li><strong><code>504 Gateway Timeout</code></strong>: 作为网关或代理的服务器在等待上游服务器的响应时超时了。</li>
<li><strong><code>505 HTTP Version Not Supported</code></strong>: 服务器不支持请求中使用的 HTTP 协议版本。</li>
</ul>
<hr>
<p>理解这些状态码有助于开发者和服务管理员快速诊断问题，并帮助用户理解为什么请求没有按预期完成。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/HTTP%20%E8%AF%B7%E6%B1%82%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E5%86%85%E5%AE%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/HTTP%20%E8%AF%B7%E6%B1%82%E5%8C%85%E5%90%AB%E5%93%AA%E4%BA%9B%E5%86%85%E5%AE%B9/" class="post-title-link" itemprop="url">HTTP 请求包含哪些内容</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:39:26" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/" itemprop="url" rel="index"><span itemprop="name">WEB</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/Http/" itemprop="url" rel="index"><span itemprop="name">Http</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="HTTP-请求包含哪些内容，请求头和请求体有哪些类型？"><a href="#HTTP-请求包含哪些内容，请求头和请求体有哪些类型？" class="headerlink" title="HTTP 请求包含哪些内容，请求头和请求体有哪些类型？"></a>HTTP 请求包含哪些内容，请求头和请求体有哪些类型？</h3><p>一个 HTTP 请求通常包含三个主要部分：<strong>请求行 (Request Line)<strong>、</strong>请求头 (Request Headers)</strong> 和 **请求体 (Request Body)**。</p>
<h3 id="HTTP-请求的组成部分"><a href="#HTTP-请求的组成部分" class="headerlink" title="HTTP 请求的组成部分"></a>HTTP 请求的组成部分</h3><ol>
<li><p><strong>请求行 (Request Line)</strong><br>这是请求的第一行，它定义了请求的基本信息。它包含三部分：</p>
<ul>
<li>**方法 (Method)**：指示对资源所执行的操作类型（例如 GET、POST、PUT 等）。</li>
<li>**URI (Uniform Resource Identifier)**：要访问的资源的路径。</li>
<li>**HTTP 版本 (HTTP Version)**：客户端使用的 HTTP 协议版本（例如 HTTP&#x2F;1.1、HTTP&#x2F;2.0）。</li>
</ul>
<p><strong>示例：</strong><br><code>GET /index.html HTTP/1.1</code></p>
</li>
<li><p><strong>请求头 (Request Headers)</strong><br>请求头提供了关于请求的附加信息、客户端的信息以及响应的期望行为。每个请求头都是一个键值对，以冒号分隔，然后是回车换行符。</p>
<p><strong>示例：</strong></p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">Host</span>: www.example.com</span><br><span class="line"><span class="attribute">User</span>-Agent: Mozilla/<span class="number">5</span>.<span class="number">0</span> (Windows NT <span class="number">10</span>.<span class="number">0</span>; Win64; x64) AppleWebKit/<span class="number">537</span>.<span class="number">36</span> (KHTML, like Gecko) Chrome/<span class="number">100</span>.<span class="number">0</span>.<span class="number">4896</span>.<span class="number">88</span> Safari/<span class="number">537</span>.<span class="number">36</span></span><br><span class="line"><span class="attribute">Accept</span>: text/html,application/xhtml+xml,application/xml;q=<span class="number">0</span>.<span class="number">9</span>,image/avif,image/webp,*/*;q=<span class="number">0</span>.<span class="number">8</span></span><br><span class="line"><span class="attribute">Accept</span>-Language: en-US,en;q=<span class="number">0</span>.<span class="number">5</span></span><br><span class="line"><span class="attribute">Accept</span>-Encoding: gzip, deflate, br</span><br><span class="line"><span class="attribute">Connection</span>: keep-alive</span><br><span class="line"><span class="attribute">Cookie</span>: SID=some_cookie_value</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>请求体 (Request Body)</strong><br>请求体包含客户端发送给服务器的数据。<strong>并非所有请求都包含请求体。</strong></p>
<ul>
<li><strong>GET 请求通常不包含请求体。</strong></li>
<li><strong>POST、PUT 等请求通常包含请求体，用于提交数据。</strong></li>
</ul>
<p>请求体可以包含各种格式的数据，这取决于 <code>Content-Type</code> 请求头所指定的类型。</p>
<p><strong>示例（POST请求的请求体，Content-Type为 application&#x2F;json）：</strong></p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;John Doe&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span> <span class="number">30</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span> <span class="string">&quot;New York&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<hr>
<h3 id="请求头的类型"><a href="#请求头的类型" class="headerlink" title="请求头的类型"></a>请求头的类型</h3><p>请求头种类繁多，可以分为几大类，用于不同的目的：</p>
<ol>
<li><p>**通用头 (General Headers)**：适用于请求和响应，但与具体的资源无关。</p>
<ul>
<li><code>Cache-Control</code>: 控制缓存行为（例如，<code>no-cache</code>、<code>max-age=3600</code>）。</li>
<li><code>Connection</code>: 控制连接的选项（例如，<code>keep-alive</code> 表示保持连接）。</li>
<li><code>Date</code>: 消息的发送日期和时间。</li>
<li><code>Pragma</code>: 向后兼容的通用头部。</li>
<li><code>Via</code>: 消息会经过的网关和代理。</li>
<li><code>Transfer-Encoding</code>: 传输编码方式（例如，<code>chunked</code> 分块传输）。</li>
<li><code>Upgrade</code>: 升级协议请求。</li>
</ul>
</li>
<li><p>**请求头 (Request Headers)**：用于提供关于客户端或请求本身的更多信息。</p>
<ul>
<li><code>Host</code>: 必须，指定服务器的域名和端口号。</li>
<li><code>User-Agent</code>: 客户端应用程序的名称和版本（例如，浏览器类型和操作系统）。</li>
<li><code>Accept</code>: 客户端能够处理的媒体类型（MIME Type），按优先级排列（例如，<code>text/html,application/xml;q=0.9</code>）。</li>
<li><code>Accept-Charset</code>: 客户端能够支持的字符集。</li>
<li><code>Accept-Encoding</code>: 客户端能够支持的内容编码（例如，<code>gzip</code>, <code>deflate</code>, <code>br</code>）。</li>
<li><code>Accept-Language</code>: 客户端能够支持的自然语言。</li>
<li><code>Authorization</code>: 客户端身份验证凭证（例如，<code>Basic Auth</code>, <code>Bearer Token</code>）。</li>
<li><code>Cookie</code>: 客户端发送给服务器的 Cookie。</li>
<li><code>Referer</code>: 前一个网页的地址，从中链接到当前请求的页面。</li>
<li><code>If-Modified-Since</code>: 协商缓存，如果资源自指定日期后未修改，则返回 304。</li>
<li><code>If-None-Match</code>: 协商缓存，如果资源的 ETag 与指定的不匹配，则返回 304。</li>
<li><code>Range</code>: 客户端请求某个资源的指定部分（字节范围）。</li>
<li><code>Origin</code>: 对于跨域请求（CORS），指示请求的来源域名。</li>
</ul>
</li>
<li><p>**实体头 (Entity Headers)**：用于描述请求体中包含的资源。</p>
<ul>
<li><code>Content-Type</code>: 请求体中内容的媒体类型（MIME Type，例如，<code>application/json</code>, <code>application/x-www-form-urlencoded</code>, <code>multipart/form-data</code>）。</li>
<li><code>Content-Length</code>: 请求体的大小（字节数）。</li>
<li><code>Content-Encoding</code>: 请求体中内容的编码方式（例如，<code>gzip</code>）。</li>
<li><code>Content-Language</code>: 请求体中内容的自然语言。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="请求体的类型-Content-Type"><a href="#请求体的类型-Content-Type" class="headerlink" title="请求体的类型 (Content-Type)"></a>请求体的类型 (Content-Type)</h3><p>请求体的类型由 <code>Content-Type</code> 请求头指定。不同的值表示不同的数据格式：</p>
<ol>
<li><p><strong><code>application/x-www-form-urlencoded</code></strong>:</p>
<ul>
<li><strong>描述：</strong> 这是HTML表单的默认编码类型，当表单数据以键值对的形式提交时使用。键值对用 <code>&amp;</code> 符号连接，键和值用 <code>=</code> 符号连接。所有的非字母数字字符都会被URL编码。</li>
<li><strong>场景：</strong> 简单的表单提交。例如，通过 <code>method=&quot;POST&quot;</code> 且不指定 <code>enctype</code> 的 <code>form</code> 提交。</li>
<li><strong>示例：</strong> <code>name=John+Doe&amp;age=30</code></li>
</ul>
</li>
<li><p><strong><code>multipart/form-data</code></strong>:</p>
<ul>
<li><strong>描述：</strong> 当表单包含文件上传（如 <code>&lt;input type=&quot;file&quot;&gt;</code>）时，必须使用此类型。它将请求体分割成多个部分，每个部分包含一个字段的数据，并用<code>--boundary</code>字符串分隔。每个部分都有自己的 <code>Content-Disposition</code> 头来描述字段名，文件部分还会额外有 <code>Content-Type</code> 和 <code>filename</code>。</li>
<li><strong>场景：</strong> 文件上传、包含文件的复杂表单提交。</li>
<li><strong>示例（简化）：</strong><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Content-<span class="keyword">Type</span>: multipart/<span class="keyword">form</span>-<span class="keyword">data</span>; boundary=----WebKitFormBoundary7MA4YWxkTrZu0gW</span><br><span class="line"></span><br><span class="line">------WebKitFormBoundary7MA4YWxkTrZu0gW</span><br><span class="line">Content-Disposition: <span class="keyword">form</span>-<span class="keyword">data</span>; <span class="keyword">name</span>=<span class="string">&quot;username&quot;</span></span><br><span class="line"></span><br><span class="line">Alice</span><br><span class="line">------WebKitFormBoundary7MA4YWxkTrZu0gW</span><br><span class="line">Content-Disposition: <span class="keyword">form</span>-<span class="keyword">data</span>; <span class="keyword">name</span>=<span class="string">&quot;profile_picture&quot;</span>; filename=<span class="string">&quot;photo.jpg&quot;</span></span><br><span class="line">Content-<span class="keyword">Type</span>: image/jpeg</span><br><span class="line"></span><br><span class="line">... <span class="keyword">file</span> <span class="keyword">data</span> here ...</span><br><span class="line">------WebKitFormBoundary7MA4YWxkTrZu0gW--</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong><code>application/json</code></strong>:</p>
<ul>
<li><strong>描述：</strong> 请求体是 JSON (JavaScript Object Notation) 格式的字符串。这是现代Web API中最常用的数据交换格式，因为其轻巧、易于阅读和解析。</li>
<li><strong>场景：</strong> RESTful API 提交结构化数据，前后端分离应用的数据交互。</li>
<li><strong>示例：</strong><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;productId&quot;</span><span class="punctuation">:</span> <span class="number">123</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;quantity&quot;</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;customerDetails&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Jane&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;email&quot;</span><span class="punctuation">:</span> <span class="string">&quot;jane@example.com&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong><code>text/plain</code></strong>:</p>
<ul>
<li><strong>描述：</strong> 请求体是纯文本。</li>
<li><strong>场景：</strong> 简单文本内容的提交，例如直接发送代码片段、日志等，不常用作结构化数据提交。</li>
<li><strong>示例：</strong> <code>This is just a plain text message.</code></li>
</ul>
</li>
<li><p><strong><code>application/xml</code> 或 <code>text/xml</code></strong>:</p>
<ul>
<li><strong>描述：</strong> 请求体是 XML (Extensible Markup Language) 格式的字符串。</li>
<li><strong>场景：</strong> 一些老旧的Web服务 (SOAP) 或特定企业级应用。</li>
<li><strong>示例：</strong><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">product</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">id</span>&gt;</span>123<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">quantity</span>&gt;</span>5<span class="tag">&lt;/<span class="name">quantity</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">product</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<p>了解这些组成部分和类型对于开发网络应用、排查问题以及理解数据如何在客户端和服务器之间传输至关重要。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/ICMP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/ICMP/" class="post-title-link" itemprop="url">ICMP</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:39:42" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/" itemprop="url" rel="index"><span itemprop="name">WEB</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/WEB/Http/" itemprop="url" rel="index"><span itemprop="name">Http</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="什么是-ICMP-协议？"><a href="#什么是-ICMP-协议？" class="headerlink" title="什么是 ICMP 协议？"></a>什么是 ICMP 协议？</h3><p><strong>ICMP (Internet Control Message Protocol)<strong>，即</strong>互联网控制消息协议</strong>，是 <strong>IP 协议</strong>的<strong>辅助协议</strong>，是 TCP&#x2F;IP 协议族中的一个核心组成部分。ICMP 通常被认为是网络层的协议（与 IP 协议同层），因为它处理 IP 数据包的错误和控制信息。</p>
<p><strong>核心特点：</strong></p>
<ul>
<li><strong>控制和错误报告：</strong> ICMP 的主要作用是报告 IP 数据包传输过程中的错误以及提供网络诊断信息。它不传输用户数据，而是传输网络层面的控制和错误消息。</li>
<li><strong>不可靠性：</strong> ICMP 报文本身不提供可靠传输机制。如果一个 ICMP 报文在传输过程中丢失，IP 层不会重传它。</li>
<li><strong>承载于 IP：</strong> ICMP 报文是“封装”在 IP 数据包中的。换句话说，一个 ICMP 报文会作为 IP 数据包的有效载荷（payload）进行传输。</li>
</ul>
<h3 id="ICMP-的主要作用："><a href="#ICMP-的主要作用：" class="headerlink" title="ICMP 的主要作用："></a>ICMP 的主要作用：</h3><p>ICMP 的主要作用可以归纳为以下几点，主要围绕<strong>错误报告</strong>和<strong>网络诊断</strong>：</p>
<ol>
<li><p><strong>报告差错信息 (Error Reporting):</strong><br>当 IP 数据包在传输过程中遇到问题时，ICMP 被用来向数据包的源主机报告这些错误。常见的 ICMP 差错报文类型包括：</p>
<ul>
<li><p><strong>目标不可达 (Destination Unreachable):</strong></p>
<ul>
<li>当路由器无法将数据包送达目的地时生成。</li>
<li><strong>示例场景：</strong><ul>
<li><strong>网络不可达：</strong> 路由器没有到达目标网络的路由。</li>
<li><strong>主机不可达：</strong> 目标网络存在，但目标主机不响应（可能已关机或地址错误）。</li>
<li><strong>协议不可达：</strong> 目标主机不支持数据包指定的上层协议（如 TCP 或 UDP）。</li>
<li><strong>端口不可达：</strong> 目标主机上没有应用程序在监听数据包指定的端口。</li>
<li><strong>需要分片但禁止分片 (Fragmentation Needed and Don’t Fragment Bit Set):</strong> 当数据包太大需要分片，但设置了“不分片”标志位时。常用于 <strong>MTU (Maximum Transmission Unit) 路径发现</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>源抑制 (Source Quench):</strong></p>
<ul>
<li>当路由器或目标主机缓冲区已满，无法处理更多传入数据包时，它会向源主机发送源抑制消息，请求降低发送速率。</li>
<li><strong>注意：</strong> 现代网络中，这种机制已很少使用，更多地依赖于 TCP 的流量控制来管理拥塞。</li>
</ul>
</li>
<li><p><strong>时间超时 (Time Exceeded):</strong></p>
<ul>
<li>当数据包的<strong>生存时间 (TTL - Time To Live)</strong> 字段减到 0 时，路由器会丢弃该数据包并向源主机发送时间超时消息。TTL 字段用于防止数据包在网络中无限循环。</li>
<li><strong>示例场景：</strong> 路由循环、网络拥塞导致数据包长时间滞留。</li>
<li><strong>应用：</strong> <strong><code>traceroute</code> (Windows 下 <code>tracert</code>)</strong> 命令就利用了 ICMP 的时间超时报文来发现数据包从源到目的地的路径上的所有路由器。</li>
</ul>
</li>
<li><p><strong>参数问题 (Parameter Problem):</strong></p>
<ul>
<li>当 IP 数据包头部中的字段不正确或缺失时，会生成此消息。</li>
<li><strong>示例场景：</strong> IP 头部字段值非法，导致无法正确处理数据包。</li>
</ul>
</li>
<li><p><strong>重定向 (Redirect):</strong></p>
<ul>
<li>路由器可能会发送重定向消息，告诉源主机有更优的路由路径可以到达特定的目的地。这通常发生在主机连接到多个路由器，但使用了次优路由的情况下。</li>
<li><strong>示例场景：</strong> 主机通过路由器A发送数据包到目的地X，但路由器A知道路由器B有一条更短或更直接的路径到X，于是路由器A会发送重定向消息给主机，让主机下次直接通过路由器B发送。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>提供网络诊断和查询功能 (Diagnostic and Query Functions):</strong></p>
<p>ICMP 不仅仅用于报告错误，还用于查询网络状态、测试连通性等。</p>
<ul>
<li><p><strong>回送请求&#x2F;回送应答 (Echo Request &#x2F; Echo Reply):</strong></p>
<ul>
<li>这是 ICMP 最广为人知和常用的功能。源主机发送一个<strong>回送请求</strong>报文到目标主机，目标主机接收到后回复一个<strong>回送应答</strong>报文。</li>
<li><strong>应用：</strong><ul>
<li><strong><code>ping</code> 命令：</strong> <code>ping</code> 命令就是基于 ICMP Echo Request&#x2F;Reply 来测试两台主机之间的网络连通性、统计丢包率和往返时间 (RTT)。</li>
<li><strong>网络故障排查：</strong> 验证主机是否在线、网络路径是否正常。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>时间戳请求&#x2F;时间戳应答 (Timestamp Request &#x2F; Timestamp Reply):</strong></p>
<ul>
<li>请求和应答报文中包含发送和接收报文时的时间戳信息，用于测量网络的延迟。</li>
<li><strong>应用：</strong> 测量精细的网络延时。</li>
</ul>
</li>
<li><p><strong>信息请求&#x2F;信息应答 (Information Request &#x2F; Information Reply):</strong></p>
<ul>
<li>早期用于无盘工作站获取其网络信息（如网络掩码）。</li>
<li><strong>注意：</strong> 已废弃，被 DHCP 等协议取代。</li>
</ul>
</li>
<li><p><strong>地址掩码请求&#x2F;地址掩码应答 (Address Mask Request &#x2F; Address Mask Reply):</strong></p>
<ul>
<li>用于获取已知 IP 地址的网络掩码。</li>
<li><strong>注意：</strong> 已废弃，被 DHCP 等协议取代。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>ICMP 协议虽然不直接承载用户数据，但它在 IP 网络中扮演着至关重要的角色，就像交通警察和路标一样。它负责：</p>
<ul>
<li><strong>通知发送方网络中出现的问题</strong> (如目标不可达、流量过载、数据包超时等)。</li>
<li><strong>提供诊断工具</strong> (如 <code>ping</code> 命令测试连通性，<code>traceroute</code> 命令跟踪路由路径)。</li>
</ul>
<p>因此，尽管它只是 IP 协议的辅助，但却是网络错误处理和故障排查中不可或缺的一部分。由于其诊断功能，ICMP 也常常被攻击者利用进行扫描探测，或者被防火墙作为攻击防护的目标。</p>
<h3 id="ICMP-重定向-ICMP-Redirect-的作用及工作原理"><a href="#ICMP-重定向-ICMP-Redirect-的作用及工作原理" class="headerlink" title="ICMP 重定向 (ICMP Redirect) 的作用及工作原理"></a>ICMP 重定向 (ICMP Redirect) 的作用及工作原理</h3><p>ICMP 重定向是 <strong>ICMP 协议</strong>的一种消息类型，它的主要作用是<strong>优化本地网络中的路由路径</strong>，帮助主机发现更优（通常是更直接的）出站路由器。它不是为了告诉主机整个网络的最佳路径，而是为了在主机最初选择的路由是“次优”时，提供一种纠正机制。</p>
<p><strong>作用：</strong></p>
<p>当路由器检测到某个连接到其本地局域网（LAN）的主机，正在通过它发送数据包到某个目的地，但实际上有另一个路由器在同一个局域网内，可以为这个目的地提供更直接或更优的路由路径时，会向发送数据包的主机发送 <strong>ICMP 重定向消息</strong>。</p>
<p><strong>核心目标：</strong></p>
<ul>
<li><strong>避免多余的跳 (Hop):</strong> 减少主机到最终目的地之间的跳数。</li>
<li><strong>优化路由效率:</strong> 提高数据包的转发效率，减少延迟和路由器负载。</li>
</ul>
<p><strong>工作原理：</strong></p>
<p>假设一个主机 (Host A) 和两个路由器 (Router X 和 Router Y) 都连接在同一个局域网 (LAN) 上。Host A 的默认网关设置为 Router X。Router X 可以通往互联网，Router Y 也可以通往互联网的某些部分。</p>
<ol>
<li><p><strong>初始状态：</strong></p>
<ul>
<li>Host A 的路由表配置了默认网关：指向 Router X 的 IP 地址。</li>
<li>Router X 知道如何到达整个互联网 (或至少大部分网络)。</li>
<li>Router Y 也连接到相同的 LAN，并且可能知道一些 Router X 不知道的，或者通过 Router Y 连接更优的特定网络路径 (例如，Router Y 是到达特定子网的更直接路径)。</li>
</ul>
</li>
<li><p><strong>次优路由发生：</strong></p>
<ul>
<li>Host A 需要向目的地 D 发送一个数据包。</li>
<li>根据其默认网关设置，Host A 将数据包发送给 Router X。</li>
<li>Router X 接收到 Host A 发来的数据包后，<strong>检查其自身的路由表</strong>。</li>
<li>Router X 发现：<ul>
<li>它确实可以到达目的地 D。</li>
<li><strong>更重要的是，它发现到达目的地 D 的“下一跳”实际上是 LAN 上的另一个路由器——Router Y。</strong> 也就是说，如果 Host A 直接将数据包发送给 Router Y，而不是先发送给自己，再由自己转发给 Router Y，那么数据包可以少走一段路径（即少了一个跳：Host A -&gt; Router X 这一跳）。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>发送 ICMP 重定向消息：</strong></p>
<ul>
<li>Router X 在它将数据包转发给 Router Y 的同时，会向 Host A 发送一个 <strong>ICMP 重定向 (Type 5)</strong> 消息。</li>
<li>这个 ICMP 重定向消息会告诉 Host A：<ul>
<li>“嘿，下次你要去目的地 (或目的地所在的网络) D 时，不要再发给我了。”</li>
<li>“你应该直接把数据包发给 **Router Y (给出 Router Y 的 IP 地址)**，那条路径更直接。”</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>主机更新路由表：</strong></p>
<ul>
<li>Host A 收到 ICMP 重定向消息后，会<strong>更新自己的路由表</strong>。</li>
<li>具体来说，Host A 会为目的地 D（或目的地所在的网络）添加一条新的路由条目，将下一跳设置为 Router Y 的 IP 地址。此条目通常是临时性的，并且优先级低于静态配置或动态路由协议学习到的路由。</li>
</ul>
</li>
<li><p><strong>后续流量优化：</strong></p>
<ul>
<li>当 Host A 再次需要向目的地 D 发送数据包时，它会首先查询更新后的路由表。</li>
<li>由于有了新的路由条目，Host A 会直接将数据包发送给 Router Y，从而绕过不必要的 Router X。这减少了网络延迟和 Router X 的处理负担。</li>
</ul>
</li>
</ol>
<p><strong>ICMP 重定向的报文结构简要说明：</strong></p>
<p>ICMP 重定向报文是一个 ICMP Type 5 的消息。它包含以下关键信息：</p>
<ul>
<li><strong>Type:</strong> 5 (表示重定向)</li>
<li><strong>Code:</strong> 指示重定向的类型（例如，0 表示重定向到网络，1 表示重定向到主机）。</li>
<li><strong>Gateway Internet Address:</strong> 告知发送方应将数据包发送给的“更优”路由器的 IP 地址 (即 Router Y 的 IP)。</li>
<li><strong>Internet Header + First 64 bits of Original Data:</strong> 包含触发重定向的原始 IP 数据包的头部以及原始数据的前 64 位。这有助于主机识别是哪个数据包触发了重定向，从而知道为哪个目的地更新路由。</li>
</ul>
<p><strong>需要注意的事项：</strong></p>
<ul>
<li><strong>本地网络限制：</strong> ICMP 重定向只能在同一物理网络（或广播域）中的路由器和主机之间进行。路由器不能向来自另一个网络的流量发送重定向消息。</li>
<li><strong>安全性考量：</strong> 由于 ICMP 重定向消息可以修改主机的路由表，它可能被恶意攻击者利用进行重定向攻击，将流量重定向到攻击者控制的设备上。因此，大多数生产环境的路由器和防火墙会配置为：<ul>
<li><strong>忽略 ICMP 重定向消息</strong>（对于主机）。</li>
<li><strong>不发送 ICMP 重定向消息</strong>（对于路由器），除非确实需要且网络管理者完全理解其影响。</li>
<li><strong>防火墙策略：</strong> 经常配置防火墙（例如 Linux 的 <code>iptables</code>）来限制或完全阻止 ICMP 重定向报文的传入和传出，以增强安全性。</li>
</ul>
</li>
</ul>
<p><strong>总结：</strong></p>
<p>ICMP 重定向是一种在路由器发现其转发的流量可以由同一局域网内更直接的路由器处理时，用于通知源主机的一种机制。它的目标是优化本地网络内的路由路径，但鉴于其潜在的安全风险，在生产环境中对其的使用通常受到严格控制。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Ingress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Ingress/" class="post-title-link" itemprop="url">Ingress</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:39:56" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">网络</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h1><p>前面我们学习了在 Kubernetes 集群内部使用 kube-dns 实现服务发现的功能，那么我们部署在 Kubernetes 集群中的应用如何暴露给外部的用户使用呢？我们知道可以使用 <code>NodePort</code> 和 <code>LoadBlancer</code> 类型的 Service 可以把应用暴露给外部用户使用，除此之外，Kubernetes 还为我们提供了一个非常重要的资源对象可以用来暴露服务给外部用户，那就是 <code>Ingress</code>。对于小规模的应用我们使用 NodePort 或许能够满足我们的需求，但是当你的应用越来越多的时候，你就会发现对于 NodePort 的管理就非常麻烦了，这个时候使用 Ingress 就非常方便了，可以避免管理大量的端口。</p>
<h2 id="资源对象"><a href="#资源对象" class="headerlink" title="资源对象"></a>资源对象</h2><p><code>Ingress</code> 资源对象是 Kubernetes 内置定义的一个对象，是从 Kuberenets 集群外部访问集群的一个入口，将外部的请求转发到集群内不同的 Service 上，其实就相当于 nginx、haproxy 等负载均衡代理服务器，可能你会觉得我们直接使用 nginx 就实现了，但是只使用 nginx 这种方式有很大缺陷，每次有新服务加入的时候怎么改 Nginx 配置？不可能让我们去手动更改或者滚动更新前端的 Nginx Pod 吧？那我们再加上一个服务发现的工具比如 consul 如何？貌似是可以，对吧？Ingress 实际上就是这样实现的，只是服务发现的功能自己实现了，不需要使用第三方的服务了，然后再加上一个域名规则定义，路由信息的刷新依靠 Ingress Controller 来提供。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/q1mcnt.png" alt="ingress flow"></p>
<p>Ingress Controller 可以理解为一个监听器，通过不断地监听 kube-apiserver，实时的感知后端 Service、Pod 的变化，当得到这些信息变化后，Ingress Controller 再结合 Ingress 的配置，更新反向代理负载均衡器，达到服务发现的作用。其实这点和服务发现工具 consul、 consul-template 非常类似。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>一个常见的 Ingress 资源清单如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-ingress</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/testpath</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">test</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>



<p>上面这个 Ingress 资源的定义，配置了一个路径为 <code>/testpath</code> 的路由，所有 <code>/testpath/**</code> 的入站请求，会被 Ingress 转发至名为 test 的服务的 80 端口的 <code>/</code> 路径下。可以将 Ingress 狭义的理解为 Nginx 中的配置文件 <code>nginx.conf</code>。</p>
<p>此外 Ingress 经常使用注解 <code>annotations</code> 来配置一些选项，当然这具体取决于 Ingress 控制器的实现方式，不同的 Ingress 控制器支持不同的注解。</p>
<p>另外需要注意的是当前集群版本是 <code>v1.22</code>，这里使用的 apiVersion 是 <code>networking.k8s.io/v1</code>，所以如果是之前版本的 Ingress 资源对象需要进行迁移。 Ingress 资源清单的描述我们可以使用 <code>kubectl explain</code> 命令来了解：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl explain ingress.spec</span><br><span class="line">KIND:     Ingress</span><br><span class="line">VERSION:  networking.k8s.io/v1</span><br><span class="line"></span><br><span class="line">RESOURCE: spec &lt;Object&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Spec is the desired state of the Ingress. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br><span class="line"></span><br><span class="line">     IngressSpec describes the Ingress the user wishes to exist.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   defaultBackend       &lt;Object&gt;</span><br><span class="line">     DefaultBackend is the backend that should handle requests that don&#x27;t match</span><br><span class="line">     any rule. If Rules are not specified, DefaultBackend must be specified. If</span><br><span class="line">     DefaultBackend is not set, the handling of requests that do not match any</span><br><span class="line">     of the rules will be up to the Ingress controller.</span><br><span class="line"></span><br><span class="line">   ingressClassName     &lt;string&gt;</span><br><span class="line">     IngressClassName is the name of the IngressClass cluster resource. The</span><br><span class="line">     associated IngressClass defines which controller will implement the</span><br><span class="line">     resource. This replaces the deprecated `kubernetes.io/ingress.class`</span><br><span class="line">     annotation. For backwards compatibility, when that annotation is set, it</span><br><span class="line">     must be given precedence over this field. The controller may emit a warning</span><br><span class="line">     if the field and annotation have different values. Implementations of this</span><br><span class="line">     API should ignore Ingresses without a class specified. An IngressClass</span><br><span class="line">     resource may be marked as default, which can be used to set a default value</span><br><span class="line">     for this field. For more information, refer to the IngressClass</span><br><span class="line">     documentation.</span><br><span class="line"></span><br><span class="line">   rules        &lt;[]Object&gt;</span><br><span class="line">     A list of host rules used to configure the Ingress. If unspecified, or no</span><br><span class="line">     rule matches, all traffic is sent to the default backend.</span><br><span class="line"></span><br><span class="line">   tls  &lt;[]Object&gt;</span><br><span class="line">     TLS configuration. Currently the Ingress only supports a single TLS port,</span><br><span class="line">     443. If multiple members of this list specify different hosts, they will be</span><br><span class="line">     multiplexed on the same port according to the hostname specified through</span><br><span class="line">     the SNI TLS extension, if the ingress controller fulfilling the ingress</span><br><span class="line">     supports SNI.</span><br></pre></td></tr></table></figure>



<p>从上面描述可以看出 Ingress 资源对象中有几个重要的属性：<code>defaultBackend</code>、<code>ingressClassName</code>、<code>rules</code>、<code>tls</code>。</p>
<h3 id="rules"><a href="#rules" class="headerlink" title="rules"></a>rules</h3><p>其中核心部分是 <code>rules</code> 属性的配置，每个路由规则都在下面进行配置：</p>
<ul>
<li><code>host</code>：可选字段，上面我们没有指定 host 属性，所以该规则适用于通过指定 IP 地址的所有入站 HTTP 通信，如果提供了 host 域名，则 <code>rules</code> 则会匹配该域名的相关请求，此外 <code>host</code> 主机名可以是精确匹配（例如 <code>foo.bar.com</code>）或者使用通配符来匹配（例如 <code>*.foo.com</code>）。</li>
<li><code>http.paths</code>：定义访问的路径列表，比如上面定义的 <code>/testpath</code>，每个路径都有一个由 <code>backend.service.name</code> 和 <code>backend.service.port.number</code> 定义关联的 Service 后端，在控制器将流量路由到引用的服务之前，<code>host</code> 和 <code>path</code> 都必须匹配传入的请求才行。</li>
<li><code>backend</code>：该字段其实就是用来定义后端的 Service 服务的，与路由规则中 <code>host</code> 和 <code>path</code> 匹配的流量会将发送到对应的 backend 后端去。</li>
</ul>
<blockquote>
<p>此外一般情况下在 Ingress 控制器中会配置一个 <code>defaultBackend</code> 默认后端，当请求不匹配任何 Ingress 中的路由规则的时候会使用该后端。<code>defaultBackend</code> 通常是 Ingress 控制器的配置选项，而非在 Ingress 资源中指定。</p>
</blockquote>
<h3 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h3><p><code>backend</code> 后端除了可以引用一个 Service 服务之外，还可以通过一个 <code>resource</code> 资源进行关联，<code>Resource</code> 是当前 Ingress 对象命名空间下引用的另外一个 Kubernetes 资源对象，但是需要注意的是 <code>Resource</code> 与 <code>Service</code> 配置是互斥的，只能配置一个，<code>Resource</code> 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ingress-resource-backend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/icons</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">ImplementationSpecific</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">resource:</span></span><br><span class="line">                <span class="attr">apiGroup:</span> <span class="string">k8s.example.com</span></span><br><span class="line">                <span class="attr">kind:</span> <span class="string">StorageBucket</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">icon-assets</span></span><br></pre></td></tr></table></figure>



<p>该 Ingress 资源对象描述了所有的 <code>/icons</code> 请求会被路由到同命名空间下的名为 <code>icon-assets</code> 的 <code>StorageBucket</code> 资源中去进行处理。</p>
<h3 id="pathType"><a href="#pathType" class="headerlink" title="pathType"></a>pathType</h3><p>上面的示例中在定义路径规则的时候都指定了一个 <code>pathType</code> 的字段，事实上每个路径都需要有对应的路径类型，当前支持的路径类型有三种：</p>
<ul>
<li><code>ImplementationSpecific</code>：该路径类型的匹配方法取决于 <code>IngressClass</code>，具体实现可以将其作为单独的 pathType 处理或者与 <code>Prefix</code> 或 <code>Exact</code> 类型作相同处理。</li>
<li><code>Exact</code>：精确匹配 URL 路径，且区分大小写。</li>
<li><code>Prefix</code>：基于以 <code>/</code> 分隔的 URL 路径前缀匹配，匹配区分大小写，并且对路径中的元素逐个完成，路径元素指的是由 <code>/</code> 分隔符分隔的路径中的标签列表。</li>
</ul>
<p><code>Exact</code> 比较简单，就是需要精确匹配 URL 路径，对于 <code>Prefix</code> 前缀匹配，需要注意如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配，例如 <code>/foo/bar</code> 可以匹配 <code>/foo/bar/baz</code>, 但不匹配 <code>/foo/barbaz</code>，可以查看下表了解更多的匹配场景（来自官网）：</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/6s4hd5.png" alt="示例"></p>
<blockquote>
<p>在某些情况下，Ingress 中的多条路径会匹配同一个请求，这种情况下最长的匹配路径优先，如果仍然有两条同等的匹配路径，则精确路径类型优先于前缀路径类型。</p>
</blockquote>
<h3 id="IngressClass"><a href="#IngressClass" class="headerlink" title="IngressClass"></a>IngressClass</h3><p>Kubernetes 1.18 起，正式提供了一个 <code>IngressClass</code> 资源，作用与 <code>kubernetes.io/ingress.class</code> 注解类似，因为可能在集群中有多个 Ingress 控制器，可以通过该对象来定义我们的控制器，例如：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">IngressClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">controller:</span> <span class="string">nginx-ingress-internal-controller</span></span><br><span class="line">  <span class="attr">parameters:</span></span><br><span class="line">    <span class="attr">apiGroup:</span> <span class="string">k8s.example.com</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">IngressParameters</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">external-lb</span></span><br></pre></td></tr></table></figure>



<p>其中重要的属性是 <code>metadata.name</code> 和 <code>spec.controller</code>，前者是这个 <code>IngressClass</code> 的名称，需要设置在 Ingress 中，后者是 Ingress 控制器的名称。</p>
<p>Ingress 中的 <code>spec.ingressClassName</code> 属性就可以用来指定对应的 IngressClass，并进而由 IngressClass 关联到对应的 Ingress 控制器，如：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ingressClassName:</span> <span class="string">external-lb</span> <span class="comment"># 上面定义的 IngressClass 对象名称</span></span><br><span class="line">  <span class="attr">defaultBackend:</span></span><br><span class="line">    <span class="attr">service:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">myapp</span></span><br><span class="line">      <span class="attr">port:</span></span><br><span class="line">        <span class="attr">number:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>



<p>不过需要注意的是 <code>spec.ingressClassName</code> 与老版本的 <code>kubernetes.io/ingress.class</code> 注解的作用并不完全相同，因为 <code>ingressClassName</code> 字段引用的是 <code>IngressClass</code> 资源的名称，<code>IngressClass</code> 资源中除了指定了 Ingress 控制器的名称之外，还可能会通过 <code>spec.parameters</code> 属性定义一些额外的配置。</p>
<p>比如 <code>parameters</code> 字段有一个 <code>scope</code> 和 <code>namespace</code> 字段，可用来引用特定于命名空间的资源，对 Ingress 类进行配置。 <code>scope</code> 字段默认为 <code>Cluster</code>，表示默认是集群作用域的资源。将 <code>scope</code> 设置为 <code>Namespace</code> 并设置 <code>namespace</code> 字段就可以引用某特定命名空间中的参数资源，比如：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">IngressClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">external-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">controller:</span> <span class="string">nginx-ingress-internal-controller</span></span><br><span class="line">  <span class="attr">parameters:</span></span><br><span class="line">    <span class="attr">apiGroup:</span> <span class="string">k8s.example.com</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">IngressParameters</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">external-lb</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">external-configuration</span></span><br><span class="line">    <span class="attr">scope:</span> <span class="string">Namespace</span></span><br></pre></td></tr></table></figure>



<p>由于一个集群中可能有多个 Ingress 控制器，所以我们还可以将一个特定的 <code>IngressClass</code> 对象标记为集群默认是 Ingress 类。只需要将一个 IngressClass 资源的 <code>ingressclass.kubernetes.io/is-default-class</code> 注解设置为 true 即可，这样未指定 <code>ingressClassName</code> 字段的 Ingress 就会使用这个默认的 IngressClass。</p>
<blockquote>
<p>如果集群中有多个 <code>IngressClass</code> 被标记为默认，准入控制器将阻止创建新的未指定 <code>ingressClassName</code> 的 Ingress 对象。最好的方式还是确保集群中最多只能有一个 <code>IngressClass</code> 被标记为默认。</p>
</blockquote>
<h3 id="TLS"><a href="#TLS" class="headerlink" title="TLS"></a>TLS</h3><p>Ingress 资源对象还可以用来配置 Https 的服务，可以通过设定包含 TLS 私钥和证书的 Secret 来保护 Ingress。 Ingress 只支持单个 TLS 端口 443，如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过 SNI TLS 扩展指定的主机名 （如果 Ingress 控制器支持 SNI）在同一端口上进行复用。需要注意 TLS Secret 必须包含名为 <code>tls.crt</code> 和 <code>tls.key</code> 的键名，例如：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">testsecret-tls</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">tls.crt:</span> <span class="string">base64</span> <span class="string">编码的</span> <span class="string">cert</span></span><br><span class="line">  <span class="attr">tls.key:</span> <span class="string">base64</span> <span class="string">编码的</span> <span class="string">key</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/tls</span></span><br></pre></td></tr></table></figure>



<p>在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道，我们需要确保创建的 TLS Secret 创建自包含 <code>https-example.foo.com</code> 的公用名称的证书，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tls-example-ingress</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tls:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">hosts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">https-example.foo.com</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">testsecret-tls</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">https-example.foo.com</span></span><br><span class="line">      <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">service1</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>



<p>现在我们了解了如何定义 Ingress 资源对象了，但是仅创建 Ingress 资源本身没有任何效果。还需要部署 Ingress 控制器，例如 <code>ingress-nginx</code>，现在可以供大家使用的 Ingress 控制器有很多，比如 traefik、nginx-controller、Kubernetes Ingress Controller for Kong、HAProxy Ingress controller，当然你也可以自己实现一个 Ingress Controller，现在普遍用得较多的是 traefik 和 ingress-nginx，traefik 的性能比 ingress-nginx 差，但是配置使用要简单许多，我们这里会重点给大家介绍 ingress-nginx、traefik 以及 apisix 的使用。</p>
<blockquote>
<p>实际上社区目前还在开发一组高配置能力的 API，被称为 <a target="_blank" rel="noopener" href="https://gateway-api.sigs.k8s.io/">Service API</a>，新 API 会提供一种 Ingress 的替代方案，它的存在目的不是替代 Ingress，而是提供一种更具配置能力的新方案。</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Job%20%E4%B8%8E%20CronJob/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Job%20%E4%B8%8E%20CronJob/" class="post-title-link" itemprop="url">Job 与 CronJob</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:43:14" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/Job/" itemprop="url" rel="index"><span itemprop="name">Job</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Job-与-CronJob"><a href="#Job-与-CronJob" class="headerlink" title="Job 与 CronJob"></a>Job 与 CronJob</h1><p>接下来给大家介绍另外一类资源对象：<code>Job</code>，我们在日常的工作中经常都会遇到一些需要进行批量数据处理和分析的需求，当然也会有按时间来进行调度的工作，在我们的 Kubernetes 集群中为我们提供了 <code>Job</code> 和 <code>CronJob</code> 两种资源对象来应对我们的这种需求。</p>
<p><code>Job</code> 负责处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。而<code>CronJob</code> 则就是在 <code>Job</code> 上加上了时间调度。</p>
<h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>我们用 <code>Job</code> 这个资源对象来创建一个任务，我们定义一个 <code>Job</code> 来执行一个倒计时的任务，对应的资源清单如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># job-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">job-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">counter</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;bin/sh&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;-c&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i; done&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们可以看到 <code>Job</code> 中也是一个 Pod 模板，和之前的 Deployment、StatefulSet 之类的是一致的，只是 Pod 中的容器要求是一个任务，而不是一个常驻前台的进程了，因为需要退出，另外值得注意的是 <code>Job</code> 的 <code>RestartPolicy</code> 仅支持 <code>Never</code> 和 <code>OnFailure</code> 两种，不支持 <code>Always</code>，我们知道 <code>Job</code> 就相当于来执行一个批处理任务，执行完就结束了，如果支持 <code>Always</code> 的话是不是就陷入了死循环了？</p>
<p>直接创建这个 Job 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f job-demo.yaml</span><br><span class="line">job.batch/job-demo created</span><br><span class="line">➜  ~  kubectl get job</span><br><span class="line">NAME       COMPLETIONS   DURATION   AGE</span><br><span class="line">job-demo   1/1           19s        42s</span><br><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                READY   STATUS      RESTARTS   AGE</span><br><span class="line">job-demo--1-p9s5r   0/1     Completed   0          52s</span><br></pre></td></tr></table></figure>

<p><code>Job</code> 对象创建成功后，我们可以查看下对象的详细描述信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl describe job job-demo</span><br><span class="line">Name:             job-demo</span><br><span class="line">Namespace:        default</span><br><span class="line">Selector:         controller-uid=10618fc6-5610-41c6-bdeb-531167716179</span><br><span class="line">Labels:           controller-uid=10618fc6-5610-41c6-bdeb-531167716179</span><br><span class="line">                  job-name=job-demo</span><br><span class="line">Annotations:      &lt;none&gt;</span><br><span class="line">Parallelism:      1</span><br><span class="line">Completions:      1</span><br><span class="line">Completion Mode:  NonIndexed</span><br><span class="line">Start Time:       Sat, 13 Nov 2021 18:34:17 +0800</span><br><span class="line">Completed At:     Sat, 13 Nov 2021 18:34:36 +0800</span><br><span class="line">Duration:         19s</span><br><span class="line">Pods Statuses:    0 Running / 1 Succeeded / 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  controller-uid=10618fc6-5610-41c6-bdeb-531167716179</span><br><span class="line">           job-name=job-demo</span><br><span class="line">  Containers:</span><br><span class="line">   counter:</span><br><span class="line">    Image:      busybox</span><br><span class="line">    Port:       &lt;none&gt;</span><br><span class="line">    Host Port:  &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      bin/sh</span><br><span class="line">      -c</span><br><span class="line">      for i in 9 8 7 6 5 4 3 2 1; do echo $i; done</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age   From            Message</span><br><span class="line">  ----    ------            ----  ----            -------</span><br><span class="line">  Normal  SuccessfulCreate  73s   job-controller  Created pod: job-demo--1-p9s5r</span><br><span class="line">  Normal  Completed         54s   job-controller  Job completed</span><br></pre></td></tr></table></figure>

<p>可以看到，Job 对象在创建后，它的 Pod 模板，被自动加上了一个 <code>controller-uid=&lt; 一个随机字符串 &gt;</code> 这样的 Label 标签，而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。而 Job 控制器之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。</p>
<p>我们可以看到很快 Pod 变成了 <code>Completed</code> 状态，这是因为容器的任务执行完成正常退出了，我们可以查看对应的日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl logs job-demo--1-p9s5r</span><br><span class="line">9</span><br><span class="line">8</span><br><span class="line">7</span><br><span class="line">6</span><br><span class="line">5</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">➜  ~ kubectl get pod</span><br><span class="line">NAME                       READY   STATUS       RESTARTS   AGE</span><br><span class="line">job-demo--1-p9s5r          0/1     Completed    0          11m</span><br></pre></td></tr></table></figure>

<p>上面我们这里的 Job 任务对应的 Pod 在运行结束后，会变成 <code>Completed</code> 状态，但是如果执行任务的 Pod 因为某种原因一直没有结束怎么办呢？同样我们可以在 Job 对象中通过设置字段 <code>spec.activeDeadlineSeconds</code> 来限制任务运行的最长时间，比如：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"> <span class="attr">activeDeadlineSeconds:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure>

<p>那么当我们的任务 Pod 运行超过了 100s 后，这个 Job 的所有 Pod 都会被终止，并且， Pod 的终止原因会变成 <code>DeadlineExceeded</code>。</p>
<p>如果的任务执行失败了，会怎么处理呢，这个和定义的 <code>restartPolicy</code> 有关系，比如定义如下所示的 Job 任务，定义 <code>restartPolicy: Never</code> 的重启策略：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># job-failed-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">job-failed-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-job</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;echo123&quot;</span>, <span class="string">&quot;test failed job!&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure>

<p>直接创建上面的资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f job-failed-demo.yaml</span><br><span class="line">job.batch/job-failed-demo created</span><br><span class="line">➜  ~ kubectl get pod</span><br><span class="line">NAME                       READY   STATUS       RESTARTS   AGE</span><br><span class="line">job-failed-demo--1-87wvj   0/1     StartError   0          4m40s</span><br><span class="line">job-failed-demo--1-bl7jm   0/1     StartError   0          5m7s</span><br><span class="line">job-failed-demo--1-dcmph   0/1     StartError   0          3m</span><br><span class="line">job-failed-demo--1-hb24j   0/1     StartError   0          20s</span><br><span class="line">job-failed-demo--1-n7h24   0/1     StartError   0          4m20s</span><br><span class="line">job-failed-demo--1-q7vxq   0/1     StartError   0          5m24s</span><br></pre></td></tr></table></figure>

<p>可以看到当我们设置成 <code>Never</code> 重启策略的时候，Job 任务执行失败后会不断创建新的 Pod，但是不会一直创建下去，会根据 <code>spec.backoffLimit</code> 参数进行限制，默认为6，通过该字段可以定义重建 Pod 的次数，另外需要注意的是 Job 控制器重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10s、20s、40s… 后。</p>
<p>但是如果我们设置的 <code>restartPolicy: OnFailure</code> 重启策略，则当 Job 任务执行失败后不会创建新的 Pod 出来，只会不断重启 Pod。</p>
<p>除此之外，我们还可以通过设置 <code>spec.parallelism</code> 参数来进行并行控制，该参数定义了一个 Job 在任意时间最多可以有多少个 Pod 同时运行。<code>spec.completions</code> 参数可以定义 Job 至少要完成的 Pod 数目。如下所示创建一个新的 Job 任务，设置允许并行数为2，至少要完成的 Pod 数为8：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># job-para-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">job-para-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">parallelism:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-job</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;echo&quot;</span>, <span class="string">&quot;test paralle job!&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure>



<p>创建完成后查看任务状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get pod</span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">job-para-test--1-rwxm8   0/1     ContainerCreating   0          6s</span><br><span class="line">job-para-test--1-vgtxf   0/1     ContainerCreating   0          2s</span><br><span class="line">➜  ~ kubectl get job</span><br><span class="line">NAME            COMPLETIONS   DURATION   AGE</span><br><span class="line">job-para-test   0/8           29s        29s</span><br><span class="line">➜  ~ kubectl get job</span><br><span class="line">NAME            COMPLETIONS   DURATION   AGE</span><br><span class="line">job-para-test   8/8           111s       2m34s</span><br><span class="line">➜  ~ kubectl get pod</span><br><span class="line">NAME                     READY   STATUS      RESTARTS   AGE</span><br><span class="line">job-para-test--1-7nk2x   0/1     Completed   0          76s</span><br><span class="line">job-para-test--1-dcdvp   0/1     Completed   0          2m2s</span><br><span class="line">job-para-test--1-k9sgw   0/1     Completed   0          2m36s</span><br><span class="line">job-para-test--1-rwkkb   0/1     Completed   0          2m17s</span><br><span class="line">job-para-test--1-rwxm8   0/1     Completed   0          2m36s</span><br><span class="line">job-para-test--1-tqlzd   0/1     Completed   0          106s</span><br><span class="line">job-para-test--1-vgtxf   0/1     Completed   0          2m32s</span><br><span class="line">job-para-test--1-vxj6b   0/1     Completed   0          91s</span><br></pre></td></tr></table></figure>

<p>可以看到一次可以有2个 Pod 同时运行，需要8个 Pod 执行成功，如果不是8个成功，那么会根据 <code>restartPolicy</code> 的策略进行处理，可以认为是一种检查机制。</p>
<h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><p><code>CronJob</code> 其实就是在 <code>Job</code> 的基础上加上了时间调度，我们可以在给定的时间点运行一个任务，也可以周期性地在给定时间点运行。这个实际上和我们 Linux 中的 <code>crontab</code> 就非常类似了。</p>
<p>一个 <code>CronJob</code> 对象其实就对应中 <code>crontab</code> 文件中的一行，它根据配置的时间格式周期性地运行一个 <code>Job</code>，格式和 <code>crontab</code> 也是一样的。</p>
<p>crontab 的格式为：<code>分 时 日 月 星期 要运行的命令</code> 。</p>
<ul>
<li>第1列分钟 0～59</li>
<li>第2列小时 0～23</li>
<li>第3列日 1～31</li>
<li>第4列月 1～12</li>
<li>第5列星期 0～7（0和7表示星期天）</li>
<li>第6列要运行的命令</li>
</ul>
<p>现在，我们用 <code>CronJob</code> 来管理我们上面的 <code>Job</code> 任务，定义如下所示的资源清单：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cronjob-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cronjob-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">&quot;*/1 * * * *&quot;</span></span><br><span class="line">  <span class="attr">jobTemplate:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">hello</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">            <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;bin/sh&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;-c&quot;</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i; done&quot;</span></span><br></pre></td></tr></table></figure>

<p>这里的 Kind 变成了 <code>CronJob</code> 了，要注意的是 <code>.spec.schedule</code> 字段是必须填写的，用来指定任务运行的周期，格式就和 <code>crontab</code> 一样，另外一个字段是 <code>.spec.jobTemplate</code>, 用来指定需要运行的任务，格式当然和 <code>Job</code> 是一致的。还有一些值得我们关注的字段 <code>.spec.successfulJobsHistoryLimit</code>(默认为3) 和 <code>.spec.failedJobsHistoryLimit</code>(默认为1)，表示历史限制，是可选的字段，指定可以保留多少完成和失败的 <code>Job</code>。然而，当运行一个 <code>CronJob</code> 时，<code>Job</code> 可以很快就堆积很多，所以一般推荐设置这两个字段的值，如果设置限制的值为 0，那么相关类型的 <code>Job</code> 完成后将不会被保留。</p>
<p>我们直接新建上面的资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f cronjob-demo.yaml</span><br><span class="line">cronjob &quot;cronjob-demo&quot; created</span><br></pre></td></tr></table></figure>

<p>然后可以查看对应的 Cronjob 资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get cronjob</span><br><span class="line">NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">cronjob-demo   */1 * * * *   False     0        28s             36s</span><br></pre></td></tr></table></figure>

<p>稍微等一会儿查看可以发现多了几个 Job 资源对象，这个就是因为上面我们设置的 CronJob 资源对象，每1分钟执行一个新的 Job：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get job</span><br><span class="line">NAME                    COMPLETIONS   DURATION   AGE</span><br><span class="line">cronjob-demo-27280016   1/1           17s        118s</span><br><span class="line">cronjob-demo-27280017   1/1           16s        58s</span><br><span class="line">➜  ~ kubectl get pods</span><br><span class="line">NAME                             READY   STATUS              RESTARTS   AGE</span><br><span class="line">cronjob-demo-27280016--1-kvkt4   0/1     Completed           0          2m9s</span><br><span class="line">cronjob-demo-27280017--1-pkwdb   0/1     Completed           0          69s</span><br><span class="line">cronjob-demo-27280018--1-q9n7w   0/1     ContainerCreating   0          9s</span><br></pre></td></tr></table></figure>

<p>这个就是 CronJob 的基本用法，一旦不再需要 CronJob，我们可以使用 kubectl 命令删除它：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl delete cronjob cronjob-demo</span><br><span class="line">cronjob &quot;cronjob-demo&quot; deleted</span><br></pre></td></tr></table></figure>



<p>不过需要注意的是这将会终止正在创建的 Job，但是运行中的 Job 将不会被终止，不会删除 Job 或 它们的 Pod。</p>
<p>思考：那如果我们想要在每个节点上去执行一个 Job 或者 Cronjob 又该怎么来实现呢？</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Kafka%20%E4%B8%AD%E5%85%B3%E4%BA%8E%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF%E7%9A%84%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Kafka%20%E4%B8%AD%E5%85%B3%E4%BA%8E%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF%E7%9A%84%E5%AE%9E%E7%8E%B0/" class="post-title-link" itemprop="url">Kafka 中关于事务消息的实现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:45:16" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MQ/" itemprop="url" rel="index"><span itemprop="name">MQ</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MQ/Kafka/" itemprop="url" rel="index"><span itemprop="name">Kafka</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Kafka 自 0.11.0 版本引入了<strong>事务（Transactions）</strong>功能，旨在提供<strong>端到端（End-to-End）的“精确一次（Exactly-Once）”</strong>语义。这与 RocketMQ 的事务消息（主要解决本地事务与消息发送的原子性）在设计目标和实现机制上有所不同。</p>
<p>Kafka 的事务功能主要解决以下两个问题：</p>
<ol>
<li><strong>原子写入：</strong> 保证生产者（Producer）向多个 Topic 的多个分区写入的一组消息是原子性的，要么全部写入成功并对消费者可见，要么全部失败并不可见。</li>
<li><strong>原子读写：</strong> 结合消费者（Consumer）的偏移量提交，可以在一个事务中原子地处理“消费消息 -&gt; 处理逻辑 -&gt; 生产新消息 -&gt; 提交消费偏移量”这整个流程，从而实现“读取-处理-写入”的精确一次语义。</li>
</ol>
<hr>
<h3 id="Kafka-事务消息的实现核心"><a href="#Kafka-事务消息的实现核心" class="headerlink" title="Kafka 事务消息的实现核心"></a>Kafka 事务消息的实现核心</h3><p>Kafka 的事务机制基于 <strong>两阶段提交（Two-Phase Commit, 2PC）</strong>的变种，但它是<strong>针对 Kafka 内部消息处理</strong>的 2PC，不直接涵盖外部数据库事务。</p>
<h4 id="1-前提：幂等生产者（Idempotent-Producer）"><a href="#1-前提：幂等生产者（Idempotent-Producer）" class="headerlink" title="1. 前提：幂等生产者（Idempotent Producer）"></a>1. 前提：幂等生产者（Idempotent Producer）</h4><p>这是实现事务的基础。在 Kafka 0.11.0 之前，生产者重试可能导致消息重复（At-least-once 语义）。为了实现精确一次，首先要解决重复发送的问题。</p>
<ul>
<li><strong>实现原理：</strong> 生产者在初始化时会获取一个唯一的 <code>Producer ID (PID)</code>。对于发送给每个分区，生产者会为每条消息维护一个递增的序列号 (<code>Sequence Number</code>)。</li>
<li><strong>Broker 端的去重：</strong> Broker 会为每个 <code>(PID, Partition)</code> 对存储最近接收到的序列号。当 Broker 收到消息时，会检查其序列号：<ul>
<li>如果序列号是期望的下一个序列号，则接受消息并更新序列号。</li>
<li>如果序列号小于或等于已接收的序列号，则说明是重复消息， Broker 会直接丢弃（但仍会返回成功，避免生产者重试）。</li>
<li>如果序列号跳跃（大于期望的下一个序列号），则说明有消息丢失，会抛出异常。</li>
</ul>
</li>
<li><strong>效果：</strong> 保证了单个生产者对于单个分区发送的消息，在重试的情况下不会产生重复，提供了“精确一次发送”的保证。</li>
</ul>
<h4 id="2-核心组件"><a href="#2-核心组件" class="headerlink" title="2. 核心组件"></a>2. 核心组件</h4><ul>
<li><strong>Transactional Producer (事务生产者)：</strong> 生产者客户端，通过配置 <code>transactional.id</code> 来启用事务功能。它会与Transaction Coordinator进行通信。</li>
<li><strong>Transaction Coordinator (事务协调器)：</strong> 集群中的一个 Broker 充当协调器角色（类似 Consumer Group 的 Group Coordinator）。它负责管理所有事务的生命周期，包括注册事务 ID、协调事务提交&#x2F;中止过程、存储事务状态等。每个 <code>transactional.id</code> 都会映射到一个特定的 Transaction Coordinator。</li>
<li><strong>Transaction Log &#x2F; __transaction_state Topic (事务日志&#x2F;内部主题)：</strong> 一个特殊的、高复制的内部 Kafka Topic，用于持久化存储所有正在进行或已完成的事务的状态信息。这保证了即使 Transaction Coordinator 宕机也能恢复事务状态。</li>
<li><strong>Consumer Isolation Level (消费者隔离级别)：</strong> 消费者可以配置两种隔离级别：<ul>
<li><code>read_uncommitted</code> (默认)：消费者会看到所有消息，包括未提交的事务消息和已中止的事务消息。这类似于数据库的读未提交隔离级别。</li>
<li><code>read_committed</code>：消费者只会看到已提交的事务消息；对于正在进行中的事务或已中止的事务中的消息，消费者是不可见的。这是实现精确一次语义的关键。</li>
</ul>
</li>
</ul>
<h4 id="3-事务流程（Producer）"><a href="#3-事务流程（Producer）" class="headerlink" title="3. 事务流程（Producer）"></a>3. 事务流程（Producer）</h4><ol>
<li><p><strong>初始化事务：<code>initTransactions()</code></strong></p>
<ul>
<li>生产者向 <code>Transaction Coordinator</code> 注册 <code>transactional.id</code>。</li>
<li><code>Coordinator</code> 会检查该 <code>transactional.id</code> 是否有历史未完成的事务。如果有，会执行恢复或中止操作（Fencing 机制，旧的生产者实例会被“围栏”）。</li>
<li><code>Coordinator</code> 返回成功后，生产者进入可用状态。</li>
</ul>
</li>
<li><p><strong>开始事务：<code>beginTransaction()</code></strong></p>
<ul>
<li>生产者向 <code>Transaction Coordinator</code> 发送请求，表示开始一个新的事务。</li>
<li><code>Coordinator</code> 在 <code>__transaction_state</code> Log 中记录事务开始状态。</li>
</ul>
</li>
<li><p><strong>发送消息：<code>producer.send()</code></strong></p>
<ul>
<li>生产者正常发送消息，但这些消息会带有事务标记。</li>
<li>这些消息并不会立即对 <code>read_committed</code> 消费者可见，它们会先被写入到各个目标分区的日志中，但处于“未提交”状态。</li>
</ul>
</li>
<li><p><strong>发送消费偏移量到事务（可选 Atomic Read-Process-Write）：<code>sendOffsetsToTransaction(offsets, consumerGroupId)</code></strong></p>
<ul>
<li>这是实现端到端精确一次的关键一步。如果生产者也是一个消费者，它可以在这个事务中，将它已经处理过的消费偏移量原子地提交。</li>
<li>这些偏移量信息不是直接提交给 Kafka 的 <code>__consumer_offsets</code> topic，而是作为事务的一部分发送给 <code>Transaction Coordinator</code>。当事务提交时，<code>Coordinator</code> 会将这些偏移量原子地更新到 <code>__consumer_offsets</code> topic。</li>
</ul>
</li>
<li><p><strong>提交事务：<code>commitTransaction()</code></strong></p>
<ul>
<li>生产者向 <code>Transaction Coordinator</code> 发送提交请求。</li>
<li><strong>第一阶段（Prepare）：</strong> <code>Coordinator</code> 首先在 <code>__transaction_state</code> Log 中记录 <code>PREPARE_COMMIT</code> 状态，并向所有参与该事务的分区（包括消息分区和偏移量分区）发送一个 <code>Prepare Commit</code> 标记。</li>
<li><strong>第二阶段（Commit）：</strong> 一旦所有分区都响应了 <code>Prepare Commit</code>，<code>Coordinator</code> 就会在 <code>__transaction_state</code> Log 中记录 <code>COMMIT</code> 状态。然后，它会向所有参与分区发送一个 <code>Commit Marker</code>。</li>
<li>当分区收到 <code>Commit Marker</code> 后，这些消息就会对 <code>read_committed</code> 消费者可见。</li>
</ul>
</li>
<li><p><strong>中止事务：<code>abortTransaction()</code></strong></p>
<ul>
<li>如果生产者在事务进行中遇到错误，可以调用 <code>abortTransaction()</code>。</li>
<li>类似提交，<code>Coordinator</code> 会在 <code>__transaction_state</code> Log 中记录 <code>PREPARE_ABORT</code> 和 <code>ABORT</code> 状态，并向所有参与分区发送 <code>Abort Marker</code>。</li>
<li>收到 <code>Abort Marker</code> 的分区会使这些未提交的消息对 <code>read_committed</code> 消费者不可见（它们会被跳过）。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="精确一次语义的保证"><a href="#精确一次语义的保证" class="headerlink" title="精确一次语义的保证"></a>精确一次语义的保证</h3><ul>
<li><strong>生产者到 Broker：</strong> 幂等生产者保证了即使有重试，消息也不会重复写入。</li>
<li><strong>Broker 到 Consumer (read_committed)：</strong><ul>
<li>新的消息在事务提交前，不会被 <code>read_committed</code> 消费者看到。</li>
<li>如果事务中止，这些消息永远不会被 <code>read_committed</code> 消费者看到。</li>
<li>Broker 会维护每个分区的 **Last Stable Offset (LSO)**，只有 LSO 之前的消息才对 <code>read_committed</code> 消费者可见。LSO 会随着事务的提交而推进。</li>
</ul>
</li>
<li><strong>端到端 Atomic Read-Process-Write：</strong> 通过在一个事务中同时包含发送新消息和提交消费偏移量，确保了“消费了一条消息并处理后发送结果消息，然后标记这条消息已消费”是原子性的。要么结果消息发出且偏移量提交，要么都回滚。</li>
</ul>
<h3 id="缺点与考量"><a href="#缺点与考量" class="headerlink" title="缺点与考量"></a>缺点与考量</h3><ol>
<li><strong>性能开销和延迟：</strong> 事务引入了额外的网络请求（与 Coordinator 的交互、标记的写入）和协调步骤（2PC），这无疑会增加消息发送的延迟和吞吐量开销。对于对性能要求极高的场景，需要权衡是否值得。</li>
<li><strong>复杂度增加：</strong> 生产者代码需要管理事务的开始、提交和回滚，同时需要处理可能的 <code>ProducerFencedException</code>（当有重复的 <code>transactional.id</code> 启动时，旧的会被“围栏”）。</li>
<li><strong>不直接覆盖外部系统：</strong> Kafka 的事务是<strong>内部事务</strong>，它只保证<strong>在 Kafka 内部</strong>的消息原子性。它无法将 Kafka 的事务与外部数据库的事务或其他服务的事务结合起来形成一个全局的分布式事务。如果你需要实现“本地数据库事务 + Kafka 消息发送”的原子性，你仍然需要结合如<strong>本地消息表（Outbox Pattern）</strong>这样的方案。</li>
<li><strong><code>transactional.id</code> 的设计与管理：</strong> 每个事务生产者实例都需要唯一的 <code>transactional.id</code>。这个 ID 应该在应用重启后保持不变，以便 Kafka 能够恢复或中止未完成的事务。这需要仔细的规划和管理。</li>
</ol>
<p>总的来说，Kafka 的事务功能为在 Kafka 流处理应用中实现强大的精确一次语义提供了坚实的基础，特别适用于流式 ETL、聚合等场景。但它有其特定的适用范围和相应的性能与复杂度成本。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Kafka%20%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Kafka%20%E7%9A%84%E9%AB%98%E6%80%A7%E8%83%BD/" class="post-title-link" itemprop="url">Kafka 的高性能</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:45:06" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MQ/" itemprop="url" rel="index"><span itemprop="name">MQ</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MQ/Kafka/" itemprop="url" rel="index"><span itemprop="name">Kafka</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Kafka 的高性能不是单一技术带来的，而是由<strong>一系列精心设计的机制和优化</strong>共同作用的结果。我可以将其归结为以下几个核心方面：</p>
<hr>
<h3 id="1-顺序读写磁盘-Sequential-I-O"><a href="#1-顺序读写磁盘-Sequential-I-O" class="headerlink" title="1. 顺序读写磁盘 (Sequential I&#x2F;O)"></a>1. 顺序读写磁盘 (Sequential I&#x2F;O)</h3><p>这是 Kafka 性能的<strong>基石</strong>。我们通常认为磁盘 I&#x2F;O 是慢的，但这是针对<strong>随机 I&#x2F;O</strong> 而言的。磁盘的顺序读写速度非常快，甚至可以逼近内存的随机读写速度。</p>
<ul>
<li><strong>普通消息队列的问题：</strong> 传统的消息队列（如早期的 ActiveMQ）为了维护消息的状态（是否被消费），需要在磁盘上进行大量的随机读写，这严重限制了性能。</li>
<li><strong>Kafka 的做法：</strong> Kafka 将消息以<strong>追加（Append-Only）</strong>的方式写入每个分区（Partition）的日志文件（Log File）中。这种写入是纯粹的<strong>顺序写</strong>。消费时，消费者也是按照 Offset 顺序地读取数据，这同样是<strong>顺序读</strong>。</li>
<li><strong>效果：</strong> 磁盘的顺序读写性能极高，通常比内存的随机读写还要快。这使得 Kafka 能够利用廉价的大容量磁盘，同时获得极高的吞吐量。</li>
</ul>
<h3 id="2-页缓存-Page-Cache-和零拷贝-Zero-Copy"><a href="#2-页缓存-Page-Cache-和零拷贝-Zero-Copy" class="headerlink" title="2. 页缓存 (Page Cache) 和零拷贝 (Zero-Copy)"></a>2. 页缓存 (Page Cache) 和零拷贝 (Zero-Copy)</h3><p>Kafka 充分利用了操作系统的特性来进一步提升 I&#x2F;O 效率。</p>
<ul>
<li><p><strong>页缓存 (Page Cache):</strong></p>
<ul>
<li>Kafka 的数据读写操作实际上是与操作系统的<strong>页缓存</strong>（Page Cache）直接交互的，而不是直接操作物理磁盘。</li>
<li><strong>写操作：</strong> 数据被写入页缓存后，操作系统会负责在后台异步地将数据刷新（flush）到磁盘，应用程序可以立即返回，这极大地降低了写入延迟。</li>
<li><strong>读操作：</strong> 如果数据存在于页缓存中（热点数据），读取操作会直接命中内存，速度极快。即使数据不在页缓存中，由于是顺序读，操作系统也会进行<strong>预读（read-ahead）</strong>，将磁盘上后续的数据块也提前加载到页缓存中。</li>
<li><strong>优势：</strong> Kafka 把缓存的职责交给了强大的、经过深度优化的操作系统内核，而不是在 JVM 堆内存中自己管理缓存。这避免了 JVM GC（垃圾回收）带来的性能抖动，也使得 Kafka 可以利用机器上所有的空闲内存作为缓存，而不用担心 OOM (Out Of Memory)。</li>
</ul>
</li>
<li><p><strong>零拷贝 (Zero-Copy):</strong></p>
<ul>
<li>在消费数据时，传统模式下数据需要从<strong>内核空间（页缓存）</strong>复制到<strong>用户空间（Kafka 应用内存）</strong>，然后再从<strong>用户空间</strong>复制回<strong>内核空间（Socket 缓冲区）</strong>进行网络发送。这个过程有两次多余的数据拷贝和上下文切换。</li>
<li>Kafka 使用了 Linux 的 <code>sendfile</code> 系统调用，实现了<strong>零拷贝</strong>。数据可以直接从<strong>页缓存</strong>发送到<strong>网卡（Socket 缓冲区）</strong>，全程在内核空间完成，避免了数据在内核空间和用户空间之间的来回拷贝。</li>
<li><strong>效果：</strong> 极大地减少了 CPU 和内存的消耗，尤其是在数据量大、消费者多的场景下，性能提升非常显著。</li>
</ul>
</li>
</ul>
<h3 id="3-分区架构-Partitioning-和并行处理"><a href="#3-分区架构-Partitioning-和并行处理" class="headerlink" title="3. 分区架构 (Partitioning) 和并行处理"></a>3. 分区架构 (Partitioning) 和并行处理</h3><p>Kafka 的 Topic 被分为一个或多个<strong>分区（Partition）</strong>，这是其实现高并发和水平扩展的核心。</p>
<ul>
<li><strong>并行写入：</strong> 生产者可以同时向一个 Topic 的多个分区发送消息，不同的分区可以分布在不同的 Broker（服务器）上，从而实现了写入操作的并行化。</li>
<li><strong>并行消费：</strong> 一个消费者组（Consumer Group）可以有多个消费者实例，每个实例消费一个或多个分区。一个分区在同一时间只能被一个消费者组内的一个消费者消费。这使得消费负载可以被轻松地横向扩展（通过增加消费者实例）。</li>
<li><strong>效果：</strong> 分区机制将一个大的 Topic 拆分成多个小的、可独立读写的单元，使得 Kafka 的吞吐量可以随着 Broker 和消费者数量的增加而线性扩展。</li>
</ul>
<h3 id="4-批量处理-Batching"><a href="#4-批量处理-Batching" class="headerlink" title="4. 批量处理 (Batching)"></a>4. 批量处理 (Batching)</h3><p>为了减少网络开销和提高吞吐量，Kafka 在生产者和消费者两端都采用了<strong>批量处理</strong>的设计。</p>
<ul>
<li><strong>生产者批量发送：</strong> 生产者在发送消息时，并不是来一条就发一条。它会先将消息缓存起来，积累到一定数量（<code>batch.size</code>）或等待一小段时间（<code>linger.ms</code>），然后将多条消息打包成一个批次（Batch）一次性发送给 Broker。</li>
<li><strong>Broker 批量处理：</strong> Broker 也是以批次为单位写入日志文件。</li>
<li><strong>消费者批量拉取：</strong> 消费者通过 <code>fetch</code> 请求一次性从 Broker 拉取一批消息进行处理。</li>
<li><strong>效果：</strong> 批量处理大大减少了网络请求的次数（RPC），将多次小 I&#x2F;O 合并为一次大 I&#x2F;O，显著提升了整体吞吐率。</li>
</ul>
<h3 id="5-压缩-Compression"><a href="#5-压缩-Compression" class="headerlink" title="5. 压缩 (Compression)"></a>5. 压缩 (Compression)</h3><p>Kafka 支持在生产者端对消息进行压缩，然后在消费者端解压。</p>
<ul>
<li><strong>支持的压缩算法：</strong> Snappy、Gzip、LZ4、ZStandard等。</li>
<li><strong>工作方式：</strong> 生产者将一个批次的消息进行压缩后发送给 Broker。Broker 接收到压缩后的数据后，会<strong>直接以压缩的形式写入磁盘</strong>，消费时再原样发送给消费者。</li>
<li><strong>优势：</strong><ol>
<li><strong>减少网络带宽：</strong> 大大降低了网络传输的数据量。</li>
<li><strong>减少磁盘空间：</strong> 降低了 Broker 的存储成本。</li>
</ol>
<ul>
<li>因为压缩和解压发生在生产者和消费者端，Broker本身不进行解压操作，所以 Broker 的 CPU 负载并未增加。这是一种非常高效的设计。</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>将以上几点串联起来，Kafka 的高性能画像就清晰了：</p>
<ul>
<li><strong>底层存储：</strong> 利用<strong>顺序读写</strong>将磁盘用出了逼近内存的性能。</li>
<li><strong>I&#x2F;O 优化：</strong> 通过<strong>页缓存</strong>和<strong>零拷贝</strong>，将繁重的 I&#x2F;O 操作和缓存管理交给操作系统内核，避免了 JVM GC 和不必要的数据拷贝。</li>
<li><strong>并发模型：</strong> <strong>分区架构</strong>实现了读写操作的并行化，使得系统具备了极高的水平扩展能力。</li>
<li><strong>数据处理：</strong> <strong>批量处理</strong>和<strong>数据压缩</strong>大幅减少了网络和磁盘的负载，提升了有效吞吐量。</li>
</ul>
<p>正是这些设计的组合，使得 Kafka 能够以相对较低的硬件成本，支撑起每秒百万级别的消息处理能力，成为了大数据领域不可或缺的组件。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Karmada/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Karmada/" class="post-title-link" itemprop="url">Karmada</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:45:22" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">集群管理</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>37k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>33 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kubernetes-多集群管理系统-Karmada"><a href="#Kubernetes-多集群管理系统-Karmada" class="headerlink" title="Kubernetes 多集群管理系统 Karmada"></a>Kubernetes 多集群管理系统 Karmada</h1><p>Karmada（Kubernetes Armada）是 CNCF 孵化的一个 Kubernetes 管理系统，使您能够在多个 Kubernetes 集群和云中运行云原生应用程序，而无需更改应用程序。通过使用 Kubernetes 原生 API 并提供先进的调度功能，Karmada 实现了真正的开放式、多云 Kubernetes。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/msf9hv.png" alt="Karmada（Kubernetes"></p>
<p>Karmada 旨在为多云和混合云场景下的多集群应用程序管理提供即插即用的自动化，具有集中式多云管理、高可用性、故障恢复和流量调度等关键功能。</p>
<h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul>
<li>兼容 K8s 原生 API<ul>
<li>从单集群到多集群的无侵入式升级</li>
<li>现有 K8s 工具链的无缝集成</li>
</ul>
</li>
<li>开箱即用<ul>
<li>针对场景内置策略集，包括：<code>Active-active</code>、<code>Remote DR</code>、<code>Geo Redundant</code> 等。</li>
<li>在多集群上进行跨集群应用程序自动伸缩、故障转移和负载均衡。</li>
</ul>
</li>
<li>避免供应商锁定<ul>
<li>与主流云提供商集成</li>
<li>在集群之间自动分配、迁移</li>
<li>未绑定专有供应商编排</li>
</ul>
</li>
<li>集中式管理<ul>
<li>位置无关的集群管理</li>
<li>支持公有云、本地或边缘上的集群。</li>
</ul>
</li>
<li>丰富多集群调度策略<ul>
<li>集群亲和性、实例在多集群中的拆分调度&#x2F;再平衡，</li>
<li>多维 HA:区域&#x2F;AZ&#x2F;集群&#x2F;提供商</li>
</ul>
</li>
<li>开放和中立<ul>
<li>由互联网、金融、制造业、电信、云提供商等联合发起。</li>
<li>目标是与 CNCF 一起进行开放治理。</li>
</ul>
</li>
</ul>
<h2 id="Karmada-架构"><a href="#Karmada-架构" class="headerlink" title="Karmada 架构"></a>Karmada 架构</h2><p>Karmada 的架构非常类似于单个 Kubernetes 集群，他们都有一个控制平面、一个 APIServer、一个调度器和一组控制器，而且 Karmada 完全兼容 K8s 的原生 API 操作，便于各种 K8s 集群的接入。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/8y4u2n.png" alt="Karmada 架构"></p>
<p>所以同样 Karmada 的核心是其控制平面，一个完整且可工作的 Karmada 控制平面由以下组件组成。其中 <code>karmada-agent</code> 可以是可选的，这取决于集群注册模式。</p>
<p><strong>karmada-apiserver</strong></p>
<p>APIServer 是 Karmada 控制平面的一个组件，对外暴露 Karmada API 以及 Kubernetes 原生 API，APIServer 是 Karmada 控制平面的前端。</p>
<p>Karmada APIServer 是直接使用 Kubernetes 的 kube-apiserver 实现的，因此 Karmada 与 Kubernetes API 自然兼容。这也使得 Karmada 更容易实现与 Kubernetes 生态系统的集成，例如允许用户使用 kubectl 来操作 Karmada、与 ArgoCD 集成、与 Flux 集成等等。</p>
<p><strong>karmada-aggregated-apiserver</strong></p>
<p>聚合 API 服务器是使用 Kubernetes API 聚合层技术实现的扩展 API 服务器。它提供了集群 API 以及相应的子资源，例如 <code>cluster/status</code> 和 <code>cluster/proxy</code>，实现了聚合 Kubernetes API Endpoint 等可以通过 <code>karmada-apiserver</code> 访问成员集群的高级功能。</p>
<p><strong>kube-controller-manager</strong></p>
<p><code>kube-controller-manager</code> 由一组控制器组成，Karmada 只是从 Kubernetes 的官方版本中挑选了一些控制器，以保持与原生控制器一致的用户体验和行为。值得注意的是，并非所有的原生控制器都是 Karmada 所需要的。</p>
<blockquote>
<p>注意：当用户向 Karmada APIServer 提交 Deployment 或其他 Kubernetes 标准资源时，它们只记录在 Karmada 控制平面的 etcd 中。随后，这些资源会向成员集群同步。然而，这些部署资源不会在 Karmada 控制平面集群中进行 reconcile 过程（例如创建 Pod）。</p>
</blockquote>
<p><strong>karmada-controller-manager</strong></p>
<p>Karmada 控制器管理器运行了各种自定义控制器进程。控制器负责监视 Karmada 对象，并与底层集群的 API 服务器通信，以创建原生的 Kubernetes 资源。</p>
<p><strong>karmada-scheduler</strong></p>
<p><code>karmada-scheduler</code> 负责将 Kubernetes 原生 API 资源对象（以及 CRD 资源）调度到成员集群。</p>
<p>调度器依据策略约束和可用资源来确定哪些集群对调度队列中的资源是可用的，然后调度器对每个可用集群进行打分排序，并将资源绑定到最合适的集群。</p>
<p><strong>karmada-webhook</strong></p>
<p><code>karmada-webhook</code> 是用于接收 karmada&#x2F;Kubernetes API 请求的 HTTP 回调，并对请求进行处理。你可以定义两种类型的 <code>karmada-webhook</code>，即验证性质的 webhook 和修改性质的 webhook。修改性质的准入 webhook 会先被调用。它们可以更改发送到 Karmada API 服务器的对象以执行自定义的设置默认值操作。</p>
<p>在完成了所有对象修改并且 Karmada API 服务器也验证了所传入的对象之后，验证性质的 webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。</p>
<p><strong>etcd</strong></p>
<p>一致且高可用的键值存储，用作 Karmada 的所有 Karmada&#x2F;Kubernetes 资源对象数据的后台数据库。</p>
<p>如果你的 Karmada 使用 etcd 作为其后台数据库，请确保你针对这些数据有一份备份计划。</p>
<p><strong>karmada-agent</strong></p>
<p>Karmada 有 <code>Push</code> 和 <code>Pull</code> 两种集群注册模式，<code>karmada-agent</code> 应部署在每个 <code>Pull</code> 模式的成员集群上。它可以将特定集群注册到 Karmada 控制平面，并将工作负载清单从 Karmada 控制平面同步到成员集群。此外，它也负责将成员集群及其资源的状态同步到 Karmada 控制平面。</p>
<p><strong>插件（Addons）</strong></p>
<ul>
<li><code>karmada-scheduler-estimator</code></li>
</ul>
<p>Karmada 调度估计器为每个成员集群运行精确的调度预估，它为调度器提供了更准确的集群资源信息。</p>
<blockquote>
<p>注意：早期的 Karmada 调度器只支持根据集群资源的总量来决策可调度副本的数量。在这种情况下，当集群资源的总量足够但每个节点资源不足时，会发生调度失败。为了解决这个问题，引入了估计器组件，该组件根据资源请求计算每个节点的可调度副本的数量，从而计算出真正的整个集群的可调度副本的数量。</p>
</blockquote>
<ul>
<li><code>karmada-descheduler</code></li>
</ul>
<p>Karmada 重调度组件负责定时检测所有副本（默认为两分钟），并根据成员集群中副本实例状态的变化触发重新调度。</p>
<p>该组件是通过调用 <code>karmada-scheduler-estimator</code> 来感知有多少副本实例状态发生了变化，并且只有当副本的调度策略为动态划分时，它才会发挥作用。</p>
<ul>
<li><code>karmada-search</code></li>
</ul>
<p>Karmada 搜索组件以聚合服务的形式，提供了在多云环境中进行全局搜索和资源代理等功能。</p>
<p>其中，全局搜索能力是用来跨多个集群缓存资源对象和事件，以及通过搜索 API 对外提供图形化的检索服务；资源代理能力使用户既可以访问 Karmada 控制平面所有资源，又可以访问成员集群中的所有资源。</p>
<p><strong>CLI 工具</strong></p>
<ul>
<li><code>karmadactl</code></li>
</ul>
<p>Karmada 提供了一个命令行工具 <code>karmadactl</code>，用于使用 Karmada API 与 Karmada 的控制平面进行通信。</p>
<p>你可以使用 <code>karmadactl</code> 执行成员集群的添加&#x2F;剔除，将成员集群标记&#x2F;取消标记为不可调度，等等。</p>
<ul>
<li><code>kubectl karmada</code></li>
</ul>
<p><code>kubectl karmada</code> 以 kubectl 插件的形式提供功能，但它的实现与 <code>karmadactl</code> 完全相同。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>首先要注意我们使用 Karmada 管理的多集群包含两类：</p>
<ul>
<li>host 集群：即由 karmada 控制面构成的集群，接受用户提交的工作负载部署需求，将之同步到 member 集群，并从 member 集群同步工作负载后续的运行状况。</li>
<li>member 集群：由一个或多个 K8s 集群构成，负责运行用户提交的工作负载</li>
</ul>
<p>所以首先我们需要准备几个 K8s 集群用于测试，其中 host 集群就是我们要安装 Karmada 的集群，这里我们可以使用 <code>KinD</code> 部署一个 host 集群以及两个 member 集群，用于测试 Karmada 的多集群管理功能，当然首先需要在你的测试环境中安装 Docker 和 KinD。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ docker version</span><br><span class="line">Client:</span><br><span class="line"> Cloud integration: v1.0.29</span><br><span class="line"> Version:           20.10.21</span><br><span class="line"> API version:       1.41</span><br><span class="line"> Go version:        go1.18.7</span><br><span class="line"> Git commit:        baeda1f</span><br><span class="line"> Built:             Tue Oct 25 18:01:18 2022</span><br><span class="line"> OS/Arch:           darwin/arm64</span><br><span class="line"> Context:           orbstack</span><br><span class="line"> Experimental:      <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          25.0.5</span><br><span class="line">  API version:      1.44 (minimum version 1.24)</span><br><span class="line">  Go version:       go1.21.8</span><br><span class="line">  Git commit:       e63daec</span><br><span class="line">  Built:            Tue Mar 19 15:05:27 2024</span><br><span class="line">  OS/Arch:          linux/arm64</span><br><span class="line">  Experimental:     <span class="literal">false</span></span><br><span class="line"> containerd:</span><br><span class="line">  Version:          v1.7.13</span><br><span class="line">  GitCommit:        7c3aca7a610df76212171d200ca3811ff6096eb8</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.1.12</span><br><span class="line">  GitCommit:        51d5e94601ceffbbd85688df1c928ecccbfa4685</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br><span class="line">$ kind version</span><br><span class="line">kind v0.20.0 go1.20.4 darwin/arm64</span><br></pre></td></tr></table></figure>

<p>然后我们可以使用 Karmada 官方提供的 <code>create-cluster.sh</code> 脚本来创建两个 member 集群。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/karmada-io/karmada.git</span><br><span class="line">$ <span class="built_in">cd</span> karmada</span><br><span class="line"><span class="comment"># 创建 host 集群</span></span><br><span class="line">$ hack/create-cluster.sh host <span class="variable">$HOME</span>/.kube/host.config</span><br><span class="line">$ kubectl get nodes --context host --kubeconfig /Users/cnych/.kube/host.config</span><br><span class="line">NAME                 STATUS   ROLES           AGE   VERSION</span><br><span class="line">host-control-plane   Ready    control-plane   63s   v1.27.3</span><br><span class="line"><span class="comment"># 创建 member1 集群</span></span><br><span class="line">$ hack/create-cluster.sh member1 <span class="variable">$HOME</span>/.kube/member1.config</span><br><span class="line">$ kubectl get nodes --context member1 --kubeconfig /Users/cnych/.kube/member1.config</span><br><span class="line">NAME                    STATUS   ROLES           AGE    VERSION</span><br><span class="line">member1-control-plane   Ready    control-plane   115s   v1.27.3</span><br><span class="line"><span class="comment"># 创建 member2 集群</span></span><br><span class="line">$ hack/create-cluster.sh member2 <span class="variable">$HOME</span>/.kube/member2.config</span><br><span class="line">$ kubectl get nodes --context member2 --kubeconfig /Users/cnych/.kube/member2.config</span><br><span class="line">NAME                    STATUS   ROLES           AGE   VERSION</span><br><span class="line">member2-control-plane   Ready    control-plane   29s   v1.27.3</span><br></pre></td></tr></table></figure>

<p>到这里我们就准备好了一个 host 集群和两个 member 集群，接下来我们就可以在 host 集群上安装 Karmada 了。安装 Karmada 的方法有很多，可以直接使用官方的 CLI 工具，也可以使用 Helm Chart 方式，还可以使用 Operator 方式等等，如果需要定制化安装，使用 Helm Chart 的方式会更加灵活。由于官方提供的 CLI 工具并不只是用于安装 Karmada，还可以用于管理 Karmada 集群，所以无论如何我们都可以先安装 CLI 工具 - <code>karmadactl</code>，<code>karmadactl</code> 是允许你控制 Karmada 控制面的 Karmada 命令行工具，此外还提供一个 kubectl 插件 <code>kubectl-karmada</code>，尽管这两个工具的名字不同，但其关联的命令和选项完全相同，所以无论使用哪一个都是一样的，在实际使用中，你可以根据自己的需求选择一个 CLI 工具。</p>
<p>直接使用下面的命令即可一键安装 <code>karmadactl</code>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> ./hack/install-cli.sh</span><br><span class="line">[INFO]  Downloading metadata https://api.github.com/repos/karmada-io/karmada/releases/latest</span><br><span class="line">[INFO]  Using 1.9.1 as release</span><br><span class="line">[INFO]  Downloading <span class="built_in">hash</span> https://github.com/karmada-io/karmada/releases/download/v1.9.1/karmadactl-darwin-arm64.tgz.sha256</span><br><span class="line">[INFO]  Downloading binary https://github.com/karmada-io/karmada/releases/download/v1.9.1/karmadactl-darwin-arm64.tgz</span><br><span class="line">[INFO]  Verifying binary download</span><br><span class="line">[INFO]  Installing karmadactl to /usr/local/bin/karmadactl</span><br><span class="line">$ karmadactl version</span><br><span class="line">karmadactl version: version.Info&#123;GitVersion:<span class="string">&quot;v1.9.1&quot;</span>, GitCommit:<span class="string">&quot;b57bff17d6133deb26d9c319714170a915d4fa54&quot;</span>, GitTreeState:<span class="string">&quot;clean&quot;</span>, BuildDate:<span class="string">&quot;2024-04-30T02:03:53Z&quot;</span>, GoVersion:<span class="string">&quot;go1.20.11&quot;</span>, Compiler:<span class="string">&quot;gc&quot;</span>, Platform:<span class="string">&quot;darwin/arm64&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>安装 <code>kubectl-karmada</code> 与安装 <code>karmadactl</code> 相同，你只需要添加一个 <code>kubectl-karmada</code> 参数即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> ./hack/install-cli.sh kubectl-karmada</span><br><span class="line">[INFO]  Downloading metadata https://api.github.com/repos/karmada-io/karmada/releases/latest</span><br><span class="line">[INFO]  Using 1.9.1 as release</span><br><span class="line">[INFO]  Downloading <span class="built_in">hash</span> https://github.com/karmada-io/karmada/releases/download/v1.9.1/kubectl-karmada-darwin-arm64.tgz.sha256</span><br><span class="line">[INFO]  Downloading binary https://github.com/karmada-io/karmada/releases/download/v1.9.1/kubectl-karmada-darwin-arm64.tgz</span><br><span class="line">[INFO]  Verifying binary download</span><br><span class="line">[INFO]  Installing kubectl-karmada to /usr/local/bin/kubectl-karmada</span><br><span class="line">$ kubectl karmada version</span><br><span class="line">kubectl karmada version: version.Info&#123;GitVersion:<span class="string">&quot;v1.9.1&quot;</span>, GitCommit:<span class="string">&quot;b57bff17d6133deb26d9c319714170a915d4fa54&quot;</span>, GitTreeState:<span class="string">&quot;clean&quot;</span>, BuildDate:<span class="string">&quot;2024-04-30T02:03:52Z&quot;</span>, GoVersion:<span class="string">&quot;go1.20.11&quot;</span>, Compiler:<span class="string">&quot;gc&quot;</span>, Platform:<span class="string">&quot;darwin/arm64&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>接下来我们就可以在 host 集群上安装 Karmada 了，我们已将 host 集群的 <code>kubeconfig</code> 文件放到了 <code>$HOME/.kube/config</code>。直接执行以下命令即可进行安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --kube-image-mirror-country 用于指定镜像国内源</span></span><br><span class="line"><span class="comment"># --etcd-storage-mode 用于指定 etcd 存储模式，支持 emptyDir、hostPath、PVC，默认为 hostPath</span></span><br><span class="line">$ <span class="built_in">sudo</span> kubectl karmada init --kube-image-mirror-country=cn --etcd-storage-mode PVC --storage-classes-name standard --kubeconfig=<span class="variable">$HOME</span>/.kube/host.config</span><br><span class="line">I0516 15:56:35.549617   98690 deploy.go:244] kubeconfig file: /Users/cnych/.kube/host.config, kubernetes: https://192.168.247.4:6443</span><br><span class="line">I0516 15:56:35.586638   98690 deploy.go:264] karmada apiserver ip: [192.168.247.4]</span><br><span class="line">I0516 15:56:36.330162   98690 cert.go:246] Generate ca certificate success.</span><br><span class="line">I0516 15:56:36.368464   98690 cert.go:246] Generate karmada certificate success.</span><br><span class="line">I0516 15:56:36.453671   98690 cert.go:246] Generate apiserver certificate success.</span><br><span class="line">I0516 15:56:36.535924   98690 cert.go:246] Generate front-proxy-ca certificate success.</span><br><span class="line">I0516 15:56:36.666694   98690 cert.go:246] Generate front-proxy-client certificate success.</span><br><span class="line">I0516 15:56:36.716602   98690 cert.go:246] Generate etcd-ca certificate success.</span><br><span class="line">I0516 15:56:36.772838   98690 cert.go:246] Generate etcd-server certificate success.</span><br><span class="line">I0516 15:56:36.905275   98690 cert.go:246] Generate etcd-client certificate success.</span><br><span class="line">I0516 15:56:36.905808   98690 deploy.go:360] download crds file:https://github.com/karmada-io/karmada/releases/download/v1.9.1/crds.tar.gz</span><br><span class="line">Downloading...[ 100.00% ]</span><br><span class="line">Download complete.</span><br><span class="line">I0516 15:56:39.224167   98690 deploy.go:620] Create karmada kubeconfig success.</span><br><span class="line">I0516 15:56:39.300133   98690 idempotency.go:267] Namespace karmada-system has been created or updated.</span><br><span class="line">I0516 15:56:39.352865   98690 idempotency.go:291] Service karmada-system/etcd has been created or updated.</span><br><span class="line">I0516 15:56:39.353105   98690 deploy.go:426] Create etcd StatefulSets</span><br><span class="line">I0516 15:57:02.386423   98690 deploy.go:435] Create karmada ApiServer Deployment</span><br><span class="line">I0516 15:57:02.412127   98690 idempotency.go:291] Service karmada-system/karmada-apiserver has been created or updated.</span><br><span class="line">I0516 15:57:33.480629   98690 deploy.go:450] Create karmada aggregated apiserver Deployment</span><br><span class="line">I0516 15:57:33.488145   98690 idempotency.go:291] Service karmada-system/karmada-aggregated-apiserver has been created or updated.</span><br><span class="line">I0516 15:57:48.545482   98690 idempotency.go:267] Namespace karmada-system has been created or updated.</span><br><span class="line">I0516 15:57:48.547067   98690 deploy.go:85] Initialize karmada bases crd resource `/etc/karmada/crds/bases`</span><br><span class="line">I0516 15:57:48.549059   98690 deploy.go:240] Attempting to create CRD</span><br><span class="line">I0516 15:57:48.569222   98690 deploy.go:250] Create CRD cronfederatedhpas.autoscaling.karmada.io successfully.</span><br><span class="line"><span class="comment"># ......省略部分输出</span></span><br><span class="line">I0516 15:57:49.963201   98690 deploy.go:96] Initialize karmada patches crd resource `/etc/karmada/crds/patches`</span><br><span class="line">I0516 15:57:50.372020   98690 deploy.go:108] Create MutatingWebhookConfiguration mutating-config.</span><br><span class="line">I0516 15:57:50.379939   98690 webhook_configuration.go:362] MutatingWebhookConfiguration mutating-config has been created or updated successfully.</span><br><span class="line">I0516 15:57:50.379957   98690 deploy.go:113] Create ValidatingWebhookConfiguration validating-config.</span><br><span class="line">I0516 15:57:50.387416   98690 webhook_configuration.go:333] ValidatingWebhookConfiguration validating-config has been created or updated successfully.</span><br><span class="line">I0516 15:57:50.387434   98690 deploy.go:119] Create Service <span class="string">&#x27;karmada-aggregated-apiserver&#x27;</span> and APIService <span class="string">&#x27;v1alpha1.cluster.karmada.io&#x27;</span>.</span><br><span class="line">I0516 15:57:50.390795   98690 idempotency.go:291] Service karmada-system/karmada-aggregated-apiserver has been created or updated.</span><br><span class="line">I0516 15:57:50.394479   98690 check.go:42] Waiting <span class="keyword">for</span> APIService(v1alpha1.cluster.karmada.io) condition(Available), will try</span><br><span class="line">I0516 15:57:51.506085   98690 tlsbootstrap.go:49] [bootstrap-token] configured RBAC rules to allow Karmada Agent Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> agent to get long term certificate credentials</span><br><span class="line">I0516 15:57:51.508289   98690 tlsbootstrap.go:63] [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Karmada Agent Bootstrap Token</span><br><span class="line">I0516 15:57:51.511340   98690 tlsbootstrap.go:77] [bootstrap-token] configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all agent client certificates <span class="keyword">in</span> the member cluster</span><br><span class="line">I0516 15:57:51.635344   98690 deploy.go:143] Initialize karmada bootstrap token</span><br><span class="line">I0516 15:57:51.656584   98690 deploy.go:468] Create karmada kube controller manager Deployment</span><br><span class="line">I0516 15:57:51.671152   98690 idempotency.go:291] Service karmada-system/kube-controller-manager has been created or updated.</span><br><span class="line">I0516 15:57:58.728859   98690 deploy.go:482] Create karmada scheduler Deployment</span><br><span class="line">I0516 15:58:10.763913   98690 deploy.go:493] Create karmada controller manager Deployment</span><br><span class="line">I0516 15:58:22.787659   98690 deploy.go:504] Create karmada webhook Deployment</span><br><span class="line">I0516 15:58:22.798328   98690 idempotency.go:291] Service karmada-system/karmada-webhook has been created or updated.</span><br><span class="line"></span><br><span class="line">------------------------------------------------------------------------------------------------------</span><br><span class="line"> █████   ████   █████████   ███████████   ██████   ██████   █████████   ██████████     █████████</span><br><span class="line">░░███   ███░   ███░░░░░███ ░░███░░░░░███ ░░██████ ██████   ███░░░░░███ ░░███░░░░███   ███░░░░░███</span><br><span class="line"> ░███  ███    ░███    ░███  ░███    ░███  ░███░█████░███  ░███    ░███  ░███   ░░███ ░███    ░███</span><br><span class="line"> ░███████     ░███████████  ░██████████   ░███░░███ ░███  ░███████████  ░███    ░███ ░███████████</span><br><span class="line"> ░███░░███    ░███░░░░░███  ░███░░░░░███  ░███ ░░░  ░███  ░███░░░░░███  ░███    ░███ ░███░░░░░███</span><br><span class="line"> ░███ ░░███   ░███    ░███  ░███    ░███  ░███      ░███  ░███    ░███  ░███    ███  ░███    ░███</span><br><span class="line"> █████ ░░████ █████   █████ █████   █████ █████     █████ █████   █████ ██████████   █████   █████</span><br><span class="line">░░░░░   ░░░░ ░░░░░   ░░░░░ ░░░░░   ░░░░░ ░░░░░     ░░░░░ ░░░░░   ░░░░░ ░░░░░░░░░░   ░░░░░   ░░░░░</span><br><span class="line">------------------------------------------------------------------------------------------------------</span><br><span class="line">Karmada is installed successfully.</span><br><span class="line"></span><br><span class="line">Register Kubernetes cluster to Karmada control plane.</span><br><span class="line"></span><br><span class="line">Register cluster with <span class="string">&#x27;Push&#x27;</span> mode</span><br><span class="line"></span><br><span class="line">Step 1: Use <span class="string">&quot;kubectl karmada join&quot;</span> <span class="built_in">command</span> to register the cluster to Karmada control plane. --cluster-kubeconfig is kubeconfig of the member cluster.</span><br><span class="line">(In karmada)~# MEMBER_CLUSTER_NAME=$(<span class="built_in">cat</span> ~/.kube/config  | grep current-context | sed <span class="string">&#x27;s/: /\n/g&#x27;</span>| sed <span class="string">&#x27;1d&#x27;</span>)</span><br><span class="line">(In karmada)~# kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config  <span class="built_in">join</span> <span class="variable">$&#123;MEMBER_CLUSTER_NAME&#125;</span> --cluster-kubeconfig=<span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Step 2: Show members of karmada</span><br><span class="line">(In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Register cluster with <span class="string">&#x27;Pull&#x27;</span> mode</span><br><span class="line"></span><br><span class="line">Step 1: Use <span class="string">&quot;kubectl karmada register&quot;</span> <span class="built_in">command</span> to register the cluster to Karmada control plane. <span class="string">&quot;--cluster-name&quot;</span> is <span class="built_in">set</span> to cluster of current-context by default.</span><br><span class="line">(In member cluster)~# kubectl karmada register 192.168.247.4:32443 --token rflrr9.iisxtboo8dsz8jsv --discovery-token-ca-cert-hash sha256:008fb63e3b17c3e399f9688eca0978ab3a50dbe5d5b8d4f32c6bfd1fab12a1d8</span><br><span class="line"></span><br><span class="line">Step 2: Show members of karmada</span><br><span class="line">(In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters</span><br></pre></td></tr></table></figure>

<p>安装正常的话会看到如上所示的输出信息。默认 Karmada 会安装在 host 集群的 <code>karmada-system</code> 命名空间中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n karmada-system --kubeconfig ~/.kube/host.config</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">etcd-0                                          1/1     Running   0          35m</span><br><span class="line">karmada-aggregated-apiserver-5fddf66847-nnfzv   1/1     Running   0          34m</span><br><span class="line">karmada-apiserver-6b6f5b45-fkbk4                1/1     Running   0          35m</span><br><span class="line">karmada-controller-manager-bbdf689db-rc67z      1/1     Running   0          34m</span><br><span class="line">karmada-scheduler-78f854fbd4-m24c8              1/1     Running   0          34m</span><br><span class="line">karmada-webhook-77b9945cf9-mkjrk                1/1     Running   0          33m</span><br><span class="line">kube-controller-manager-5c4975bf8d-6tx5r        1/1     Running   0          34m</span><br></pre></td></tr></table></figure>

<p>如上所示 Karmada 控制平面相关 Pod 都已经正常运行，接下来我们就可以将两个 member 集群注册到 Karmada 控制平面中了，注册集群有两种方式，一种是 <code>Push</code> 模式，一种是 <code>Pull</code> 模式：</p>
<ul>
<li><code>Push</code>：Karmada 控制平面将直接访问成员集群的 kube-apiserver 以获取集群状态并部署清单。</li>
<li><code>Pull</code>：Karmada 控制平面不会访问成员集群，而是将其委托给名为 <code>Karmada-agent</code> 的额外组件。</li>
</ul>
<p>我们这里的集群都使用的 KinD 搭建的，所以使用 <code>Push</code> 模式更方便，对于无法直接访问成员集群的环境下面可以使用 <code>Pull</code> 模式。</p>
<p>我们可以使用 <code>kubectl karmada join</code> 命令来注册集群到 Karmada 控制平面。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config <span class="built_in">join</span> member1 --cluster-kubeconfig=<span class="variable">$HOME</span>/.kube/member1.config</span><br><span class="line"><span class="built_in">sudo</span> kubectl karmada --kubeconfig /etc/karmada/karmada-apiserver.config <span class="built_in">join</span> member2 --cluster-kubeconfig=<span class="variable">$HOME</span>/.kube/member2.config</span><br></pre></td></tr></table></figure>

<p>注册成功后可以查看注册的集群列表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters</span><br><span class="line">NAME      VERSION   MODE   READY   AGE</span><br><span class="line">member1   v1.27.3   Push   True    12m</span><br><span class="line">member2   v1.27.3   Push   True    2s</span><br></pre></td></tr></table></figure>

<p>到这里我们就完成了 Karmada 的安装和集群注册，接下来我们就可以使用 Karmada 来管理多集群了。</p>
<h2 id="资源分发"><a href="#资源分发" class="headerlink" title="资源分发"></a>资源分发</h2><p>接下来我们创建一个 Deployment 资源，然后使用 Karmada 将其分发到 member1 和 member2 集群中。首先创建如下所示的 Deployment 资源：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br></pre></td></tr></table></figure>

<p>要注意我们需要使用 Karmada 控制平面的 <code>kubeconfig</code> 文件来创建资源对象，因为 Karmada 控制平面会将资源对象分发到成员集群中，所以在应用资源对象时需要使用 <code>--kubeconfig /etc/karmada/karmada-apiserver.config</code> 参数。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># karmada-apiserver 是与 Karmada 控制面交互时要使用的主要 kubeconfig</span></span><br><span class="line">$ kubectl apply -f nginx-demo.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">$ kubectl get pods --kubeconfig ~/.kube/member1.config</span><br><span class="line">No resources found <span class="keyword">in</span> default namespace.</span><br><span class="line">$ kubectl get pods --kubeconfig ~/.kube/member2.config</span><br><span class="line">No resources found <span class="keyword">in</span> default namespace.</span><br></pre></td></tr></table></figure>

<p>现在成员集群 member1 和 member2 下面并没有对应的对象。要进行资源分发我们需要使用一个名为 <code>PropagationPolicy</code>（或者 <code>ClusterPropagationPolicy</code>）的资源对象，该资源对象定义了如何将资源分发到成员集群中。比如我们要将上面的 Deployment 对象分发到 member1 和 member2 集群中，我们可以创建如下所示的 <code>PropagationPolicy</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx-propagation.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-propagation</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line">    <span class="attr">replicaScheduling:</span></span><br><span class="line">      <span class="attr">replicaDivisionPreference:</span> <span class="string">Weighted</span></span><br><span class="line">      <span class="attr">replicaSchedulingType:</span> <span class="string">Divided</span></span><br><span class="line">      <span class="attr">weightPreference:</span></span><br><span class="line">        <span class="attr">staticWeightList:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">targetCluster:</span></span><br><span class="line">              <span class="attr">clusterNames:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">            <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">targetCluster:</span></span><br><span class="line">              <span class="attr">clusterNames:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line">            <span class="attr">weight:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>在上面的 <code>PropagationPolicy</code> 对象中，首先我们通过 <code>resourceSelectors</code> 属性指定了要分发的资源对象，然后通过 <code>placement</code> 字段，指定了资源对象的分发策略。</p>
<p>其中 <code>.spec.placement.clusterAffinity</code> 字段表示对特定集群集合的调度限制，没有该限制，任何集群都可以成为调度候选者，该字段包含以下几个属性：</p>
<ul>
<li><code>LabelSelector</code>：用于选择集群的标签，<code>matchLabels</code> 和 <code>matchExpressions</code> 两种方式都支持。</li>
<li><code>FieldSelector</code>：按字段选择成员集群的过滤器。</li>
<li><code>ClusterNames</code>：直接指定所选的集群。</li>
<li><code>ExcludeClusters</code>：排除指定的集群。</li>
</ul>
<p>比如我们这里直接通过 <code>clusterNames</code> 属性指定了 member1 和 member2 集群，这意味着 Deployment 对象 <code>nginx</code> 可以被分发到 member1 和 member2 集群中。</p>
<p>此外我们还可以设置 <code>ClusterAffinities</code> 字段来声明多个集群组。调度器将按照它们在规范中出现的顺序逐一评估这些组，不满足调度限制的组将被忽略，这意味着该组中的所有集群都不会被选择。如果没有一个组满足调度限制，则调度失败，这意味着不会选择任何集群。</p>
<p>另外还要注意 <code>ClusterAffinities</code> 不能与 <code>ClusterAffinity</code> 共存。如果 <code>ClusterAffinity</code> 和 <code>ClusterAffinities</code> 均未设置，则任何集群都可以作为调度候选者。</p>
<p>比如现在我们有两个分组的集群，其中本地数据中心的私有集群可以是主要的集群，云提供商提供的托管集群可以是次组。因此，Karmada 调度程序更愿意将工作负载调度到主集群组，并且只有在主组不满足限制（例如缺乏资源）的情况下才会考虑第二组集群，那么就可以配置如下所示的 <code>PropagationPolicy</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-propagation</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinities:</span> <span class="comment"># 逐一评估这些组</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">affinityName:</span> <span class="string">local-clusters</span></span><br><span class="line">        <span class="attr">clusterNames:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">local-member1</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">local-member2</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">affinityName:</span> <span class="string">cloud-clusters</span></span><br><span class="line">        <span class="attr">clusterNames:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">public-cloud-member1</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">public-cloud-member2</span></span><br><span class="line">    <span class="comment">#...</span></span><br></pre></td></tr></table></figure>

<p>又比如对于灾难恢复的场景，集群可以分为 <code>primary</code> 集群和 <code>backup</code> 集群，工作负载将首先调度到主集群，当主集群发生故障（例如数据中心断电）时，Karmada 调度程序可以迁移工作负载到备份集群。这种情况下可以配置如下所示的 <code>PropagationPolicy</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-propagation</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinities:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">affinityName:</span> <span class="string">primary-clusters</span></span><br><span class="line">        <span class="attr">clusterNames:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">affinityName:</span> <span class="string">backup-clusters</span></span><br><span class="line">        <span class="attr">clusterNames:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line">    <span class="comment">#...</span></span><br></pre></td></tr></table></figure>

<p>现在我们已经指定了分发的集群，那么具体应该如何调度呢？哪一个集群应该有多少副本呢？这就需要指定调度策略了。和原生 Kubernetes 类似，Karmada 支持多种调度策略，比如支持容忍污点、权重等。</p>
<p>通过 <code>.spec.placement.clusterTolerations</code> 字段可以设置容忍度，与 kubernetes 一样，容忍需要与集群上的污点结合使用。在集群上设置一个或多个污点后，无法在这些集群上调度或运行工作负载，除非策略明确声明可以容忍这些污点。Karmada 目前支持效果为 <code>NoSchedule</code> 和 <code>NoExecute</code> 的污点。我们可以使用 <code>karmadactl taint</code> 命令来设置集群的污点：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为集群 foo 设置包含键 dedicated、值 special-user 和效果 NoSchedule 的污点</span></span><br><span class="line"><span class="comment"># 如果具有该键和效果的污点已经存在，则其值将按指定替换</span></span><br><span class="line">karmadactl taint clusters foo dedicated=special-user:NoSchedule</span><br></pre></td></tr></table></figure>

<p>为了调度到上述集群，我们需要在 <code>PropagationPolicy</code> 中声明以下内容：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-propagation</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterTolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">dedicated</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">special-user</span></span><br><span class="line">        <span class="attr">Effect:</span> <span class="string">NoSchedule</span></span><br></pre></td></tr></table></figure>

<p>我们常常使用 <code>NoExecute</code> 污点来实现多集群故障转移。</p>
<p>然后更多的时候我们需要设置副本调度策略，我们可以通过 <code>.spec.placement.replicaScheduling</code> 字段来设置副本调度策略，该字段表示将规范中具有副本的资源传播到成员集群时处理副本数量的调度策略。Karmada 一共提供了两种副本调度类型，用于确定 Karmada 传播资源时如何调度副本：</p>
<ul>
<li><code>Duplicated</code>：从资源中将相同的副本复制到每个候选成员集群。</li>
<li><code>Divided</code>：根据有效候选成员集群的数量将副本划分为若干部分，每个集群的确切副本由 <code>ReplicaDivisionPreference</code> 确定。</li>
</ul>
<p><code>ReplicaDivisionPreference</code> 用于描述当 <code>ReplicaSchedulingType</code> 为 <code>Divided</code> 时副本如何被划分，也提供了两种副本划分方式：</p>
<ul>
<li><code>Aggregated</code>：将副本尽可能少地划分到集群，同时在划分过程中尊重集群的资源可用性。</li>
<li><code>Weighted</code>：根据 <code>WeightPreference</code> 按权重划分副本，一共有两种方式。<code>StaticWeightList</code> 根据权重静态分配副本到目标集群，可以通过 <code>ClusterAffinity</code> 选择目标集群。<code>DynamicWeight</code> 指定生成动态权重列表的因子，如果指定，<code>StaticWeightList</code> 将被忽略。</li>
</ul>
<p>上面我们创建的 Nginx 的 <code>PropagationPolicy</code> 对象中，我们指定了 <code>ReplicaDivisionPreference</code> 为 <code>Weighted</code>，<code>ReplicaSchedulingType</code> 为 <code>Divided</code>，<code>weightPreference</code> 为 <code>1</code>，表示两个集群的权重相同，这意味着副本将均匀地传播到 member1 和 member2。</p>
<p>我们这里直接应用传播策略资源对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f samples/nginx/propagationpolicy.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">propagationpolicy.policy.karmada.io/nginx-propagation created</span><br><span class="line">$ <span class="built_in">sudo</span> kubectl get propagationpolicy --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                AGE</span><br><span class="line">nginx-propagation   31s</span><br></pre></td></tr></table></figure>

<p>当创建 <code>PropagationPolicy</code> 对象后，Karmada 控制平面 watch 到过后就会自动将资源对象分发到成员集群中，我们可以查看 Deployment 对象的状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl describe deploy nginx --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line">Events:</span><br><span class="line">  Type    Reason                  Age                    From                                Message</span><br><span class="line">  ----    ------                  ----                   ----                                -------</span><br><span class="line">  Normal  ApplyPolicySucceed      2m17s (x2 over 2m17s)  resource-detector                   Apply policy(default/nginx-propagation) succeed</span><br><span class="line">  Normal  SyncWorkSucceed         2m17s (x3 over 2m17s)  binding-controller                  Sync work of resourceBinding(default/nginx-deployment) successful.</span><br><span class="line">  Normal  ScheduleBindingSucceed  2m17s                  default-scheduler                   Binding has been scheduled successfully.</span><br><span class="line">  Normal  SyncSucceed             2m17s                  execution-controller                Successfully applied resource(default/nginx) to cluster member2</span><br><span class="line">  Normal  SyncSucceed             2m17s                  execution-controller                Successfully applied resource(default/nginx) to cluster member1</span><br><span class="line">  Normal  AggregateStatusSucceed  2m2s (x9 over 2m17s)   resource-binding-status-controller  Update resourceBinding(default/nginx-deployment) with AggregatedStatus successfully.</span><br></pre></td></tr></table></figure>

<p>可以看到 Deployment 对象已经成功分发到了 member1 和 member2 集群中，我们也可以查看 member1 和 member2 集群中的 Pod 对象来进行验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods --kubeconfig ~/.kube/member1.config</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-77b4fdf86c-54qhc   1/1     Running   0          2m59s</span><br><span class="line">$ kubectl get pods --kubeconfig ~/.kube/member2.config</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-77b4fdf86c-9x98b   1/1     Running   0          3m24s</span><br></pre></td></tr></table></figure>

<p>和我们声明的副本调度策略一样，两个 Pod 对象均匀地分布在 member1 和 member2 集群中。</p>
<h2 id="分发-CRD"><a href="#分发-CRD" class="headerlink" title="分发 CRD"></a>分发 CRD</h2><p>除了内置的资源对象之外，Karmada 还支持分发自定义资源对象（CRD）。这里我们以 Karmada 仓库中的 <code>guestbook</code> 为例进行说明。</p>
<p>首先进入 Karmada 仓库的 guestbook 目录下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  <span class="built_in">cd</span> samples/guestbook</span><br><span class="line">➜  guestbook git:(master) ll</span><br><span class="line">total 48</span><br><span class="line">-rw-r--r--  1 cnych  staff   1.8K May 16 11:26 README.md</span><br><span class="line">-rw-r--r--  1 cnych  staff   135B May 16 11:26 guestbook.yaml</span><br><span class="line">-rw-r--r--  1 cnych  staff   353B May 16 11:26 guestbooks-clusterpropagationpolicy.yaml</span><br><span class="line">-rw-r--r--  1 cnych  staff   2.7K May 16 11:26 guestbooks-crd.yaml</span><br><span class="line">-rw-r--r--  1 cnych  staff   455B May 16 11:26 guestbooks-overridepolicy.yaml</span><br><span class="line">-rw-r--r--  1 cnych  staff   255B May 16 11:26 guestbooks-propagationpolicy.yaml</span><br></pre></td></tr></table></figure>

<p>然后在 Karmada 的控制平面上创建 Guestbook CRD：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> kubectl apply -f guestbooks-crd.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>该 CRD 应该被应用到 <code>karmada-apiserver</code>。</p>
<p>然后我们可以创建一个 <code>ClusterPropagationPolicy</code> 对象，将 Guestbook CRD 分发到 member1，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># guestbooks-clusterpropagationpolicy.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterPropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">guestbooks.webapp.my.domain</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是 <code>CustomResourceDefinition</code> 是全局资源，所以我们使用 <code>ClusterPropagationPolicy</code> 对象来分发，该对象的配置和 <code>PropagationPolicy</code> 对象类似，注意 <code>resourceSelectors</code> 字段中的 <code>apiVersion</code> 和 <code>kind</code> 需要设置为 <code>apiextensions.k8s.io/v1</code> 和 <code>CustomResourceDefinition</code>，<code>name</code> 字段需要设置为 Guestbook CRD 的名称。</p>
<p>然后我们直接创建 <code>ClusterPropagationPolicy</code> 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> kubectl apply -f guestbooks-clusterpropagationpolicy.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>应用后正常就会将 Guestbook CRD 对象分发到 member1 集群中。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get crd --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                          CLUSTER   CREATED AT             ADOPTION</span><br><span class="line">guestbooks.webapp.my.domain   member1   2024-05-18T11:56:10Z   Y</span><br><span class="line">$ kubectl get crd --kubeconfig ~/.kube/member1.config</span><br><span class="line">NAME                          CREATED AT</span><br><span class="line">guestbooks.webapp.my.domain   2024-05-18T11:56:10Z</span><br><span class="line">$ kubectl get crd --kubeconfig ~/.kube/member2.config</span><br><span class="line">No resources found</span><br></pre></td></tr></table></figure>

<p>接下来我们就可以部署分发 Guestbook CRD 对象了，我们可以创建一个 Guestbook CR 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># guestbook.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">webapp.my.domain/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Guestbook</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">guestbook-sample</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">size:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">configMapName:</span> <span class="string">test</span></span><br><span class="line">  <span class="attr">alias:</span> <span class="string">Name</span></span><br></pre></td></tr></table></figure>

<p>同样在 Karmada 控制平面上应用该 Guestbook CR 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f guestbook.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>然后就可以创建 <code>PropagationPolicy</code> 对象，将 <code>guestbook-sample</code> 分发到 member1 集群：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># guestbooks-propagationpolicy.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">webapp.my.domain/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Guestbook</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br></pre></td></tr></table></figure>

<p>上面的 <code>PropagationPolicy</code> 对象和我们之前创建的类似，只是这里的 <code>resourceSelectors</code> 字段中的 <code>apiVersion</code> 和 <code>kind</code> 需要设置为 <code>webapp.my.domain/v1</code> 和 <code>Guestbook</code>（我们自己的 CRD）。同样直接应用该 <code>PropagationPolicy</code> 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f guestbooks-propagationpolicy.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>应用后就可以将 <code>guestbook-sample</code> 这个 Guestbook CR 对象分发到 member1 集群中了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get guestbook --kubeconfig ~/.kube/member1.config</span><br><span class="line">NAME               AGE</span><br><span class="line">guestbook-sample   39s</span><br></pre></td></tr></table></figure>

<p>可以看到 CRD 的分发和普通资源对象的分发原理是一样的，只是需要先将 CRD 对象分发到成员集群中。</p>
<p>有的时候我们可能需要对分发的资源到不同集群进行一些覆盖操作，这个时候我们就可以使用 <code>OverridePolicy</code> 和 <code>ClusterOverridePolicy</code> 对象，用于声明资源传播到不同集群时的覆盖规则。</p>
<p>比如我们创建一个 <code>OverridePolicy</code> 对象，用于覆盖 member1 中 <code>guestbook-sample</code> 的 size 字段，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">OverridePolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">guestbook-sample</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">webapp.my.domain/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Guestbook</span></span><br><span class="line">  <span class="attr">overrideRules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">targetCluster:</span></span><br><span class="line">        <span class="attr">clusterNames:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">      <span class="attr">overriders:</span></span><br><span class="line">        <span class="attr">plaintext:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/spec/size</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">replace</span></span><br><span class="line">            <span class="attr">value:</span> <span class="number">4</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/metadata/annotations</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">add</span></span><br><span class="line">            <span class="attr">value:</span> &#123; <span class="attr">&quot;OverridePolicy&quot;:</span> <span class="string">&quot;test&quot;</span> &#125;</span><br></pre></td></tr></table></figure>

<p>上面的对象中通过 <code>resourceSelectors</code> 字段指定了要覆盖的资源对象，然后通过 <code>overrideRules</code> 字段指定了覆盖规则，<code>targetCluster</code> 字段指定了目标集群，<code>overriders</code> 字段指定了覆盖规则，这里我们将 <code>guestbook-sample</code> 的 size 字段覆盖为 4，同时添加了一个 <code>OverridePolicy: test</code> 的注解。</p>
<p>我们直接应用该 <code>OverridePolicy</code> 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f guestbooks-overridepolicy.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>创建完成后可以查看 member1 集群中的 <code>guestbook-sample</code> 对象来进行验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get guestbook guestbook-sample --kubeconfig ~/.kube/member1.config -oyaml</span><br><span class="line">apiVersion: webapp.my.domain/v1</span><br><span class="line">kind: Guestbook</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    OverridePolicy: <span class="built_in">test</span></span><br><span class="line"><span class="comment"># ......</span></span><br><span class="line">  name: guestbook-sample</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">&quot;82669&quot;</span></span><br><span class="line">  uid: 5893b85d-3946-44a0-b210-d67bd021cb65</span><br><span class="line">spec:</span><br><span class="line">  <span class="built_in">alias</span>: Name</span><br><span class="line">  configMapName: <span class="built_in">test</span></span><br><span class="line">  size: 4</span><br></pre></td></tr></table></figure>

<p>可以看到 <code>guestbook-sample</code> 对象的 <code>size</code> 字段已经被覆盖为 4，同时添加了一个 <code>OverridePolicy: test</code> 的注解，证明覆盖操作成功。</p>
<p>Karmada 提供了多种声明覆盖规则的方案：</p>
<ul>
<li><code>ImageOverrider</code>：覆盖工作负载的镜像。</li>
<li><code>CommandOverrider</code>：覆盖工作负载的命令。</li>
<li><code>ArgsOverrider</code>：覆盖工作负载的参数。</li>
<li><code>LabelsOverrider</code>：覆盖工作负载的标签。</li>
<li><code>AnnotationsOverrider</code>：覆盖工作负载的注释。</li>
<li><code>PlaintextOverrider</code>：用于覆盖任何类型资源的通用工具。</li>
</ul>
<p><strong>PlaintextOverrider</strong></p>
<p>上面我们使用的是 <code>PlaintextOverrider</code> 覆盖规则，可以覆盖任何类型资源的字段。<code>PlaintextOverrider</code> 可以根据路径、运算符和值覆盖目标字段，就像 <code>kubectl patch</code> 一样。允许的操作如下：</p>
<ul>
<li><code>add</code>：向资源追加一个或多个元素。</li>
<li><code>remove</code>：从资源中删除一个或多个元素。</li>
<li><code>replace</code>：替换资源中的一个或多个元素。</li>
</ul>
<p><strong>ImageOverrider</strong></p>
<p><code>ImageOverrider</code> 用于覆盖工作负载的镜像，用于覆盖格式为 <code>[registry/]repository[:tag|@digest]</code>（例如 <code>/spec/template/spec/containers/0/image</code> ）的镜像。允许的操作如下：</p>
<ul>
<li><code>add</code>：将注册表、存储库或 tag&#x2F;digest 附加到容器中的镜像。</li>
<li><code>remove</code>：从容器中的镜像中删除注册表、存储库或 tag&#x2F;digest。</li>
<li><code>replace</code>：替换容器中镜像的注册表、存储库或 tag&#x2F;digest。</li>
</ul>
<p>比如我们需要创建一个如下所示的 Deployment 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">myapp:1.0.0</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">myapp</span></span><br></pre></td></tr></table></figure>

<p>当工作负载传播到特定集群时添加注册表，可以使用如下所示的 <code>OverridePolicy</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">OverridePolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment">#...</span></span><br><span class="line">  <span class="attr">overrideRules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">overriders:</span></span><br><span class="line">        <span class="attr">imageOverrider:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">component:</span> <span class="string">Registry</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">add</span></span><br><span class="line">            <span class="attr">value:</span> <span class="string">test-repo</span></span><br></pre></td></tr></table></figure>

<p>上面的覆盖规则表示添加 <code>test-repo</code> 这个镜像仓库到 <code>myapp</code> 的镜像中，这样在传播到集群时就会变成 <code>test-repo/myapp:1.0.0</code>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">test-repo/myapp:1.0.0</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">myapp</span></span><br></pre></td></tr></table></figure>

<p><code>replace</code> 和 <code>remove</code> 操作也是类似的，只是分别用于替换和删除镜像中的某些字段。</p>
<h2 id="跨集群弹性伸缩"><a href="#跨集群弹性伸缩" class="headerlink" title="跨集群弹性伸缩"></a>跨集群弹性伸缩</h2><p>在 Karmada 中，我们可以使用 <code>FederatedHPA</code> 来实现跨多个集群扩展&#x2F;缩小工作负载的副本，旨在根据需求自动调整工作负载的规模。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/lm8efw.png" alt="FederatedHPA"></p>
<p>当负载增加时，如果 Pod 的数量低于配置的最大值，则 <code>FederatedHPA</code> 扩展工作负载（例如 Deployment、StatefulSet 或其他类似资源）的副本数。当负载减少时，如果 Pod 的数量高于配置的最小值，则 <code>FederatedHPA</code> 缩小工作负载的副本数。</p>
<p><code>FederatedHPA</code> 是作为 Karmada API 资源和控制器实现的，该资源确定了控制器的行为。<code>FederatedHPA</code> 控制器运行在 Karmada 控制平面中，定期调整其目标（例如 Deployment）的所需规模，以匹配观察到的指标，例如平均 CPU 利用率、平均内存利用率或任何其他自定义指标。</p>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/rdv8z5.png" alt="FederatedHPA实现原理"></p>
<p>为了实现跨集群的自动扩缩容，Karmada 引入了 <code>FederatedHPA</code> 控制器和 <code>karmada-metrics-adapter</code>，它们的工作方式如下：</p>
<ul>
<li>HPA 控制器定期通过指标 API <code>metrics.k8s.io</code> 或 <code>custom.metrics.k8s.io</code> 使用标签选择器查询指标。</li>
<li><code>karmada-apiserver</code> 获取指标 API 查询结果，然后通过 API 服务注册将其路由到 <code>karmada-metrics-adapter</code>。</li>
<li><code>karmada-metrics-adapter</code> 将从目标集群（Pod 所在的集群）查询指标。收集到指标后，它会对这些指标进行聚合并返回结果。</li>
<li>HPA 控制器将根据指标计算所需的副本数，并直接扩展&#x2F;缩小工作负载的规模。然后，<code>karmada-scheduler</code> 将这些副本调度到成员集群中。</li>
</ul>
<blockquote>
<p>注意：要使用此功能，Karmada 版本必须为 v1.6.0 或更高版本。</p>
</blockquote>
<p>下面我们就来演示如何使用 <code>FederatedHPA</code> 控制器来实现跨集群的自动扩缩容。首先至少需要两个成员集群，我们需要在成员集群中安装 <code>ServiceExport</code> 和 <code>ServiceImport</code> 来启用多集群服务。在 Karmada 控制平面上安装 <code>ServiceExport</code> 和 <code>ServiceImport</code> 后（init 安装后会自动安装），我  们可以创建 <code>ClusterPropagationPolicy</code> 将这两个 CRD 传播到成员集群。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># propagate-service-export-import.yaml</span></span><br><span class="line"><span class="comment"># propagate ServiceExport CRD</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterPropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serviceexport-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">serviceexports.multicluster.x-k8s.io</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># propagate ServiceImport CRD</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterPropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serviceimport-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apiextensions.k8s.io/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">CustomResourceDefinition</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">serviceimports.multicluster.x-k8s.io</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br></pre></td></tr></table></figure>

<p>直接应用该 <code>ClusterPropagationPolicy</code> 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f propagate-service-export-import.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>应用后就可以在 member1 和 member2 集群中创建 <code>ServiceExport</code> 和 <code>ServiceImport</code> 对象了。</p>
<p>另外我们还需要为成员集群安装 <code>metrics-server</code> 来提供 metrics API，通过运行以下命令来安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hack/deploy-k8s-metrics-server.sh <span class="variable">$HOME</span>/.kube/member1.config member1</span><br><span class="line">hack/deploy-k8s-metrics-server.sh <span class="variable">$HOME</span>/.kube/member2.config member2</span><br></pre></td></tr></table></figure>

<p>最后我们还需要在 Karmada 控制平面中安装 <code>karmada-metrics-adapter</code> 以提供指标 API，通过运行以下命令来安装它：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> hack/deploy-metrics-adapter.sh ~/.kube/host.config host /etc/karmada/karmada-apiserver.config karmada-apiserver</span><br></pre></td></tr></table></figure>

<blockquote>
<p>需要注意使用 <code>karmada init</code> 安装的 Karmada 控制平面，需要将 <code>karmada-cert</code> 这个 Secret 对象重新拷贝创建一个名为 <code>karmada-cert-secret</code> 的 Secret 对象。</p>
</blockquote>
<p>部署后在 Karmada 控制平面中就会有 <code>karmada-metrics-adapter</code> 这个 Pod 对象。</p>
<p>接下来我们在 member1 和 member2 中部署 Deployment（1 个副本）和 Service 对象，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">64Mi</span></span><br><span class="line">            <span class="attr">limits:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">25m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">64Mi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-propagation</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line">    <span class="attr">replicaScheduling:</span></span><br><span class="line">      <span class="attr">replicaDivisionPreference:</span> <span class="string">Weighted</span></span><br><span class="line">      <span class="attr">replicaSchedulingType:</span> <span class="string">Divided</span></span><br><span class="line">      <span class="attr">weightPreference:</span></span><br><span class="line">        <span class="attr">staticWeightList:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">targetCluster:</span></span><br><span class="line">              <span class="attr">clusterNames:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">            <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">targetCluster:</span></span><br><span class="line">              <span class="attr">clusterNames:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">member2</span></span><br><span class="line">            <span class="attr">weight:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>直接应用上面的资源对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f nginx.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">deployment.apps/nginx configured</span><br><span class="line">service/nginx-service created</span><br><span class="line">propagationpolicy.policy.karmada.io/nginx-propagation configured</span><br><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get pods --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                     CLUSTER   READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-5c54b4855f-ztmnk   member1   1/1     Running   0          43s</span><br><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get svc --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME            CLUSTER   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     ADOPTION</span><br><span class="line">nginx-service   member2   ClusterIP   100.171.35.78    &lt;none&gt;        80/TCP    52s     Y</span><br><span class="line">nginx-service   member1   ClusterIP   100.91.124.245   &lt;none&gt;        80/TCP    52s     Y</span><br></pre></td></tr></table></figure>

<p>然后让我们在 Karmada 控制平面中部署一个 <code>FederatedHPA</code> 对象，用来自动扩缩容，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx-federatedhpa.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">FederatedHPA</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">minReplicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">maxReplicas:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">behavior:</span></span><br><span class="line">    <span class="attr">scaleDown:</span></span><br><span class="line">      <span class="attr">stabilizationWindowSeconds:</span> <span class="number">10</span></span><br><span class="line">    <span class="attr">scaleUp:</span></span><br><span class="line">      <span class="attr">stabilizationWindowSeconds:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">metrics:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Resource</span></span><br><span class="line">      <span class="attr">resource:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">cpu</span></span><br><span class="line">        <span class="attr">target:</span></span><br><span class="line">          <span class="attr">type:</span> <span class="string">Utilization</span></span><br><span class="line">          <span class="attr">averageUtilization:</span> <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>上面的 <code>FederatedHPA</code> 对象中，我们指定了 <code>scaleTargetRef</code> 字段为 <code>Deployment</code> 对象 <code>nginx</code>，<code>minReplicas</code> 和 <code>maxReplicas</code> 分别为 1 和 10，<code>metrics</code> 字段中指定了 CPU 利用率为 10% 时进行扩缩容。同样直接应用该 <code>FederatedHPA</code> 对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f nginx-federatedhpa.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">$ <span class="built_in">sudo</span> kubectl get fhpa --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME    REFERENCE-KIND   REFERENCE-NAME   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx   Deployment       nginx            1         10        1          19s</span><br></pre></td></tr></table></figure>

<p>我们还需要一个多集群服务将请求路由到 member1 和 member2 集群中的 pod。首先在 Karmada 控制平面上创建 <code>ServiceExport</code> 对象，然后创建 <code>PropagationPolicy</code> 以将 <code>ServiceExport</code> 对象传播到 member1 和 member2 集群。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx-serviceexport.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">multicluster.x-k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceExport</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serve-export-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">multicluster.x-k8s.io/v1alpha1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">ServiceExport</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member2</span></span><br></pre></td></tr></table></figure>

<p>然后在 Karmada 控制平面上创建 <code>ServiceImport</code> 对象，然后创建 <code>PropagationPolicy</code> 以将 <code>ServiceImport</code> 对象传播到 member1 集群。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nginx-serviceimport.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">multicluster.x-k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceImport</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterSetIP</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PropagationPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">serve-import-policy</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resourceSelectors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">multicluster.x-k8s.io/v1alpha1</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">ServiceImport</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-service</span></span><br><span class="line">  <span class="attr">placement:</span></span><br><span class="line">    <span class="attr">clusterAffinity:</span></span><br><span class="line">      <span class="attr">clusterNames:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">member1</span></span><br></pre></td></tr></table></figure>

<p>直接应用上面的资源对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f nginx-serviceexport.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">$ <span class="built_in">sudo</span> kubectl apply -f nginx-serviceimport.yaml --kubeconfig /etc/karmada/karmada-apiserver.config</span><br></pre></td></tr></table></figure>

<p>部署完成后，可以查看多集群服务：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get svc --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                    CLUSTER   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     ADOPTION</span><br><span class="line">nginx-service           member2   ClusterIP   100.171.35.78    &lt;none&gt;        80/TCP    6m36s   Y</span><br><span class="line">derived-nginx-service   member1   ClusterIP   100.91.3.68      &lt;none&gt;        80/TCP    17s     Y</span><br><span class="line">nginx-service           member1   ClusterIP   100.91.124.245   &lt;none&gt;        80/TCP    6m36s   Y</span><br></pre></td></tr></table></figure>

<p>接下来我们在 member1 集群使用 <code>hey</code> 工具来进行 http 负载测试，模拟请求增加，从而触发 Pod 的 CPU 使用率增加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://hey-release.s3.us-east-2.amazonaws.com/hey_linux_amd64</span><br><span class="line">$ <span class="built_in">chmod</span> +x hey_linux_amd64</span><br><span class="line">$ docker <span class="built_in">cp</span> hey_linux_amd64 member1-control-plane:/usr/local/bin/hey</span><br></pre></td></tr></table></figure>

<p>然后我们可以使用 <code>hey</code> 请求多集群服务以增加 nginx pod 的 CPU 使用率。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> member1-control-plane hey -c 1000 -z 1m http://100.91.3.68</span><br><span class="line"></span><br><span class="line">Summary:</span><br><span class="line">  Total:        61.4678 secs</span><br><span class="line">  Slowest:      4.7916 secs</span><br><span class="line">  Fastest:      0.0244 secs</span><br><span class="line">  Average:      0.9024 secs</span><br><span class="line">  Requests/sec: 1090.3758</span><br><span class="line"></span><br><span class="line">  Total data:   41219145 bytes</span><br><span class="line">  Size/request: 615 bytes</span><br><span class="line"></span><br><span class="line">Response time histogram:</span><br><span class="line">  0.024 [1]     |</span><br><span class="line">  0.501 [23047] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■</span><br><span class="line">  0.978 [23117] |■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■</span><br><span class="line">  1.455 [8696]  |■■■■■■■■■■■■■■■</span><br><span class="line">  1.931 [5681]  |■■■■■■■■■■</span><br><span class="line">  2.408 [3352]  |■■■■■■</span><br><span class="line">  2.885 [1534]  |■■■</span><br><span class="line">  3.361 [832]   |■</span><br><span class="line">  3.838 [375]   |■</span><br><span class="line">  4.315 [318]   |■</span><br><span class="line">  4.792 [70]    |</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Latency distribution:</span><br><span class="line">  10% <span class="keyword">in</span> 0.2733 secs</span><br><span class="line">  25% <span class="keyword">in</span> 0.4264 secs</span><br><span class="line">  50% <span class="keyword">in</span> 0.6478 secs</span><br><span class="line">  75% <span class="keyword">in</span> 1.1603 secs</span><br><span class="line">  90% <span class="keyword">in</span> 1.9114 secs</span><br><span class="line">  95% <span class="keyword">in</span> 2.3694 secs</span><br><span class="line">  99% <span class="keyword">in</span> 3.4382 secs</span><br><span class="line"></span><br><span class="line">Details (average, fastest, slowest):</span><br><span class="line">  DNS+dialup:   0.0019 secs, 0.0244 secs, 4.7916 secs</span><br><span class="line">  DNS-lookup:   0.0000 secs, 0.0000 secs, 0.0000 secs</span><br><span class="line">  req write:    0.0006 secs, 0.0000 secs, 0.1423 secs</span><br><span class="line">  resp <span class="built_in">wait</span>:    0.7861 secs, 0.0002 secs, 4.6641 secs</span><br><span class="line">  resp <span class="built_in">read</span>:    0.0553 secs, 0.0000 secs, 1.3870 secs</span><br><span class="line"></span><br><span class="line">Status code distribution:</span><br><span class="line">  [200] 67023 responses</span><br></pre></td></tr></table></figure>

<p>等一会儿，副本就会开始扩容了，我们可以查看 <code>FederatedHPA</code> 对象的状态来了解副本的变化：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl describe fhpa nginx --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">Name:         nginx</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">API Version:  autoscaling.karmada.io/v1alpha1</span><br><span class="line">Kind:         FederatedHPA</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">Spec:</span><br><span class="line">  Behavior:</span><br><span class="line">    Scale Down:</span><br><span class="line">      Policies:</span><br><span class="line">        Period Seconds:              15</span><br><span class="line">        Type:                        Percent</span><br><span class="line">        Value:                       100</span><br><span class="line">      Select Policy:                 Max</span><br><span class="line">      Stabilization Window Seconds:  10</span><br><span class="line">    Scale Up:</span><br><span class="line">      Policies:</span><br><span class="line">        Period Seconds:              15</span><br><span class="line">        Type:                        Pods</span><br><span class="line">        Value:                       4</span><br><span class="line">        Period Seconds:              15</span><br><span class="line">        Type:                        Percent</span><br><span class="line">        Value:                       100</span><br><span class="line">      Select Policy:                 Max</span><br><span class="line">      Stabilization Window Seconds:  10</span><br><span class="line">  Max Replicas:                      10</span><br><span class="line">  Metrics:</span><br><span class="line">    Resource:</span><br><span class="line">      Name:  cpu</span><br><span class="line">      Target:</span><br><span class="line">        Average Utilization:  10</span><br><span class="line">        Type:                 Utilization</span><br><span class="line">    Type:                     Resource</span><br><span class="line">  Min Replicas:               1</span><br><span class="line">  Scale Target Ref:</span><br><span class="line">    API Version:  apps/v1</span><br><span class="line">    Kind:         Deployment</span><br><span class="line">    Name:         nginx</span><br><span class="line">Status:</span><br><span class="line">  Conditions:</span><br><span class="line">    Last Transition Time:  2024-05-19T01:43:16Z</span><br><span class="line">    Message:               recommended size matches current size</span><br><span class="line">    Reason:                ReadyForNewScale</span><br><span class="line">    Status:                True</span><br><span class="line">    Type:                  AbleToScale</span><br><span class="line">    Last Transition Time:  2024-05-19T01:43:16Z</span><br><span class="line">    Message:               the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)</span><br><span class="line">    Reason:                ValidMetricFound</span><br><span class="line">    Status:                True</span><br><span class="line">    Type:                  ScalingActive</span><br><span class="line">    Last Transition Time:  2024-05-19T01:45:16Z</span><br><span class="line">    Message:               the desired replica count is less than the minimum replica count</span><br><span class="line">    Reason:                TooFewReplicas</span><br><span class="line">    Status:                True</span><br><span class="line">    Type:                  ScalingLimited</span><br><span class="line">  Current Metrics:</span><br><span class="line">    Resource:</span><br><span class="line">      Current:</span><br><span class="line">        Average Utilization:  0</span><br><span class="line">        Average Value:        0</span><br><span class="line">      Name:                   cpu</span><br><span class="line">    Type:                     Resource</span><br><span class="line">  Current Replicas:           1</span><br><span class="line">  Desired Replicas:           1</span><br><span class="line">  Last Scale Time:            2024-05-19T01:45:16Z</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                     Message</span><br><span class="line">  ----    ------             ----  ----                     -------</span><br><span class="line">  Normal  SuccessfulRescale  2m7s  federatedHPA-controller  New size: 5; reason: cpu resource utilization (percentage of request) above target</span><br><span class="line">  Normal  SuccessfulRescale  112s  federatedHPA-controller  New size: 10; reason: cpu resource utilization (percentage of request) above target</span><br><span class="line">  Normal  SuccessfulRescale  67s   federatedHPA-controller  New size: 3; reason: All metrics below target</span><br><span class="line">  Normal  SuccessfulRescale  52s   federatedHPA-controller  New size: 1; reason: All metrics below target</span><br></pre></td></tr></table></figure>

<p>同时可以查看 member1 和 member2 集群中的 Pod 对象：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get pods --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                     CLUSTER   READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-5c54b4855f-4p6wq   member1   1/1     Running   0          40s</span><br><span class="line">nginx-5c54b4855f-kdwpc   member1   1/1     Running   0          2m6s</span><br><span class="line">nginx-5c54b4855f-l4vm4   member1   1/1     Running   0          40s</span><br><span class="line">nginx-5c54b4855f-t4ghv   member1   1/1     Running   0          25s</span><br><span class="line">nginx-5c54b4855f-vbj9c   member1   1/1     Running   0          25s</span><br><span class="line">nginx-5c54b4855f-hx2xn   member2   1/1     Running   0          25s</span><br><span class="line">nginx-5c54b4855f-kfnbh   member2   1/1     Running   0          40s</span><br><span class="line">nginx-5c54b4855f-rmbv9   member2   1/1     Running   0          40s</span><br><span class="line">nginx-5c54b4855f-wfd92   member2   1/1     Running   0          25s</span><br><span class="line">nginx-5c54b4855f-wwsvq   member2   1/1     Running   0          25s</span><br></pre></td></tr></table></figure>

<p>可以看到 Pod 的副本数已经扩容到 10 个了。同样当负载测试结束后，Pod 的副本数会自动缩小为 1 个副本。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> kubectl karmada get pods --kubeconfig /etc/karmada/karmada-apiserver.config</span><br><span class="line">NAME                     CLUSTER   READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-5c54b4855f-kdwpc   member1   1/1     Running   0          5m25s</span><br></pre></td></tr></table></figure>

<p>到这里我们就完成了使用 <code>FederatedHPA</code> 进行跨集群的自动扩缩容，除此之外我们还可以使用 <code>CronFederatedHPA</code> 用于定期自动缩放操作，它可以缩放具有 scale 子资源的工作负载或 Karmada FederatedHPA。典型的场景是在可预见的流量高峰到来前提前扩容工作负载。例如，如果我知道每天早上 9 点会突发流量洪峰，我们就可以提前半个小时扩容相关服务，以处理高峰负载并确保服务持续可用性。在 Karmada 控制平面内运行的 <code>CronFederatedHPA</code> 控制器根据预定义的 cron 计划来伸缩工作负载的副本或 <code>FederatedHPA</code> 的最小&#x2F;最大副本数。</p>
<p>比如我们有一个如下所示的 <code>CronFederatedHPA</code> 对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling.karmada.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronFederatedHPA</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-cronfhpa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;scale-up&quot;</span></span><br><span class="line">      <span class="attr">schedule:</span> <span class="string">&quot;*/1 * * * *&quot;</span></span><br><span class="line">      <span class="attr">targetReplicas:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">suspend:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>其中表达式 <code>*/1 * * * *</code> 的意思是 nginx deployment 的副本应该每分钟更新为 5 个，确保了处理接下来的流量突发流量洪峰。</p>
<p>除了这些使用场景之外，Karmada 还有很多实践场景，比如跨集群的灾备、多集群网络、多集群服务治理、多集群 CI&#x2F;CD 等等，这些场景都可以通过 Karmada 来实现，更多最佳实践方案可以参考 <a target="_blank" rel="noopener" href="https://karmada.io/zh/docs/">Karmada 官方文档</a>以了解更多。</p>
<blockquote>
<p>参考链接：<a target="_blank" rel="noopener" href="https://karmada.io/zh/docs/">https://karmada.io/zh/docs/</a></p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Kubernetes%20Operator%20%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Kubernetes%20Operator%20%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">Kubernetes Operator 快速入门教程</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:46:26" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/Devops/" itemprop="url" rel="index"><span itemprop="name">Devops</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>18 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kubernetes-Operator-快速入门教程"><a href="#Kubernetes-Operator-快速入门教程" class="headerlink" title="Kubernetes Operator 快速入门教程"></a>Kubernetes Operator 快速入门教程</h1><p>在 Kubernetes 的监控方案中我们经常会使用到一个<a target="_blank" rel="noopener" href="https://www.qikqiak.com/tags/operator/">Promethues Operator</a>的项目，该项目可以让我们更加方便的去使用 Prometheus，而不需要直接去使用最原始的一些资源对象，比如 Pod、Deployment，随着 Prometheus Operator 项目的成功，CoreOS 公司开源了一个比较厉害的工具：<a target="_blank" rel="noopener" href="https://github.com/operator-framework">Operator Framework</a>，该工具可以让开发人员更加容易的开发 Operator 应用。</p>
<p>在本篇文章中我们会为大家介绍一个简单示例来演示如何使用 Operator Framework 框架来开发一个 Operator 应用。</p>
<h2 id="Kubernetes-Operator"><a href="#Kubernetes-Operator" class="headerlink" title="Kubernetes Operator"></a>Kubernetes Operator</h2><p>Operator 是由 CoreOS 开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的领域知识。创建 Operator 的关键是 CRD（自定义资源）的设计。</p>
<p>Kubernetes 1.7 版本以来就引入了<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">自定义控制器</a>的概念，该功能可以让开发人员扩展添加新功能，更新现有的功能，并且可以自动执行一些管理任务，这些自定义的控制器就像 Kubernetes 原生的组件一样，Operator 直接使用 Kubernetes API 进行开发，也就是说他们可以根据这些控制器内部编写的自定义规则来监控集群、更改 Pods&#x2F;Services、对正在运行的应用进行扩缩容。</p>
<h2 id="Operator-Framework"><a href="#Operator-Framework" class="headerlink" title="Operator Framework"></a>Operator Framework</h2><p>Operator Framework 同样也是 CoreOS 开源的一个用于快速开发 Operator 的工具包，该框架包含两个主要的部分：</p>
<ul>
<li>Operator SDK: 无需了解复杂的 Kubernetes API 特性，即可让你根据你自己的专业知识构建一个 Operator 应用。</li>
<li>Operator Lifecycle Manager OLM: 帮助你安装、更新和管理跨集群的运行中的所有 Operator（以及他们的相关服务）</li>
</ul>
<p><img data-src="https://mudutestmenu.mudu.tv/upload/crwp1o.png" alt="operator sdk"></p>
<h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><p>Operator SDK 提供以下工作流来开发一个新的 Operator：</p>
<ul>
<li><ol>
<li>使用 SDK 创建一个新的 Operator 项目</li>
</ol>
</li>
<li><ol>
<li>通过添加自定义资源（CRD）定义新的资源 API</li>
</ol>
</li>
<li><ol>
<li>指定使用 SDK API 来 watch 的资源</li>
</ol>
</li>
<li><ol>
<li>定义 Operator 的协调（reconcile）逻辑</li>
</ol>
</li>
<li><ol>
<li>使用 Operator SDK 构建并生成 Operator 部署清单文件</li>
</ol>
</li>
</ul>
<h2 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h2><p>我们平时在部署一个简单的 Webserver 到 Kubernetes 集群中的时候，都需要先编写一个 Deployment 的控制器，然后创建一个 Service 对象，通过 Pod 的 label 标签进行关联，最后通过 Ingress 或者 type&#x3D;NodePort 类型的 Service 来暴露服务，每次都需要这样操作，是不是略显麻烦，我们就可以创建一个自定义的资源对象，通过我们的 CRD 来描述我们要部署的应用信息，比如镜像、服务端口、环境变量等等，然后创建我们的自定义类型的资源对象的时候，通过控制器去创建对应的 Deployment 和 Service，是不是就方便很多了，相当于我们用一个资源清单去描述了 Deployment 和 Service 要做的两件事情。</p>
<p>这里我们将创建一个名为 AppService 的 CRD 资源对象，然后定义如下的资源清单进行应用部署：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">app.example.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">AppService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">size:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">30002</span></span><br></pre></td></tr></table></figure>

<p>通过这里的自定义的 AppService 资源对象去创建副本数为 2 的 Pod，然后通过 nodePort&#x3D;30002 的端口去暴露服务，接下来我们就来一步一步的实现我们这里的这个简单的 Operator 应用。</p>
<h3 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h3><h4 id="环境需求"><a href="#环境需求" class="headerlink" title="环境需求"></a>环境需求</h4><p>要开发 Operator 自然 Kubernetes 集群是少不了的，还需要 Golang 的环境，这里的安装就不多说了，然后还需要一个 Go 语言的依赖管理工具包：<a target="_blank" rel="noopener" href="https://github.com/golang/dep">dep</a>，由于 Operator SDK 是使用的 dep 该工具包，所以需要我们提前安装好，可以查看资料：<a target="_blank" rel="noopener" href="https://github.com/golang/dep%EF%BC%8C%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E9%9C%80%E8%A6%81%E8%AF%B4%E6%98%8E%E7%9A%84%E6%98%AF%EF%BC%8C%E7%94%B1%E4%BA%8E">https://github.com/golang/dep，另外一个需要说明的是，由于</a> dep 去安装的时候需要去谷歌的网站拉取很多代码，所以正常情况下的话是会失败的，需要做什么工作大家应该清楚吧？要科学。</p>
<h4 id="安装-operator-sdk"><a href="#安装-operator-sdk" class="headerlink" title="安装 operator-sdk"></a>安装 operator-sdk</h4><p>operator sdk 安装方法非常多，我们可以直接在 github 上面下载需要使用的版本，然后放置到 PATH 环境下面即可，当然也可以将源码 clone 到本地手动编译安装即可，如果你是 Mac，当然还可以使用常用的 brew 工具进行安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">brew install operator-sdk</span></span><br><span class="line">......</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk version</span></span><br><span class="line">operator-sdk version: v0.7.0</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">go version</span></span><br><span class="line">go version go1.11.4 darwin/amd64</span><br></pre></td></tr></table></figure>

<p>我们这里使用的 sdk 版本是<code>v0.7.0</code>，其他安装方法可以参考文档：<a target="_blank" rel="noopener" href="https://github.com/operator-framework/operator-sdk/blob/master/doc/user/install-operator-sdk.md">https://github.com/operator-framework/operator-sdk/blob/master/doc/user/install-operator-sdk.md</a></p>
<h3 id="演示"><a href="#演示" class="headerlink" title="演示"></a>演示</h3><h4 id="创建新项目"><a href="#创建新项目" class="headerlink" title="创建新项目"></a>创建新项目</h4><p>环境准备好了，接下来就可以使用 operator-sdk 直接创建一个新的项目了，命令格式为： <strong>operator-sdk new</strong></p>
<p>按照上面我们预先定义的 CRD 资源清单，我们这里可以这样创建：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建项目目录</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p operator-learning</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置项目目录为 GOPATH 路径</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> operator-learning &amp;&amp; <span class="built_in">export</span> GOPATH=<span class="variable">$PWD</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> -p <span class="variable">$GOPATH</span>/src/github.com/cnych</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> <span class="variable">$GOPATH</span>/src/github.com/cnych</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用 sdk 创建一个名为 opdemo 的 operator 项目</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk new opdemo</span></span><br><span class="line">......</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">该过程需要科学上网，需要花费很长时间，请耐心等待</span></span><br><span class="line">......</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> opdemo &amp;&amp; tree -L 2</span></span><br><span class="line">.</span><br><span class="line">├── Gopkg.lock</span><br><span class="line">├── Gopkg.toml</span><br><span class="line">├── build</span><br><span class="line">│   ├── Dockerfile</span><br><span class="line">│   ├── _output</span><br><span class="line">│   └── bin</span><br><span class="line">├── cmd</span><br><span class="line">│   └── manager</span><br><span class="line">├── deploy</span><br><span class="line">│   ├── crds</span><br><span class="line">│   ├── operator.yaml</span><br><span class="line">│   ├── role.yaml</span><br><span class="line">│   ├── role_binding.yaml</span><br><span class="line">│   └── service_account.yaml</span><br><span class="line">├── pkg</span><br><span class="line">│   ├── apis</span><br><span class="line">│   └── controller</span><br><span class="line">├── vendor</span><br><span class="line">│   ├── cloud.google.com</span><br><span class="line">│   ├── contrib.go.opencensus.io</span><br><span class="line">│   ├── github.com</span><br><span class="line">│   ├── go.opencensus.io</span><br><span class="line">│   ├── go.uber.org</span><br><span class="line">│   ├── golang.org</span><br><span class="line">│   ├── google.golang.org</span><br><span class="line">│   ├── gopkg.in</span><br><span class="line">│   ├── k8s.io</span><br><span class="line">│   └── sigs.k8s.io</span><br><span class="line">└── version</span><br><span class="line">    └── version.go</span><br><span class="line"></span><br><span class="line">23 directories, 8 files</span><br></pre></td></tr></table></figure>

<p>到这里一个全新的 Operator 项目就新建完成了。</p>
<h4 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h4><p>使用<code>operator-sdk new</code>命令创建新的 Operator 项目后，项目目录就包含了很多生成的文件夹和文件。</p>
<ul>
<li><strong>Gopkg.toml Gopkg.lock</strong> — Go Dep 清单，用来描述当前 Operator 的依赖包。</li>
<li><strong>cmd</strong> - 包含 main.go 文件，使用 operator-sdk API 初始化和启动当前 Operator 的入口。</li>
<li><strong>deploy</strong> - 包含一组用于在 Kubernetes 集群上进行部署的通用的 Kubernetes 资源清单文件。</li>
<li><strong>pkg&#x2F;apis</strong> - 包含定义的 API 和自定义资源（CRD）的目录树，这些文件允许 sdk 为 CRD 生成代码并注册对应的类型，以便正确解码自定义资源对象。</li>
<li><strong>pkg&#x2F;controller</strong> - 用于编写所有的操作业务逻辑的地方</li>
<li><strong>vendor</strong> - golang vendor 文件夹，其中包含满足当前项目的所有外部依赖包，通过 go dep 管理该目录。</li>
</ul>
<p>我们主要需要编写的是<strong>pkg</strong>目录下面的 api 定义以及对应的 controller 实现。</p>
<h4 id="添加-API"><a href="#添加-API" class="headerlink" title="添加 API"></a>添加 API</h4><p>接下来为我们的自定义资源添加一个新的 API，按照上面我们预定义的资源清单文件，在 Operator 相关根目录下面执行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk add api --api-version=app.example.com/v1 --kind=AppService</span></span><br></pre></td></tr></table></figure>

<p>添加完成后，我们可以看到类似于下面的这样项目结构：<img data-src="https://mudutestmenu.mudu.tv/upload/fiq6j1.png" alt="operator project layout"></p>
<h4 id="添加控制器"><a href="#添加控制器" class="headerlink" title="添加控制器"></a>添加控制器</h4><p>上面我们添加自定义的 API，接下来可以添加对应的自定义 API 的具体实现 Controller，同样在项目根目录下面执行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk add controller --api-version=app.example.com/v1 --kind=AppService</span></span><br></pre></td></tr></table></figure>

<p>这样整个 Operator 项目的脚手架就已经搭建完成了，接下来就是具体的实现了。</p>
<h4 id="自定义-API"><a href="#自定义-API" class="headerlink" title="自定义 API"></a>自定义 API</h4><p>打开源文件<code>pkg/apis/app/v1/appservice_types.go</code>，需要我们根据我们的需求去自定义结构体 AppServiceSpec，我们最上面预定义的资源清单中就有 size、image、ports 这些属性，所有我们需要用到的属性都需要在这个结构体中进行定义：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> AppServiceSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">	<span class="comment">// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster</span></span><br><span class="line">	<span class="comment">// Important: Run &quot;operator-sdk generate k8s&quot; to regenerate code after modifying this file</span></span><br><span class="line">	<span class="comment">// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html</span></span><br><span class="line">	Size  	  *<span class="type">int32</span>                      <span class="string">`json:&quot;size&quot;`</span></span><br><span class="line">	Image     <span class="type">string</span>                      <span class="string">`json:&quot;image&quot;`</span></span><br><span class="line">	Resources corev1.ResourceRequirements <span class="string">`json:&quot;resources,omitempty&quot;`</span></span><br><span class="line">	Envs      []corev1.EnvVar             <span class="string">`json:&quot;envs,omitempty&quot;`</span></span><br><span class="line">	Ports     []corev1.ServicePort        <span class="string">`json:&quot;ports,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码中会涉及到一些包名的导入，由于包名较多，所以我们会使用一些别名进行区分，主要的包含下面几个：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    appsv1 <span class="string">&quot;k8s.io/api/apps/v1&quot;</span></span><br><span class="line">    corev1 <span class="string">&quot;k8s.io/api/core/v1&quot;</span></span><br><span class="line">    appv1 <span class="string">&quot;github.com/cnych/opdemo/pkg/apis/app/v1&quot;</span></span><br><span class="line">    metav1 <span class="string">&quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这里的 resources、envs、ports 的定义都是直接引用的<code>&quot;k8s.io/api/core/v1&quot;</code>中定义的结构体，而且需要注意的是我们这里使用的是<code>ServicePort</code>，而不是像传统的 Pod 中定义的 ContanerPort，这是因为我们的资源清单中不仅要描述容器的 Port，还要描述 Service 的 Port。</p>
<p>然后一个比较重要的结构体<code>AppServiceStatus</code>用来描述资源的状态，当然我们可以根据需要去自定义状态的描述，我这里就偷懒直接使用 Deployment 的状态了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">type AppServiceStatus struct &#123;</span><br><span class="line">	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster</span><br><span class="line">	// Important: Run &quot;operator-sdk generate k8s&quot; to regenerate code after modifying this file</span><br><span class="line">	// Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html</span><br><span class="line">	appsv1.DeploymentStatus `json:&quot;,inline&quot;`</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>定义完成后，在项目根目录下面执行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk generate k8s</span></span><br></pre></td></tr></table></figure>

<p>改命令是用来根据我们自定义的 API 描述来自动生成一些代码，目录<code>pkg/apis/app/v1/</code>下面以<code>zz_generated</code>开头的文件就是自动生成的代码，里面的内容并不需要我们去手动编写。</p>
<p>这样我们就算完成了对自定义资源对象的 API 的声明。</p>
<h4 id="实现业务逻辑"><a href="#实现业务逻辑" class="headerlink" title="实现业务逻辑"></a>实现业务逻辑</h4><p>上面 API 描述声明完成了，接下来就需要我们来进行具体的业务逻辑实现了，编写具体的 controller 实现，打开源文件<code>pkg/controller/appservice/appservice_controller.go</code>，需要我们去更改的地方也不是很多，核心的就是<code>Reconcile</code>方法，该方法就是去不断的 watch 资源的状态，然后根据状态的不同去实现各种操作逻辑，核心代码如下：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *ReconcileAppService)</span></span> Reconcile(request reconcile.Request) (reconcile.Result, <span class="type">error</span>) &#123;</span><br><span class="line">	reqLogger := log.WithValues(<span class="string">&quot;Request.Namespace&quot;</span>, request.Namespace, <span class="string">&quot;Request.Name&quot;</span>, request.Name)</span><br><span class="line">	reqLogger.Info(<span class="string">&quot;Reconciling AppService&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Fetch the AppService instance</span></span><br><span class="line">	instance := &amp;appv1.AppService&#123;&#125;</span><br><span class="line">	err := r.client.Get(context.TODO(), request.NamespacedName, instance)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">if</span> errors.IsNotFound(err) &#123;</span><br><span class="line">			<span class="comment">// Request object not found, could have been deleted after reconcile request.</span></span><br><span class="line">			<span class="comment">// Owned objects are automatically garbage collected. For additional cleanup logic use finalizers.</span></span><br><span class="line">			<span class="comment">// Return and don&#x27;t requeue</span></span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// Error reading the object - requeue the request.</span></span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> instance.DeletionTimestamp != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 如果不存在，则创建关联资源</span></span><br><span class="line">	<span class="comment">// 如果存在，判断是否需要更新</span></span><br><span class="line">	<span class="comment">//   如果需要更新，则直接更新</span></span><br><span class="line">	<span class="comment">//   如果不需要更新，则正常返回</span></span><br><span class="line"></span><br><span class="line">	deploy := &amp;appsv1.Deployment&#123;&#125;</span><br><span class="line">	<span class="keyword">if</span> err := r.client.Get(context.TODO(), request.NamespacedName, deploy); err != <span class="literal">nil</span> &amp;&amp; errors.IsNotFound(err) &#123;</span><br><span class="line">		<span class="comment">// 创建关联资源</span></span><br><span class="line">		<span class="comment">// 1. 创建 Deploy</span></span><br><span class="line">		deploy := resources.NewDeploy(instance)</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Create(context.TODO(), deploy); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 2. 创建 Service</span></span><br><span class="line">		service := resources.NewService(instance)</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Create(context.TODO(), service); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 3. 关联 Annotations</span></span><br><span class="line">		data, _ := json.Marshal(instance.Spec)</span><br><span class="line">		<span class="keyword">if</span> instance.Annotations != <span class="literal">nil</span> &#123;</span><br><span class="line">			instance.Annotations[<span class="string">&quot;spec&quot;</span>] = <span class="type">string</span>(data)</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			instance.Annotations = <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;<span class="string">&quot;spec&quot;</span>: <span class="type">string</span>(data)&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> err := r.client.Update(context.TODO(), instance); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	oldspec := appv1.AppServiceSpec&#123;&#125;</span><br><span class="line">	<span class="keyword">if</span> err := json.Unmarshal([]<span class="type">byte</span>(instance.Annotations[<span class="string">&quot;spec&quot;</span>]), oldspec); err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> !reflect.DeepEqual(instance.Spec, oldspec) &#123;</span><br><span class="line">		<span class="comment">// 更新关联资源</span></span><br><span class="line">		newDeploy := resources.NewDeploy(instance)</span><br><span class="line">		oldDeploy := &amp;appsv1.Deployment&#123;&#125;</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Get(context.TODO(), request.NamespacedName, oldDeploy); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line">		oldDeploy.Spec = newDeploy.Spec</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Update(context.TODO(), oldDeploy); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		newService := resources.NewService(instance)</span><br><span class="line">		oldService := &amp;corev1.Service&#123;&#125;</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Get(context.TODO(), request.NamespacedName, oldService); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line">		oldService.Spec = newService.Spec</span><br><span class="line">		<span class="keyword">if</span> err := r.client.Update(context.TODO(), oldService); err != <span class="literal">nil</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> reconcile.Result&#123;&#125;, err</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> reconcile.Result&#123;&#125;, <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面就是业务逻辑实现的核心代码，逻辑很简单，就是去判断资源是否存在，不存在，则直接创建新的资源，创建新的资源除了需要创建 Deployment 资源外，还需要创建 Service 资源对象，因为这就是我们的需求，当然你还可以自己去扩展，比如在创建一个 Ingress 对象。更新也是一样的，去对比新旧对象的声明是否一致，不一致则需要更新，同样的，两种资源都需要更新的。</p>
<p>另外两个核心的方法就是上面的<code>resources.NewDeploy(instance)</code>和<code>resources.NewService(instance)</code>方法，这两个方法实现逻辑也很简单，就是根据 CRD 中的声明去填充 Deployment 和 Service 资源对象的 Spec 对象即可。</p>
<p>NewDeploy 方法实现如下：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewDeploy</span><span class="params">(app *appv1.AppService)</span></span> *appsv1.Deployment &#123;</span><br><span class="line">	labels := <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;<span class="string">&quot;app&quot;</span>: app.Name&#125;</span><br><span class="line">	selector := &amp;metav1.LabelSelector&#123;MatchLabels: labels&#125;</span><br><span class="line">	<span class="keyword">return</span> &amp;appsv1.Deployment&#123;</span><br><span class="line">		TypeMeta: metav1.TypeMeta&#123;</span><br><span class="line">			APIVersion: <span class="string">&quot;apps/v1&quot;</span>,</span><br><span class="line">			Kind:       <span class="string">&quot;Deployment&quot;</span>,</span><br><span class="line">		&#125;,</span><br><span class="line">		ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">			Name:      app.Name,</span><br><span class="line">			Namespace: app.Namespace,</span><br><span class="line"></span><br><span class="line">			OwnerReferences: []metav1.OwnerReference&#123;</span><br><span class="line">				*metav1.NewControllerRef(app, schema.GroupVersionKind&#123;</span><br><span class="line">					Group: v1.SchemeGroupVersion.Group,</span><br><span class="line">					Version: v1.SchemeGroupVersion.Version,</span><br><span class="line">					Kind: <span class="string">&quot;AppService&quot;</span>,</span><br><span class="line">				&#125;),</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">		Spec: appsv1.DeploymentSpec&#123;</span><br><span class="line">			Replicas: app.Spec.Size,</span><br><span class="line">			Template: corev1.PodTemplateSpec&#123;</span><br><span class="line">				ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">					Labels: labels,</span><br><span class="line">				&#125;,</span><br><span class="line">				Spec: corev1.PodSpec&#123;</span><br><span class="line">					Containers: newContainers(app),</span><br><span class="line">				&#125;,</span><br><span class="line">			&#125;,</span><br><span class="line">			Selector: selector,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newContainers</span><span class="params">(app *v1.AppService)</span></span> []corev1.Container &#123;</span><br><span class="line">	containerPorts := []corev1.ContainerPort&#123;&#125;</span><br><span class="line">	<span class="keyword">for</span> _, svcPort := <span class="keyword">range</span> app.Spec.Ports &#123;</span><br><span class="line">		cport := corev1.ContainerPort&#123;&#125;</span><br><span class="line">		cport.ContainerPort = svcPort.TargetPort.IntVal</span><br><span class="line">		containerPorts = <span class="built_in">append</span>(containerPorts, cport)</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> []corev1.Container&#123;</span><br><span class="line">		&#123;</span><br><span class="line">			Name: app.Name,</span><br><span class="line">			Image: app.Spec.Image,</span><br><span class="line">			Resources: app.Spec.Resources,</span><br><span class="line">			Ports: containerPorts,</span><br><span class="line">			ImagePullPolicy: corev1.PullIfNotPresent,</span><br><span class="line">			Env: app.Spec.Envs,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>newService 对应的方法实现如下：</p>
<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewService</span><span class="params">(app *v1.AppService)</span></span> *corev1.Service &#123;</span><br><span class="line">	<span class="keyword">return</span> &amp;corev1.Service &#123;</span><br><span class="line">		TypeMeta: metav1.TypeMeta &#123;</span><br><span class="line">			Kind: <span class="string">&quot;Service&quot;</span>,</span><br><span class="line">			APIVersion: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line">		&#125;,</span><br><span class="line">		ObjectMeta: metav1.ObjectMeta&#123;</span><br><span class="line">			Name: app.Name,</span><br><span class="line">			Namespace: app.Namespace,</span><br><span class="line">			OwnerReferences: []metav1.OwnerReference&#123;</span><br><span class="line">				*metav1.NewControllerRef(app, schema.GroupVersionKind&#123;</span><br><span class="line">					Group: v1.SchemeGroupVersion.Group,</span><br><span class="line">					Version: v1.SchemeGroupVersion.Version,</span><br><span class="line">					Kind: <span class="string">&quot;AppService&quot;</span>,</span><br><span class="line">				&#125;),</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">		Spec: corev1.ServiceSpec&#123;</span><br><span class="line">			Type: corev1.ServiceTypeNodePort,</span><br><span class="line">			Ports: app.Spec.Ports,</span><br><span class="line">			Selector: <span class="keyword">map</span>[<span class="type">string</span>]<span class="type">string</span>&#123;</span><br><span class="line">				<span class="string">&quot;app&quot;</span>: app.Name,</span><br><span class="line">			&#125;,</span><br><span class="line">		&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样我们就实现了 AppService 这种资源对象的业务逻辑。</p>
<h4 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h4><p>如果我们本地有一个可以访问的 Kubernetes 集群，我们也可以直接进行调试，在本地用户<code>~/.kube/config</code>文件中配置集群访问信息，下面的信息表明可以访问 Kubernetes 集群：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://ydzs-master:6443</span><br><span class="line">KubeDNS is running at https://ydzs-master:6443/api/v1/namespaces/kube-system/services/kube-dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.</span><br></pre></td></tr></table></figure>

<p>首先，在集群中安装 CRD 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/crds/app_v1_appservice_crd.yaml</span></span><br><span class="line">customresourcedefinition &quot;appservices.app.example.com&quot; created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get crd</span></span><br><span class="line">NAME                                   AGE</span><br><span class="line">appservices.app.example.com            &lt;invalid&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>当我们通过<code>kubectl get crd</code>命令获取到我们定义的 CRD 资源对象，就证明我们定义的 CRD 安装成功了。其实现在只是 CRD 的这个声明安装成功了，但是我们这个 CRD 的具体业务逻辑实现方式还在我们本地，并没有部署到集群之中，我们可以通过下面的命令来在本地项目中启动 Operator 的调试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk up <span class="built_in">local</span></span></span><br><span class="line">INFO[0000] Running the operator locally.</span><br><span class="line">INFO[0000] Using namespace default.</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.964137,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Go Version: go1.11.4&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.964192,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Go OS/Arch: darwin/amd64&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.9641972,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Version of operator-sdk: v0.7.0&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.965905,&quot;logger&quot;:&quot;leader&quot;,&quot;msg&quot;:&quot;Trying to become the leader.&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207203.965945,&quot;logger&quot;:&quot;leader&quot;,&quot;msg&quot;:&quot;Skipping leader election; not running in a cluster.&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.928867,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Registering Components.&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.929077,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting EventSource&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;source&quot;:&quot;kind source: /, Kind=&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207206.9292521,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting EventSource&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;source&quot;:&quot;kind source: /, Kind=&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.622659,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;failed to initialize service object for metrics: OPERATOR_NAME must be set&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.622693,&quot;logger&quot;:&quot;cmd&quot;,&quot;msg&quot;:&quot;Starting the Cmd.&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.7236018,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting Controller&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207209.8284118,&quot;logger&quot;:&quot;kubebuilder.controller&quot;,&quot;msg&quot;:&quot;Starting workers&quot;,&quot;controller&quot;:&quot;appservice-controller&quot;,&quot;worker count&quot;:1&#125;</span><br></pre></td></tr></table></figure>

<p>上面的命令会在本地运行 Operator 应用，通过<code>~/.kube/config</code>去关联集群信息，现在我们去添加一个 AppService 类型的资源然后观察本地 Operator 的变化情况，资源清单文件就是我们上面预定义的（deploy&#x2F;crds&#x2F;app_v1_appservice_cr.yaml）</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">app.example.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">AppService</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-app</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">size:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">30002</span></span><br></pre></td></tr></table></figure>

<p>直接创建这个资源对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/crds/app_v1_appservice_cr.yaml</span></span><br><span class="line">appservice &quot;nginx-app&quot; created</span><br></pre></td></tr></table></figure>

<p>我们可以看到我们的应用创建成功了，这个时候查看 Operator 的调试窗口会有如下的信息出现：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207416.670523,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207417.004226,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207417.004331,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207418.33779,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1559207418.951193,&quot;logger&quot;:&quot;controller_appservice&quot;,&quot;msg&quot;:&quot;Reconciling AppService&quot;,&quot;Request.Namespace&quot;:&quot;default&quot;,&quot;Request.Name&quot;:&quot;nginx-app&quot;&#125;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>然后我们可以去查看集群中是否有符合我们预期的资源出现：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get AppService</span></span><br><span class="line">NAME        AGE</span><br><span class="line">nginx-app   &lt;invalid&gt;</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get deploy</span></span><br><span class="line">NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-app                2         2         2            2           &lt;invalid&gt;</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        76d</span><br><span class="line">nginx-app    NodePort    10.108.227.5   &lt;none&gt;        80:30002/TCP   &lt;invalid&gt;</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                                      READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-app-76b6449498-2j82j                1/1       Running   0          &lt;invalid&gt;</span><br><span class="line">nginx-app-76b6449498-m4h58                1/1       Running   0          &lt;invalid&gt;</span><br></pre></td></tr></table></figure>

<p>看到了吧，我们定义了两个副本（size&#x3D;2），这里就出现了两个 Pod，还有一个 NodePort&#x3D;30002 的 Service 对象，我们可以通过该端口去访问下应用：<img data-src="https://mudutestmenu.mudu.tv/upload/5f90yb.png" alt="crd nginx app demo"></p>
<p>如果应用在安装过程中出现了任何问题，我们都可以通过本地的 Operator 调试窗口找到有用的信息，然后调试修改即可。</p>
<p>清理：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/crds/app_v1_appservice_crd.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/crds/app_v1_appservice_cr.yaml</span></span><br></pre></td></tr></table></figure>

<h4 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h4><p>自定义的资源对象现在测试通过了，但是如果我们将本地的<code>operator-sdk up local</code>命令终止掉，我们可以猜想到就没办法处理 AppService 资源对象的一些操作了，所以我们需要将我们的业务逻辑实现部署到集群中去。</p>
<p>执行下面的命令构建 Operator 应用打包成 Docker 镜像：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">operator-sdk build cnych/opdemo</span></span><br><span class="line">INFO[0002] Building Docker image cnych/opdemo</span><br><span class="line">Sending build context to Docker daemon  400.7MB</span><br><span class="line">Step 1/7 : FROM registry.access.redhat.com/ubi7-dev-preview/ubi-minimal:7.6</span><br><span class="line">......</span><br><span class="line">Successfully built a8cde91be6ab</span><br><span class="line">Successfully tagged cnych/opdemo:latest</span><br><span class="line">INFO[0053] Operator build complete.</span><br></pre></td></tr></table></figure>

<p>镜像构建成功后，推送到 docker hub：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker push cnych/opdemo</span></span><br></pre></td></tr></table></figure>

<p>镜像推送成功后，使用上面的镜像地址更新 Operator 的资源清单：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sed -i <span class="string">&#x27;s|REPLACE_IMAGE|cnych/opdemo|g&#x27;</span> deploy/operator.yaml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果你使用的是 Mac 系统，使用下面的命令</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sed -i <span class="string">&quot;&quot;</span> <span class="string">&#x27;s|REPLACE_IMAGE|cnych/opdemo|g&#x27;</span> deploy/operator.yaml</span></span><br></pre></td></tr></table></figure>

<p>现在 Operator 的资源清单文件准备好了，然后创建对应的 RBAC 的对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Setup Service Account</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/service_account.yaml</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Setup RBAC</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/role.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/role_binding.yaml</span></span><br></pre></td></tr></table></figure>

<p>权限相关声明已经完成，接下来安装 CRD 和 Operator：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Setup the CRD</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f deploy/crds/app_v1_appservice_crd.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get crd</span></span><br><span class="line">NAME                                   CREATED AT</span><br><span class="line">appservices.app.example.com            2019-05-30T17:03:32Z</span><br><span class="line">......</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Deploy the Operator</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/operator.yaml</span></span><br><span class="line">deployment.apps/opdemo created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pods</span></span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">opdemo-64db96d575-9vtq6                   1/1     Running   0          2m2s</span><br></pre></td></tr></table></figure>

<p>到这里我们的 CRD 和 Operator 实现都已经安装成功了。</p>
<p>现在我们再来部署我们的 AppService 资源清单文件，现在的业务逻辑就会在上面的<code>opdemo-64db96d575-9vtq6</code>的 Pod 中去处理了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl create -f deploy/crds/app_v1_appservice_cr.yaml</span></span><br><span class="line">appservice.app.example.com/nginx-app created</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get appservice</span></span><br><span class="line">NAME        AGE</span><br><span class="line">nginx-app   18s</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> kubectl get deploy</span></span><br><span class="line">NAME                     READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-app                2/2     2            2           24s</span><br><span class="line">opdemo                   1/1     1            1           5m51s</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        76d</span><br><span class="line">nginx-app    NodePort    10.106.129.82   &lt;none&gt;        80:30002/TCP   29s</span><br><span class="line">opdemo       ClusterIP   10.100.233.51   &lt;none&gt;        8383/TCP       4m25s</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> kubectl get pods</span></span><br><span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-app-76b6449498-ffhgx                1/1     Running   0          32s</span><br><span class="line">nginx-app-76b6449498-wzjq2                1/1     Running   0          32s</span><br><span class="line">opdemo-64db96d575-9vtq6                   1/1     Running   0          5m59s</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl describe appservice nginx-app</span></span><br><span class="line">Name:         nginx-app</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  spec: &#123;&quot;size&quot;:2,&quot;image&quot;:&quot;nginx:1.7.9&quot;,&quot;resources&quot;:&#123;&#125;,&quot;ports&quot;:[&#123;&quot;protocol&quot;:&quot;TCP&quot;,&quot;port&quot;:80,&quot;targetPort&quot;:80,&quot;nodePort&quot;:30002&#125;]&#125;</span><br><span class="line">API Version:  app.example.com/v1</span><br><span class="line">Kind:         AppService</span><br><span class="line">Metadata:</span><br><span class="line">  Creation Timestamp:  2019-05-30T17:41:28Z</span><br><span class="line">  Generation:          2</span><br><span class="line">  Resource Version:    19666617</span><br><span class="line">  Self Link:           /apis/app.example.com/v1/namespaces/default/appservices/nginx-app</span><br><span class="line">  UID:                 2756f232-8302-11e9-80ca-525400cc3c00</span><br><span class="line">Spec:</span><br><span class="line">  Image:  nginx:1.7.9</span><br><span class="line">  Ports:</span><br><span class="line">    Node Port:    30002</span><br><span class="line">    Port:         80</span><br><span class="line">    Protocol:     TCP</span><br><span class="line">    Target Port:  80</span><br><span class="line">  Resources:</span><br><span class="line">  Size:  2</span><br><span class="line">Events:  &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>然后同样的可以通过 30002 这个 NodePort 端口去访问应用，到这里应用就部署成功了。</p>
<h4 id="清理"><a href="#清理" class="headerlink" title="清理"></a>清理</h4><p>有资源清单文件，直接删除即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/crds/app_v1_appservice_cr.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/operator.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/role.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/role_binding.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/service_account.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl delete -f deploy/crds/app_v1_appservice_crd.yaml</span></span><br></pre></td></tr></table></figure>

<h4 id="开发"><a href="#开发" class="headerlink" title="开发"></a>开发</h4><p>Operator SDK 为我们创建了一个快速启动的代码和相关配置，如果我们要开始处理相关的逻辑，我们可以在项目中搜索<code>TODO(user)</code>这个注释来实现我们自己的逻辑，比如在我的 VSCode 环境中，看上去是这样的：<img data-src="https://mudutestmenu.mudu.tv/upload/7l7d5q.png" alt="operator code todo demo"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/11/Kubernetes%20%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AE%BE%E7%BD%AE%E3%80%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="六一">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hui's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/09/11/Kubernetes%20%E5%AE%89%E5%85%A8%E4%B8%8A%E4%B8%8B%E6%96%87%E8%AE%BE%E7%BD%AE%E3%80%81/" class="post-title-link" itemprop="url">Kubernetes 安全上下文设置、</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-09-11 20:32:34 / 修改时间：21:46:00" itemprop="dateCreated datePublished" datetime="2025-09-11T20:32:34+08:00">2025-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/" itemprop="url" rel="index"><span itemprop="name">k8s</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/k8s/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">基础知识</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kubernetes-安全上下文设置"><a href="#Kubernetes-安全上下文设置" class="headerlink" title="Kubernetes 安全上下文设置"></a>Kubernetes 安全上下文设置</h1><p>在 Kubernetes 中安全地运行工作负载是很困的，有很多配置都可能会影响到整个 Kubernetes API 的安全性，这需要我们有大量的知识积累来正确的实施。Kubernetes 在安全方面提供了一个强大的工具 <code>securityContext</code>，每个 Pod 和容器清单都可以使用这个属性。在本文中我们将了解各种 <code>securityContext</code> 的配置，探讨它们的含义，以及我们应该如何使用它们。</p>
<blockquote>
<p><code>securityContext</code> 设置在 <code>PodSpec</code> 和<code>ContainerSpec</code> 规范中都有定义，这里我们分别用<code>[P]</code>和<code>[C]</code>来表示。需要注意的是，如果一个设置在两个作用域中都可以使用和配置，那么我们应该优先考虑设置容器级别的。</p>
</blockquote>
<h2 id="1-runAsNonRoot-P-C"><a href="#1-runAsNonRoot-P-C" class="headerlink" title="1. runAsNonRoot [P&#x2F;C]"></a>1. runAsNonRoot [P&#x2F;C]</h2><p>我们知道容器是使用 namespaces 和 cgroups 来限制其进程，但只要在部署的时候做了一次错误的配置，就可以让这些进程访问主机上的资源。如果该进程以 root 身份运行，它对这些资源的访问权限与主机 root 账户是相同的。此外，如果其他 pod 或容器设置被用来减少约束（比如 <code>procMount</code> 或 <code>capabilities</code>），拥有一个 <code>root UID</code> 就会提高风险，除非你有一个非常好的原因，否则你不应该以 root 身份运行一个容器。</p>
<p>那么，如果你有一个使用 root 的镜像需要部署，那应该怎么办呢？</p>
<h3 id="1-1-使用基础镜像中提供的用户"><a href="#1-1-使用基础镜像中提供的用户" class="headerlink" title="1.1 使用基础镜像中提供的用户"></a>1.1 使用基础镜像中提供的用户</h3><p>通常情况下，基础镜像已经创建并提供了一个用户，例如，官方的 Node.js 镜像带有一个 UID 为 1000 的名为 node 的用户，我们就可以使用该身份来运行容器，但他们并没有在 <code>Dockerfile</code> 中明确地设置当前用户。我们可以在运行时用 <code>runAsUser</code> 设置来配置它，或者用自定义的 <code>Dockerfile</code> 来更改镜像中的当前用户。这里我们来看看使用自定义的 <code>Dockerfile</code> 来构建我们自己的镜像的例子。</p>
<p>在不深入了解镜像构建的情况下，让我们假设我们有一个预先构建好的 npm 应用程序。这里是一个最小的 Dockerfile 文件，用来构建一个基于 <code>node:slim</code> 的镜像，并以提供的 node 用户身份运行。</p>
<figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:slim</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --<span class="built_in">chown</span>=node . /home/node/app/   <span class="comment"># &lt;--- Copy app into the home directory with right ownership</span></span></span><br><span class="line"><span class="keyword">USER</span> <span class="number">1000</span>                             <span class="comment"># &lt;--- Switch active user to “node” (by UID)</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /home/node/app                <span class="comment"># &lt;--- Switch current directory to app</span></span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;npm&quot;</span>, <span class="string">&quot;start&quot;</span>]           <span class="comment"># &lt;--- This will now exec as the “node” user instead of root</span></span></span><br></pre></td></tr></table></figure>

<p>其中以 USER 开头的一行就是关键设置，这使得 node 成为从这个镜像启动的任何容器里面的默认用户。我们使用 UID 而不是用户的名字，因为 Kubernetes 无法在启动容器前将镜像的默认用户名映射到 UID 上，并且在部署时指定 <code>runAsNotRoot: true</code>，会返回有关错误。</p>
<h3 id="1-2-基础镜像没有提供用户"><a href="#1-2-基础镜像没有提供用户" class="headerlink" title="1.2 基础镜像没有提供用户"></a>1.2 基础镜像没有提供用户</h3><p>如果我们使用的基础镜像没有提供一个可以使用的用户，那么我们又应该怎么做呢？对于大部分进程来说，我们只需在自定义的 Dockerfile 中创建一个用户并使用它即可。如下所示：</p>
<figure class="highlight docker"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> node:slim</span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> useradd somebody -u 10001 --create-home --user-group  <span class="comment"># &lt;--- Create a user</span></span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> --<span class="built_in">chown</span>=somebody . /home/somebody/app/</span></span><br><span class="line"><span class="keyword">USER</span> <span class="number">10001</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /home/somebody/app</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;npm&quot;</span>, <span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure>

<p>这里我们增加了一行创建用户的 <code>RUN</code> 命令即可。不过需要注意的是这对于 <code>node.js</code> 和 <code>npm</code> 来说，这很好用，但是其他工具可能需要文件系统的不同元素进行所有权变更。如果遇到任何问题，需要查阅对应工具的文档。</p>
<h2 id="2-runAsUser-runAsGroup-P-C"><a href="#2-runAsUser-runAsGroup-P-C" class="headerlink" title="2. runAsUser&#x2F;runAsGroup [P&#x2F;C]"></a>2. runAsUser&#x2F;runAsGroup [P&#x2F;C]</h2><p>容器镜像可能有一个特定的用户或组，我们可以用 <code>runAsUser</code> 和 <code>runAsGroup</code> 来进行覆盖。通常，这些设置与包含具有相同所有权 ID 的文件的卷挂载结合在一起。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">....</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">mycorp/webapp:1.2.3</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">runAsNonRoot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">runAsUser:</span> <span class="number">10001</span></span><br><span class="line"><span class="string">....</span></span><br></pre></td></tr></table></figure>

<p>不过使用这些配置也是有风险的，因为你为容器做出的运行时决定可能与原始镜像不兼容。例如，<code>jenkins/jenkins</code> 镜像以名为 <code>jenkins:jenkins</code> 的<strong>组:用户</strong>身份运行，其应用文件全部由该用户拥有。如果我们配置一个不同的用户，它将无法启动，因为该用户不存在于镜像的 <code>/etc/passwd</code> 文件中。即使它以某种方式存在，它也很可能在读写 <code>jenkins:jenkins</code> 拥有的文件时出现问题。我们可以用一个简单的 docker 运行命令来验证这个问题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">docker run --<span class="built_in">rm</span> -it -u eric:eric jenkins/jenkins</span></span><br><span class="line">docker: Error response from daemon: unable to find user eric: no matching entries in passwd file.</span><br></pre></td></tr></table></figure>

<p>上面我们提到确保容器进程不以 root 用户身份运行是一个非常好的主意，但不要依赖 <code>runAsUser</code> 或 <code>runAsGroup</code> 设置来保证这一点，未来有人可能会删除这些配置，请确保同时将 <code>runAsNonRoot</code> 设置为 true。</p>
<h2 id="3-seLinuxOptions-P-C"><a href="#3-seLinuxOptions-P-C" class="headerlink" title="3. seLinuxOptions [P&#x2F;C]"></a>3. seLinuxOptions [P&#x2F;C]</h2><p><code>SELinux</code> 是一个用于控制对 Linux 系统上的应用、进程和文件进行访问的策略驱动系统，它在 Linux 内核中实现了 Linux 安全模块框架。SELinux 是基于标签的策略，它将一些标签应用于系统中的所有元素，然后将元素进行分组。这些标签被称为<strong>安全上下文</strong>（不要和 Kubernetes 中的 <code>securityContext</code> 混淆了）- 由用户、角色、类型和可选的一些其他属性组成，格式为：<code>user:role:type:level</code>。</p>
<p>然后，SELinux 使用策略来定义特定上下文中的哪些进程可以访问系统中其他被标记的对象。SELinux 可以是严格执行 <code>enforced</code> 模式，在这种情况下，访问将被拒绝，如果被配置为允许的 <code>permissive</code> 模式，那么安全策略没有被强制执行，当安全策略规则应该拒绝访问时，访问仍然被允许，然而，此时会向日志文件发送一条消息，表示该访问应该被拒绝。在容器中，SELinux 通常给容器进程和容器镜像打上标签，以限制该进程只能访问镜像中的文件。</p>
<p>默认的 SELinux 策略将在实例化容器时由容器运行时应用，<code>securityContext</code> 中的 <code>seLinuxOptions</code> 允许配置自定义的 SELinux 策略标签，请注意，改变容器的 SELinux 策略标签有可能允许容器进程摆脱容器镜像并访问主机文件系统。</p>
<p>当然只有当宿主机操作系统支持 SELinux 时，这个功能才会起作用。</p>
<h2 id="4-seccompProfile-P-C"><a href="#4-seccompProfile-P-C" class="headerlink" title="4. seccompProfile [P&#x2F;C]"></a>4. seccompProfile [P&#x2F;C]</h2><p><code>Seccomp</code> 表示一种安全计算模式，是 Linux 内核的一项功能，它可以限制一个特定进程从用户空间到内核的调用。seccomp 配置文件是使用一个 JSON 文件进行定义的，通常由一组系统调用和发生这些系统调用时的默认动作组成。如下配置所示：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;defaultAction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;SCMP_ACT_ERRNO&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;architectures&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;SCMP_ARCH_X86_64&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;SCMP_ARCH_X86&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;SCMP_ARCH_X32&quot;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;syscalls&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;accept&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;SCMP_ACT_ALLOW&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;accept4&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;action&quot;</span><span class="punctuation">:</span> <span class="string">&quot;SCMP_ACT_ALLOW&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>Kubernetes 通过在 <code>securityContext</code> 中的 <code>seccompProfile</code> 属性来提供一个使用自定义配置文件的机制。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">seccompProfile:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">Localhost</span></span><br><span class="line">  <span class="attr">localhostProfile:</span> <span class="string">profiles/myprofile.json</span></span><br></pre></td></tr></table></figure>

<p>这里配置的 <code>type</code> 字段有三个可选的值：</p>
<ul>
<li><code>Localhost</code>：其中 <code>localhostProfile</code> 配置为容器内的 <code>seccomp</code> 配置文件路径。</li>
<li><code>Unconfined</code>：其中没有配置文件。</li>
<li><code>RuntimeDefault</code>：其中使用容器运行时的默认值–如果没有指定类型，就是默认值。</li>
</ul>
<p>我们可以在 <code>PodSecurityContext</code> 或 <code>securityContext</code> 中使用这些配置，如果两者都配置了，就会使用容器级别中的配置。</p>
<p>此外与大多数安全相关的设置一样，<strong>最小权限原则</strong>在此同样适用。只给你的容器访问它所需要的权限即可。首先创建一个配置文件，简单地记录哪些系统调用正在发生，然后测试你的应用程序，建立一套允许的系统调用规则。我们可以在 <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tutorials/clusters/seccomp">Kubernetes 教程</a>中找到关于 <code>Seccomp</code> 的更多信息。</p>
<h2 id="5-避免使用特权容器-C"><a href="#5-避免使用特权容器-C" class="headerlink" title="5. 避免使用特权容器 [C]"></a>5. 避免使用特权容器 [C]</h2><p>给容器授予特权模式是非常危险的，一般会有一种更简单的方式来实现特定的权限，或者可以通过授予 <code>Linux Capabilities</code> 权限来控制。容器运行时控制器着特权模式的具体实现，但是它会授予容器所有的特权，并解除由 cgroup 控制器执行的限制，它还可以修改 Linux 安全模块的配置，并允许容器内的进程逃离容器。</p>
<p>容器在宿主机中提供了进程隔离，所以即使容器是使用 root 身份运行的，也有容器运行时不授予容器的 <code>Capabilities</code>。如果配置了特权模式，容器运行时就会授予系统 root 的所有能力，从安全角度来看，这是很危险的，因为它允许对底层宿主机系统的所有操作访问。</p>
<p>避免使用特权模式，如果你的容器确实需要额外的能力，只需通过添加 <code>capabilities</code> 来满足你的需求。除非你的容器需要控制主机内核中的系统级设置，如访问特定的硬件或重新配置网络，并且需要访问主机文件系统，那么它就不需要特权模式。</p>
<h2 id="6-Linux-Capabilities-C"><a href="#6-Linux-Capabilities-C" class="headerlink" title="6. Linux Capabilities [C]"></a>6. Linux Capabilities [C]</h2><p><code>Capabilities</code> 是一个内核级别的权限，它允许对内核调用权限进行更细粒度的控制，而不是简单地以 root 身份运行。<code>Capabilities</code> 包括更改文件权限、控制网络子系统和执行系统管理等功能。在 <code>securityContext</code> 中，Kubernetes 可以添加或删除 <code>Capabilities</code>，单个 <code>Capabilities</code> 或逗号分隔的列表可以作为一个字符串数组进行配置。另外，我们也可以使用 <code>all</code> 来添加或删除所有的配置。这种配置会被传递给容器运行时，在它创建容器的时候会配置上 <code>Capabilities</code> 集合，如果 <code>securityContext</code> 中没有配置，那么容器将会直接容器运行时提供的所有默认配置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">securityContext:</span></span><br><span class="line">  <span class="attr">capabilities:</span></span><br><span class="line">    <span class="attr">drop:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">all</span></span><br><span class="line">    <span class="attr">add:</span> [<span class="string">&quot;MKNOD&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>一般推荐的做法是先删除所有的配置，然后只添加你的应用程序实际需要的，在大部分情况下，应用程序在正常运行中实际上不需要任何 <code>Capabilities</code>，通过删除所有配置来测试，并通过监控审计日志来调试问题，看看哪些功能被阻止了。</p>
<p>请注意，当在 <code>securityContext</code> 中列出要放弃或添加的 <code>Capabilities</code> 时，你要删除内核在命名 <code>Capabilities</code> 时使用的 <code>CAP_</code> 前缀。<code>capsh</code> 工具可以给我们一个比较友好的调试信息，可以来说明你的容器中到底启用了哪些 <code>Capabilities</code>，当然不要在生产容器中使用这个工具，因为这使得攻击者很容易弄清楚哪些 <code>Capabilities</code> 被启用了。</p>
<h2 id="7-以只读文件系统运行-C"><a href="#7-以只读文件系统运行-C" class="headerlink" title="7. 以只读文件系统运行 [C]"></a>7. 以只读文件系统运行 [C]</h2><p>如果你的容器被入侵，而且它有一个可读写的文件系统，那么攻击者就可以随意地改变它的配置、安装软件，并有可能启动其他的漏洞。拥有一个只读的文件系统有助于防止这些类型的安全问题，因为它限制了攻击者可以执行的操作。一般来说，容器不应该要求对容器文件系统进行写入，如果你的应用程序是有状态数据，那么你应该使用外部持久化方法，如数据库、volume 或其他一些服务。另外，确保所有的日志都写到 stdout 或日志转发器上。</p>
<h2 id="8-procMount-C"><a href="#8-procMount-C" class="headerlink" title="8. procMount [C]"></a>8. procMount [C]</h2><p>默认情况下，为了防止潜在的安全问题，容器运行时会屏蔽容器内 <code>/proc</code> 文件系统的某些部分文件。然而有时需要访问 <code>/proc</code> 的这些文件，特别是在使用嵌套容器时，因为它经常被用作集群内构建过程的一部分。该配置只有两个有效的选项：</p>
<ul>
<li><code>Default</code>：保持标准的容器运行时行为</li>
<li><code>Unmasked</code>：它删除 <code>/proc</code> 文件系统的所有屏蔽行为</li>
</ul>
<p>显然只有当我们知道在做什么的时候才应该使用这个配置，如果你是为了构建镜像而使用它，请检查构建工具的最新版本，因为许多工具不再需要这个设置了，最好升级下工具并设置为 <code>Default</code> 默认的 <code>procMount</code>。</p>
<h2 id="9-fsGroup-fsGroupChangePolicy-P"><a href="#9-fsGroup-fsGroupChangePolicy-P" class="headerlink" title="9. fsGroup&#x2F;fsGroupChangePolicy [P]"></a>9. fsGroup&#x2F;fsGroupChangePolicy [P]</h2><p><code>fsGroup</code> 设置定义了一个组，当卷被 pod 挂载时，Kubernetes 将把卷中所有文件的权限改为该组。这里的行为也由 <code>fsGroupChangePolicy</code> 控制，它可以被设置为 <code>onRootMismatch</code> 或 <code>Always</code>。如果设置为 <code>onRootMismatch</code> 则只有当权限与容器 root 的权限不匹配时才会被改变。</p>
<p>不过在使用 <code>fsGroup</code> 时也要慎重，改变整个 volume 卷的组所有权会导致<strong>变慢</strong>，如果是大型文件系统<strong>启动也会延迟</strong>。如果共享同一卷的其他进程没有对新的 GID 的访问权限，它也会对这些进程造成损害。由于这个原因，一些共享文件系统如 NFS，没有实现这个功能。这些设置也不影响临时的 ephemeral 卷。</p>
<h2 id="10-sysctls-P"><a href="#10-sysctls-P" class="headerlink" title="10. sysctls [P]"></a>10. sysctls [P]</h2><p><code>Sysctls</code> 是 Linux 内核的一个功能，它允许管理员修改内核配置。在一个完整的 Linux 操作系统中，这些是通过使用 <code>/etc/sysctl.conf</code> 定义的，也可以使用 <code>sysctl</code> 工具进行修改。</p>
<p><code>securityContext</code> 中的 <code>sysctls</code> 配置允许在容器中修改特定的 <code>sysctls</code>。只有一小部分的 <code>sysctls</code> 可以在每个容器的基础上进行修改，它们都在内核中被命名的。 在这个可以配置的子集中，有些被认为是安全的，而更多的则被认为是不安全的，这取决于对其他 pod 的潜在影响。在集群中，不安全的 <code>sysctls</code> 通常是被禁用，需要由集群管理员专门开启。</p>
<p>鉴于有可能破坏底层操作系统的稳定，除非你有非常特殊的要求，否则应该避免通过 <code>sysctls</code> 修改内核参数。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在用 <code>securityContext</code> 加固你的应用时，有很多事情需要注意。如果使用得当，它们是一种非常有效的工具，我们希望这个列表能帮助你的团队为你的工作负载和环境进行正确的安全配置。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">六一</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">273</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/huiaz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;huiaz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">六一</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">1.7m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">25:34</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
