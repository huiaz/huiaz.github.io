<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CGroups 与 Namespaces | Hui's Blog</title><meta name="author" content="六一"><meta name="copyright" content="六一"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CGroups 与 Namespaces本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。 CGroups 概述CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念：  Task">
<meta property="og:type" content="article">
<meta property="og:title" content="CGroups 与 Namespaces">
<meta property="og:url" content="https://huiaz.github.io/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/index.html">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:description" content="CGroups 与 Namespaces本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。 CGroups 概述CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念：  Task">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://huiaz.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-11T12:32:34.000Z">
<meta property="article:modified_time" content="2025-09-11T14:25:19.626Z">
<meta property="article:author" content="六一">
<meta property="article:tag" content="运维">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://huiaz.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CGroups 与 Namespaces",
  "url": "https://huiaz.github.io/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/",
  "image": "https://huiaz.github.io/img/butterfly-icon.png",
  "datePublished": "2025-09-11T12:32:34.000Z",
  "dateModified": "2025-09-11T14:25:19.626Z",
  "author": [
    {
      "@type": "Person",
      "name": "六一",
      "url": "https://huiaz.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://huiaz.github.io/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CGroups 与 Namespaces',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hui's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">CGroups 与 Namespaces</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">CGroups 与 Namespaces</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-11T12:32:34.000Z" title="发表于 2025-09-11 20:32:34">2025-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-11T14:25:19.626Z" title="更新于 2025-09-11 22:25:19">2025-09-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/">k8s</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/">容器运行时</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="CGroups-与-Namespaces"><a href="#CGroups-与-Namespaces" class="headerlink" title="CGroups 与 Namespaces"></a>CGroups 与 Namespaces</h1><p>本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。</p>
<h2 id="CGroups-概述"><a href="#CGroups-概述" class="headerlink" title="CGroups 概述"></a>CGroups 概述</h2><p><code>CGroups</code> 全称为 <code>Linux Control Group</code>，其作用是限制一组进程使用的资源（CPU、内存等）上限，<code>CGroups</code> 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 <code>CGroups</code> 的基本概念：</p>
<ul>
<li>Task: 在 cgroup 中，task 可以理解为一个进程，但这里的进程和一般意义上的操作系统进程不太一样，实际上是进程 ID 和线程 ID 列表。</li>
<li>CGroup: 即控制组，一个控制组就是一组按照某种标准划分的 Tasks，可以理解为资源限制是以进程组为单位实现的，一个进程加入到某个控制组后，就会受到相应配置的资源限制。</li>
<li>Hierarchy: cgroup 的层级组织关系，cgroup 以树形层级组织，每个 cgroup 子节点默认继承其父 cgroup 节点的配置属性，这样每个 Hierarchy 在初始化会有 root cgroup。</li>
<li>Subsystem: 即子系统，子系统表示具体的资源配置，如 CPU 使用，内存占用等，Subsystem 附加到 Hierarchy 上后可用。</li>
</ul>
<p><code>CGroups</code> 支持的子系统包含以下几类，即为每种可以控制的资源定义了一个子系统:</p>
<ul>
<li><code>cpuset</code>: 为 cgroup 中的进程分配单独的 CPU 节点，即可以绑定到特定的 CPU</li>
<li><code>cpu</code>: 限制 cgroup 中进程的 CPU 使用份额</li>
<li><code>cpuacct</code>: 统计 cgroup 中进程的 CPU 使用情况</li>
<li><code>memory</code>: 限制 cgroup 中进程的内存使用,并能报告内存使用情况</li>
<li><code>devices</code>: 控制 cgroup 中进程能访问哪些文件设备(设备文件的创建、读写)</li>
<li><code>freezer</code>: 挂起或恢复 cgroup 中的 task</li>
<li><code>net_cls</code>: 可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块(traffic contro)对数据包进行控制</li>
<li><code>blkio</code>: 限制 cgroup 中进程的块设备 IO</li>
<li><code>perf_event</code>: 监控 cgroup 中进程的 perf 时间，可用于性能调优</li>
<li><code>hugetlb</code>: hugetlb 的资源控制功能</li>
<li><code>pids</code>: 限制 cgroup 中可以创建的进程数</li>
<li><code>net_prio</code>: 允许管理员动态的通过各种应用程序设置网络传输的优先级</li>
</ul>
<p>通过上面的各个子系统，可以看出使用 <code>CGroups</code> 可以控制的资源有: CPU、内存、网络、IO、文件设备等。<code>CGroups</code> 具有以下几个特点：</p>
<ul>
<li>CGroups 的 API 以一个伪文件系统（&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;）的实现方式，用户的程序可以通过文件系统实现 CGroups 的组件管理</li>
<li>CGroups 的组件管理操作单元可以细粒度到线程级别，用户可以创建和销毁 CGroups，从而实现资源载分配和再利用</li>
<li>所有资源管理的功能都以子系统（cpu、cpuset 这些）的方式实现，接口统一子任务创建之初与其父任务处于同一个 CGroups 的控制组</li>
</ul>
<p>我们可以通过查看 <code>/proc/cgroups</code> 文件来查找当前系统支持的 <code>CGroups</code> 子系统:</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/l9c1gs.png" alt="cgroups subsystem"></p>
<p>在使用 <code>CGroups</code> 时需要先挂载，我们可以使用 <code>df -h | grep cgroup</code> 命令进行查看:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ df -h | grep cgroup</span><br><span class="line">tmpfs                          3.9G     0  3.9G   0% /sys/fs/cgroup</span><br></pre></td></tr></table></figure>

<p>可以看到被挂载到了 cd，cgroup 其实是一种文件系统类型，所有的操作都是通过文件来完成的，我们可以使用 <code>mount --type cgroup</code>命令查看当前系统挂载了哪些 cgroup：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ mount --type cgroup</span><br><span class="line">cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br></pre></td></tr></table></figure>

<p><code>/sys/fs/cgroup</code> 目录下的每个子目录就对应着一个子系统，cgroup 是以目录形式组织的，<code>/</code> 是 cgroup 的根目录，但是这个根目录可以被挂载到任意目录，例如 CGroups 的 memory 子系统的挂载点是 <code>/sys/fs/cgroup/memory</code>，那么 <code>/sys/fs/cgroup/memory/</code> 对应 memory 子系统的根目录，我们可以列出该目录下面的文件：c</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root 0 Oct 21 10:10 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  4 root root 0 Oct 21 10:10 kubepods.slice</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.failcnt</span><br><span class="line">--w-------  1 root root 0 Oct 21 10:10 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.numa_stat</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.oom_control</span><br><span class="line">----------  1 root root 0 Oct 21 10:10 memory.pressure_level</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.stat</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.swappiness</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.use_hierarchy</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 notify_on_release</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 release_agent</span><br><span class="line">drwxr-xr-x 65 root root 0 Oct 21 10:25 system.slice</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 tasks</span><br><span class="line">drwxr-xr-x  2 root root 0 Oct 21 10:10 user.slice</span><br></pre></td></tr></table></figure>

<p>上面包含 <code>kubepods.slice</code>、<code>system.slice</code>、<code>user.slice</code> 等目录，这些目录下可能还会有子目录，相当于一颗有层级关系的树来进行组织：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/</span><br><span class="line">├── kubepods.slice</span><br><span class="line">├── system.slice</span><br><span class="line">└── user.slice</span><br></pre></td></tr></table></figure>

<p>例如我在节点上使用 systemd 管理了一个 Prometheus 的应用，我们可以使用 <code>systemctl status prometheus</code> 命令查看 Prometheus 进程所在的 cgroup 为 <code>/system.slice/prometheus.service</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ systemctl status prometheus</span><br><span class="line">● prometheus.service - prometheus service</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2021-10-21 10:10:12 CST; 1h 40min ago</span><br><span class="line">     Docs: https://prometheus.io</span><br><span class="line"> Main PID: 1065 (prometheus)</span><br><span class="line">    Tasks: 10</span><br><span class="line">   Memory: 167.4M</span><br><span class="line">   CGroup: /system.slice/prometheus.service</span><br><span class="line">           └─1065 /root/p8strain/prometheus-2.30.2.linux-amd64/prometheus --config.file=/root/p8strain/prometheu...</span><br></pre></td></tr></table></figure>

<p>上面显示的 CGroup 只是一个相对的路径，实际的文件系统目录是在对应的子系统下面，比如 <code>/sys/fs/cgroup/cpu/system.slice/prometheus.service</code>、<code>/sys/fs/cgroup/memory/system.slice/prometheus.service</code> 目录：</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/ld20w2.png" alt="prometheus cgroup"></p>
<p>这其实可以理解为 cpu 和 memory 子系统被附加到了 <code>/system.slice/prometheus.service</code> 这个 cgroup 上。</p>
<blockquote>
<p>如果 linux 系统使用 systemd 初始化系统，初始化进程会生成一个 root cgroup，每个 <code>systemd unit</code> 都将会被分配一个 cgroup，同样可以配置容器运行时如 containerd 选择使用 cgroupfs 或 systemd 作为 cgroup 驱动，containerd 默认使用的是 cgroupfs，但对于使用了 systemd 的 linux 发行版来说就同时存在两个 cgroup 管理器，对于该服务器上启动的容器使用的是 cgroupfs，而对于其他 systemd 管理的进程使用的是 systemd，这样在服务器资源负载高的情况下可能会变的不稳定。因此对于使用了 systemd 的 linux 系统，推荐将容器运行时的 cgroup 驱动使用 systemd。</p>
</blockquote>
<h2 id="CGroup-测试"><a href="#CGroup-测试" class="headerlink" title="CGroup 测试"></a>CGroup 测试</h2><p>接下来我们来尝试手动设置下 cgroup，以 CPU 这个子系统为例进行说明，首先我们在 <code>/sys/fs/cgroup/cpu</code> 目录下面创建一个名为 <code>ydzs.test</code> 的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ mkdir -p /sys/fs/cgroup/cpu/ydzs.test</span><br><span class="line">➜  ~ ls /sys/fs/cgroup/cpu/ydzs.test/</span><br><span class="line">cgroup.clone_children  cpuacct.stat          cpu.cfs_period_us  cpu.rt_runtime_us  notify_on_release</span><br><span class="line">cgroup.event_control   cpuacct.usage         cpu.cfs_quota_us   cpu.shares         tasks</span><br><span class="line">cgroup.procs           cpuacct.usage_percpu  cpu.rt_period_us   cpu.stat</span><br></pre></td></tr></table></figure>

<p>我们可以看到目录创建完成后，下面就会已经自动创建 cgroup 的相关文件，这里我们重点关注 <code>cpu.cfs_period_us</code> 和 <code>cpu.cfs_quota_us</code> 这两个文件，前面一个是用来配置 CPU 时间周期长度的，默认为 <code>100000us</code>，后者用来设置在此时间周期长度内所能使用的 CPU 时间数，默认值为-1，表示不受时间限制。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_period_us</span><br><span class="line">100000</span><br><span class="line">➜  ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</span><br><span class="line">-1</span><br></pre></td></tr></table></figure>

<p>现在我们写一个简单的 Python 脚本来消耗 CPU：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cgroup.py</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>直接执行这个死循环脚本即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ python cgroup.py &amp;</span><br><span class="line">[1] 2113</span><br></pre></td></tr></table></figure>

<p>使用 top 命令可以看到进程号 2113 的 CPU 使用率达到了 100%</p>
<p><img src="https://picdn.youdianzhishi.com/images/20211021121336.png" alt="cgroup test"></p>
<p>现在我们将这个进程 ID 写入到 <code>/sys/fs/cgroup/cpu/ydzs.test/tasks</code> 文件下面去，然后设置 <code>/sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</code> 为 <code>10000us</code>，因为 <code>cpu.cfs_period_us</code> 默认值为 <code>100000us</code>，所以这表示我们要限制 CPU 使用率为 10%：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ echo 2113 &gt; /sys/fs/cgroup/cpu/ydzs.test/tasks</span><br><span class="line">➜  ~ echo 10000 &gt; /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</span><br></pre></td></tr></table></figure>

<p>设置完过后上面我们的测试进程 CPU 就会被限制在 10% 左右了，再次使用 top 命令查看该进程可以验证。</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/5hh68b.png" alt="cgroup test valid"></p>
<p>如果要限制内存等其他资源的话，同样去对应的子系统下面设置资源，并将进程 ID 加入 tasks 中即可。如果要删除这个 cgroup，直接删除文件夹是不行的，需要使用 <code>libcgroup</code> 工具：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ yum install libcgroup libcgroup-tools</span><br><span class="line">➜  ~ cgdelete cpu:ydzs.test</span><br><span class="line">➜  ~ ls /sys/fs/cgroup/cpu/ydzs.test</span><br><span class="line">ls: cannot access /sys/fs/cgroup/cpu/ydzs.test: No such file or directory</span><br></pre></td></tr></table></figure>

<h2 id="在容器中使用-CGroups"><a href="#在容器中使用-CGroups" class="headerlink" title="在容器中使用 CGroups"></a>在容器中使用 CGroups</h2><p>上面我们测试了一个普通应用如何配置 cgroup，接下来我们在 Containerd 的容器中来使用 cgroup，比如使用 nerdctl 启动一个 nginx 容器，并限制其使用内存为 50M:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl run -d -m 50m --name nginx nginx:alpine</span><br><span class="line">8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">➜  ~ nerdctl ps</span><br><span class="line">CONTAINER ID    IMAGE                             COMMAND                   CREATED           STATUS    PORTS    NAMES</span><br><span class="line">8690c7dba4ff    docker.io/library/nginx:alpine    &quot;/docker-entrypoint.…&quot;    53 seconds ago    Up                 nginx</span><br></pre></td></tr></table></figure>

<p>在使用 <code>nerdctl run</code> 启动容器的时候可以使用 <code>-m</code> 或 <code>--memory</code> 参数来现在内存，启动完成后该容器的 cgroup 会出现在 名为 <code>default</code> 的目录下面，比如查看内存子系统的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/default/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Oct 21 15:01 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>上面我们启动的 nginx 容器 ID 的目录会出现在 <code>/sys/fs/cgroup/memory/default/</code> 下面，该文件夹下面有很多和内存相关的 cgroup 配置文件，要进行相关的配置就需要在该目录下对应的文件中去操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.failcnt</span><br><span class="line">--w------- 1 root root 0 Oct 21 15:01 memory.force_empty</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.numa_stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.oom_control</span><br><span class="line">---------- 1 root root 0 Oct 21 15:01 memory.pressure_level</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.swappiness</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.use_hierarchy</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 notify_on_release</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 tasks</span><br></pre></td></tr></table></figure>

<p>我们这里需要关心的是 <code>memory.limit_in_bytes</code> 文件，该文件就是用来设置内存大小的，正常应该是 50M 的内存限制：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/memory.limit_in_bytes</span><br><span class="line">52428800</span><br></pre></td></tr></table></figure>

<p>同样我们的 nginx 容器进程 ID 也会出现在上面的 <code>tasks</code> 文件中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/tasks</span><br><span class="line">2686</span><br><span class="line">2815</span><br><span class="line">2816</span><br><span class="line">2817</span><br><span class="line">2818</span><br></pre></td></tr></table></figure>

<p>我们可以通过如下命令过滤该进程号，可以看出第一行的 2686 就是 nginx 进程在主机上的进程 ID，下面几个是这个进程下的线程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ps -ef | grep 2686</span><br><span class="line">root       2686   2656  0 15:01 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">101        2815   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2816   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2817   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2818   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">root       2950   1976  0 15:36 pts/0    00:00:00 grep --color=auto 2686</span><br></pre></td></tr></table></figure>

<p>我们删除这个容器后，<code>/sys/fs/cgroup/memory/default/</code> 目录下的容器 ID 文件夹也会自动删除。</p>
<h2 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h2><p><code>namespace</code> 也称命名空间，是 Linux 为我们提供的用于隔离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用个人 PC 时，我们并没有运行多个完全分离的服务器的需求，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这是我们不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。而我们这里的容器其实就通过 Linux 的 Namespaces 技术来实现的对不同的容器进行隔离。</p>
<p>linux 共有 6(7)种命名空间:</p>
<ul>
<li><code>ipc namespace</code>: 管理对 IPC 资源（进程间通信（信号量、消息队列和共享内存）的访问</li>
<li><code>net namespace</code>: 网络设备、网络栈、端口等隔离</li>
<li><code>mnt namespace</code>: 文件系统挂载点隔离</li>
<li><code>pid namespace</code>: 用于进程隔离</li>
<li><code>user namespace</code>: 用户和用户组隔离（3.8 以后的内核才支持）</li>
<li><code>uts namespace</code>: 主机和域名隔离</li>
<li><code>cgroup namespace</code>：用于 cgroup 根目录隔离（4.6 以后版本的内核才支持）</li>
</ul>
<p>我们可以通过 <code>lsns</code> 命令查看当前系统已经创建的命名空间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ lsns</span><br><span class="line">        NS TYPE  NPROCS   PID USER    COMMAND</span><br><span class="line">4026531836 pid      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531837 user     143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531838 uts      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531839 ipc      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531840 mnt      138     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531856 mnt        1    28 root    kdevtmpfs</span><br><span class="line">4026531956 net      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026532503 mnt        2   728 root    /usr/sbin/NetworkManager --no-daemon</span><br><span class="line">4026532504 mnt        1   745 chrony  /usr/sbin/chronyd</span><br><span class="line">4026532642 mnt        1  1076 grafana /usr/sbin/grafana-server --config=/etc/grafana/grafana.ini --pidfile=/var/run</span><br></pre></td></tr></table></figure>

<p>要查看一个进程所属的命名空间信息，可以到 <code>/proc/&lt;pid&gt;/ns</code> 目录下查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ps -ef | grep prometheus</span><br><span class="line">root       1065      1  0 10:10 ?        00:01:13 /root/p8strain/prometheus-2.30.2.linux-amd64/prometheus --config.file=/root/p8strain/prometheus-2.30.2.linux-amd64/prometheus.yml</span><br><span class="line">➜  ~ ll /proc/1065/ns</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 ipc -&gt; ipc:[4026531839]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 mnt -&gt; mnt:[4026531840]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 net -&gt; net:[4026531956]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 pid -&gt; pid:[4026531836]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 uts -&gt; uts:[4026531838]</span><br></pre></td></tr></table></figure>

<p>这些 namespace 都是链接文件, 格式为 <code>namespaceType:[inode number]</code>，<code>inode number</code> 用来标识一个 namespace，可以理解为 namespace id，如果两个进程的某个命名空间的链接文件指向同一个，那么其相关资源在同一个命名空间中，也就没有隔离了。比如同样针对上面运行的 nginx 容器，我们查看其命名空间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ lsns  |grep nginx</span><br><span class="line">4026532505 mnt        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532506 uts        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532507 ipc        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532508 pid        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532510 net        5  3171 root    nginx: master process nginx -g daemon off</span><br></pre></td></tr></table></figure>

<p>可以看出 nginx 容器启动后，已经为该容器自动创建了单独的 <code>mtn</code>、<code>uts</code>、<code>ipc</code>、<code>pid</code>、<code>net</code> 命名空间，也就是这个容器在这些方面是独立隔离的，其他容器想要和该容器共享某一个命名空间，那么就需要指向同一个命名空间。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://huiaz.github.io">六一</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://huiaz.github.io/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/">https://huiaz.github.io/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://huiaz.github.io" target="_blank">Hui's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%BF%90%E7%BB%B4/">运维</a><a class="post-meta__tags" href="/tags/k8s/">k8s</a><a class="post-meta__tags" href="/tags/Linux/">Linux</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/09/11/CRD/" title="CRD"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">CRD</div></div><div class="info-2"><div class="info-item-1">CRDCustom Resource Define 简称 CRD，是 Kubernetes（v1.7+）为提高可扩展性，让开发者去自定义资源的一种方式。CRD 资源可以动态注册到集群中，注册完毕后，用户可以通过 kubectl 来创建访问这个自定义的资源对象，类似于操作 Pod 一样。不过需要注意的是 CRD 仅仅是资源的定义而已，需要一个 Controller 去监听 CRD 的各种事件来添加自定义的业务逻辑。 定义如果说只是对 CRD 资源本身进行 CRUD 操作的话，不需要 Controller 也是可以实现的，相当于就是只有数据存入了 etcd 中，而没有对这个数据的相关操作而已。比如我们可以定义一个如下所示的 CRD 资源清单文件： 12345678910111213141516171819202122232425262728293031323334353637383940# crd-demo.yamlapiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  # name 必须...</div></div></div></a><a class="pagination-related" href="/2025/09/11/BGP%20%E5%8D%8F%E8%AE%AE/" title="BGP 协议"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">BGP 协议</div></div><div class="info-2"><div class="info-item-1">什么是 BGP (Border Gateway Protocol)？BGP (Border Gateway Protocol)，即边界网关协议，是互联网上路由选择的核心协议，也被称为“互联网的胶水”或“互联网的骨干”。它是一种外部网关协议 (EGP)，用于在自治系统 (Autonomous System, AS) 之间交换路由信息，从而实现全球互联网的互联互通。 核心概念：  自治系统 (AS)： 互联网上一个由单一行政实体或组织控制的、拥有统一路由策略的 IP 网络集合。每个 AS 都被分配一个唯一的 16 位或 32 位的数字，称为 **AS 号 (ASN)**。例如，大型的互联网服务提供商 (ISP)、大学网络、大型企业网络等都可能是一个 AS。 路由： 数据包从源头到达目的地的路径选择过程。 外部网关协议 (EGP)： 指在不同自治系统之间交换路由信息的协议。 内部网关协议 (IGP)： 指在同一个自治系统内部交换路由信息的协议，例如 OSPF、EIGRP、RIP 等。  BGP 的主要目标是确保数据包能够从全球任何一个 AS 发送到另一个 AS，并找到最佳的（或符合策略...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/11/NFS%20%E5%85%B1%E4%BA%AB%E5%AD%98%E5%82%A8/" title="NFS 共享存储"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">NFS 共享存储</div></div><div class="info-2"><div class="info-item-1">NFS 共享存储前面我们学习了 hostPath 与 Local PV 两种本地存储方式，但是平时我们的应用更多的是无状态服务，可能会同时发布在不同的节点上，这个时候本地存储就不适用了，往往就需要使用到共享存储了，比如最简单常用的网络共享存储 NFS，本节课我们就来介绍下如何在 Kubernetes 下面使用 NFS 共享存储。 安装我们这里为了演示方便，先使用相对简单的 NFS 这种存储资源，接下来我们在节点 192.168.31.31 上来安装 NFS 服务，数据目录：/var/lib/k8s/data/ 关闭防火墙 12➜ systemctl stop firewalld.service➜ systemctl disable firewalld.service    安装配置 nfs 1➜ yum -y install nfs-utils rpcbind    共享目录设置权限： 12➜ mkdir -p /var/lib/k8s/data➜ chmod 755 /var/lib/k8s/data/    配置 nfs，nfs 的默认配置文件在 /etc/exports 文...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Longhorn/" title="Longhorn"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Longhorn</div></div><div class="info-2"><div class="info-item-1">Longhorn前面我们学习了本地存储、NFS 共享存储，除了这些存储类型之外，还有一个块存储，同样为 Kubernetes 提供块存储的方案有很多，比如 Ceph RBD，今天我们为大家介绍的是 Rancher 开源的一款 Kubernetes 的云原生分布式块存储方案 - Longhorn。 使用 Longhorn，可以：  使用 Longhorn 卷作为 Kubernetes 集群中分布式有状态应用程序的持久存储 将你的块存储分区为 Longhorn 卷，以便你可以在有或没有云提供商的情况下使用 Kubernetes 卷 跨多个节点和数据中心复制块存储以提高可用性 将备份数据存储在 NFS 或 AWS S3 等外部存储中 创建跨集群灾难恢复卷，以便可以从第二个 Kubernetes 集群中的备份中快速恢复主 Kubernetes 集群中的数据 调度一个卷的快照，并将备份调度到 NFS 或 S3 兼容的二级存储 从备份还原卷 不中断持久卷的情况下升级 Longhorn  Longhorn 还带有独立的 UI，可以使用 Helm、kubectl 或 Rancher 应用程序目录...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Pod%20%E4%BD%BF%E7%94%A8%E8%BF%9B%E9%98%B6/" title="Pod 使用进阶"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Pod 使用进阶</div></div><div class="info-2"><div class="info-item-1">Pod 使用进阶-深入理解 Pod 对象Pod 资源配置实际上上面几个步骤就是影响一个 Pod 生命周期的大的部分，但是还有一些细节也会在 Pod 的启动过程进行设置，比如在容器启动之前还会为当前的容器设置分配的 CPU、内存等资源，我们知道我们可以通过 CGroup 来对容器的资源进行限制，同样的，在 Pod 中我们也可以直接配置某个容器的使用的 CPU 或者内存的上限。那么 Pod 是如何来使用和控制这些资源的分配的呢？ 首先对于 CPU，我们知道计算机里 CPU 的资源是按“时间片”的方式来进行分配的，系统里的每一个操作都需要 CPU 的处理，所以，哪个任务要是申请的 CPU 时间片越多，那么它得到的 CPU 资源就越多，这个很容器理解。 然后还需要了解下 CGroup 里面对于 CPU 资源的单位换算： 121 CPU =  1000 millicpu（1 Core = 1000m）0.5 CPU = 500 millicpu （0.5 Core = 500m）  这里的 m 就是毫、毫核的意思，Kubernetes 集群中的每一个节点可以通过操作系统的命令来确认本节点的...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Prometheus%20%E7%9B%91%E6%8E%A7%20Kubernetes%20Job%20%E8%B5%84%E6%BA%90%E8%AF%AF%E6%8A%A5%E7%9A%84%E5%9D%91/" title="Prometheus 监控 Kubernetes Job 资源误报的坑"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Prometheus 监控 Kubernetes Job 资源误报的坑</div></div><div class="info-2"><div class="info-item-1">昨天在 Prometheus 课程辅导群里面有同学提到一个问题，是关于 Prometheus 监控 Job 任务误报的问题，大概的意思就 CronJob 控制的 Job，前面执行失败了，监控会触发报警，解决后后面生成的新的 Job 可以正常执行了，但是还是会收到前面的报警：  这是因为一般在执行 Job 任务的时候我们会保留一些历史记录方便排查问题，所以如果之前有失败的 Job 了，即便稍后会变成成功的，那么之前的 Job 也会继续存在，而大部分直接使用 kube-prometheus 安装部署的话使用的默认报警规则是kube_job_status_failed &gt; 0，这显然是不准确的，只有我们去手动删除之前这个失败的 Job 任务才可以消除误报，当然这种方式是可以解决问题的，但是不够自动化，一开始没有想得很深入，想去自动化删除失败的 Job 来解决，但是这也会给运维人员带来问题，就是不方便回头去排查问题。下面我们来重新整理下思路解决下这个问题。 CronJob 会在计划的每个执行时间创建一个 Job 对象，可以通过 .spec.successfulJobsHistory...</div></div></div></a><a class="pagination-related" href="/2025/09/11/top/" title="top"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">top</div></div><div class="info-2"><div class="info-item-1">Linux top 命令top 命令是什么？top（Table of Processes 的缩写）是 Linux 和其他类 Unix 操作系统中最常用的性能监控工具之一。它提供了一个动态、实时的系统进程视图，可以帮助系统管理员和开发者监控系统负载、识别资源占用高的进程，从而进行性能分析和故障排查。  top 命令界面详解当您在终端输入 top 并回车后，会看到一个全屏的、持续更新的界面。这个界面主要分为两部分：系统摘要信息区 和 进程列表区。 12345678910top - 14:30:50 up 3 days, 21:22,  1 user,  load average: 0.05, 0.08, 0.09Tasks: 236 total,   1 running, 235 sleeping,   0 stopped,   0 zombie%Cpu(s):  1.5 us,  0.5 sy,  0.0 ni, 98.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stMiB Mem :  15833.6 total,   7938.4 free,  ...</div></div></div></a><a class="pagination-related" href="/2025/09/11/k8s%20ResourceQuota/" title="k8s ResourceQuota"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s ResourceQuota</div></div><div class="info-2"><div class="info-item-1">描述在 Kubernetes 中如何配置资源配额，并解释其作用。	 🤔 分析过程：此问题旨在考察对Kubernetes资源治理和多租户管理的理解。核心是ResourceQuota对象。一个高质量的回答不仅要说明如何创建ResourceQuota，更要阐明其核心目的：为命名空间（Namespace）设置资源“预算”。这包括计算资源（CPU&#x2F;Memory）、存储资源和对象数量的限制。此外，还应提及它与LimitRange对象的关系，这能体现出对资源管理体系的深入理解。 💡 答案生成：1. 概念或定义ResourceQuota（资源配额）是Kubernetes中的一个策略对象，它为命名空间（Namespace）提供了资源消耗的总量限制。它允许集群管理员为每个命名空间分配一个“预算”，确保单个团队或应用不会消耗掉超出其分配额度的资源，从而影响到集群中的其他用户。 核心要点： 资源配额是作用于命名空间级别的，不是作用于单个Pod或节点的。 2. 作用与目的配置资源配额的主要作用是实现集群的治理和公平性，具体目标包括：  防止“邻居吵闹”问题 (Noisy Neighbor):...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">六一</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CGroups-%E4%B8%8E-Namespaces"><span class="toc-number">1.</span> <span class="toc-text">CGroups 与 Namespaces</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CGroups-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">CGroups 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CGroup-%E6%B5%8B%E8%AF%95"><span class="toc-number">1.2.</span> <span class="toc-text">CGroup 测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8-CGroups"><span class="toc-number">1.3.</span> <span class="toc-text">在容器中使用 CGroups</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Namespaces"><span class="toc-number">1.4.</span> <span class="toc-text">Namespaces</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/27/hexo-beautify/" title="Hexo 博客美化笔记：从零搭建高颜值技术博客">Hexo 博客美化笔记：从零搭建高颜值技术博客</a><time datetime="2026-02-27T04:00:00.000Z" title="发表于 2026-02-27 12:00:00">2026-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/Jenkens-Blue%20Ocean%20%E6%8F%92%E4%BB%B6/" title="Jenkins Blue Ocean">Jenkins Blue Ocean</a><time datetime="2025-09-11T12:42:57.000Z" title="发表于 2025-09-11 20:42:57">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/jenkins-jenkinsfile/" title="Jenkins Jenkinsfile">Jenkins Jenkinsfile</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-trap/" title="Linux-trap">Linux-trap</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-%E6%95%B0%E7%BB%84/" title="Shell-数组">Shell-数组</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 六一</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body></html>