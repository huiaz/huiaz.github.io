<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CGroups 与 Namespaces | Hui's Blog</title><meta name="author" content="六一"><meta name="copyright" content="六一"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CGroups 与 Namespaces本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。 CGroups 概述CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念：  Task">
<meta property="og:type" content="article">
<meta property="og:title" content="CGroups 与 Namespaces">
<meta property="og:url" content="http://example.com/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/index.html">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:description" content="CGroups 与 Namespaces本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。 CGroups 概述CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念：  Task">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-11T12:32:34.000Z">
<meta property="article:modified_time" content="2025-09-11T14:25:19.626Z">
<meta property="article:author" content="六一">
<meta property="article:tag" content="运维">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CGroups 与 Namespaces",
  "url": "http://example.com/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-09-11T12:32:34.000Z",
  "dateModified": "2025-09-11T14:25:19.626Z",
  "author": [
    {
      "@type": "Person",
      "name": "六一",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CGroups 与 Namespaces',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hui's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">CGroups 与 Namespaces</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">CGroups 与 Namespaces</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-11T12:32:34.000Z" title="发表于 2025-09-11 20:32:34">2025-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-11T14:25:19.626Z" title="更新于 2025-09-11 22:25:19">2025-09-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/">k8s</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/">容器运行时</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="CGroups-与-Namespaces"><a href="#CGroups-与-Namespaces" class="headerlink" title="CGroups 与 Namespaces"></a>CGroups 与 Namespaces</h1><p>本节我们来一起了解下容器背后的两个核心技术：CGroups 和 Namespace。</p>
<h2 id="CGroups-概述"><a href="#CGroups-概述" class="headerlink" title="CGroups 概述"></a>CGroups 概述</h2><p><code>CGroups</code> 全称为 <code>Linux Control Group</code>，其作用是限制一组进程使用的资源（CPU、内存等）上限，<code>CGroups</code> 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 <code>CGroups</code> 的基本概念：</p>
<ul>
<li>Task: 在 cgroup 中，task 可以理解为一个进程，但这里的进程和一般意义上的操作系统进程不太一样，实际上是进程 ID 和线程 ID 列表。</li>
<li>CGroup: 即控制组，一个控制组就是一组按照某种标准划分的 Tasks，可以理解为资源限制是以进程组为单位实现的，一个进程加入到某个控制组后，就会受到相应配置的资源限制。</li>
<li>Hierarchy: cgroup 的层级组织关系，cgroup 以树形层级组织，每个 cgroup 子节点默认继承其父 cgroup 节点的配置属性，这样每个 Hierarchy 在初始化会有 root cgroup。</li>
<li>Subsystem: 即子系统，子系统表示具体的资源配置，如 CPU 使用，内存占用等，Subsystem 附加到 Hierarchy 上后可用。</li>
</ul>
<p><code>CGroups</code> 支持的子系统包含以下几类，即为每种可以控制的资源定义了一个子系统:</p>
<ul>
<li><code>cpuset</code>: 为 cgroup 中的进程分配单独的 CPU 节点，即可以绑定到特定的 CPU</li>
<li><code>cpu</code>: 限制 cgroup 中进程的 CPU 使用份额</li>
<li><code>cpuacct</code>: 统计 cgroup 中进程的 CPU 使用情况</li>
<li><code>memory</code>: 限制 cgroup 中进程的内存使用,并能报告内存使用情况</li>
<li><code>devices</code>: 控制 cgroup 中进程能访问哪些文件设备(设备文件的创建、读写)</li>
<li><code>freezer</code>: 挂起或恢复 cgroup 中的 task</li>
<li><code>net_cls</code>: 可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块(traffic contro)对数据包进行控制</li>
<li><code>blkio</code>: 限制 cgroup 中进程的块设备 IO</li>
<li><code>perf_event</code>: 监控 cgroup 中进程的 perf 时间，可用于性能调优</li>
<li><code>hugetlb</code>: hugetlb 的资源控制功能</li>
<li><code>pids</code>: 限制 cgroup 中可以创建的进程数</li>
<li><code>net_prio</code>: 允许管理员动态的通过各种应用程序设置网络传输的优先级</li>
</ul>
<p>通过上面的各个子系统，可以看出使用 <code>CGroups</code> 可以控制的资源有: CPU、内存、网络、IO、文件设备等。<code>CGroups</code> 具有以下几个特点：</p>
<ul>
<li>CGroups 的 API 以一个伪文件系统（&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;）的实现方式，用户的程序可以通过文件系统实现 CGroups 的组件管理</li>
<li>CGroups 的组件管理操作单元可以细粒度到线程级别，用户可以创建和销毁 CGroups，从而实现资源载分配和再利用</li>
<li>所有资源管理的功能都以子系统（cpu、cpuset 这些）的方式实现，接口统一子任务创建之初与其父任务处于同一个 CGroups 的控制组</li>
</ul>
<p>我们可以通过查看 <code>/proc/cgroups</code> 文件来查找当前系统支持的 <code>CGroups</code> 子系统:</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/l9c1gs.png" alt="cgroups subsystem"></p>
<p>在使用 <code>CGroups</code> 时需要先挂载，我们可以使用 <code>df -h | grep cgroup</code> 命令进行查看:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ df -h | grep cgroup</span><br><span class="line">tmpfs                          3.9G     0  3.9G   0% /sys/fs/cgroup</span><br></pre></td></tr></table></figure>

<p>可以看到被挂载到了 cd，cgroup 其实是一种文件系统类型，所有的操作都是通过文件来完成的，我们可以使用 <code>mount --type cgroup</code>命令查看当前系统挂载了哪些 cgroup：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ mount --type cgroup</span><br><span class="line">cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br></pre></td></tr></table></figure>

<p><code>/sys/fs/cgroup</code> 目录下的每个子目录就对应着一个子系统，cgroup 是以目录形式组织的，<code>/</code> 是 cgroup 的根目录，但是这个根目录可以被挂载到任意目录，例如 CGroups 的 memory 子系统的挂载点是 <code>/sys/fs/cgroup/memory</code>，那么 <code>/sys/fs/cgroup/memory/</code> 对应 memory 子系统的根目录，我们可以列出该目录下面的文件：c</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 cgroup.clone_children</span><br><span class="line">--w--w--w-  1 root root 0 Oct 21 10:10 cgroup.event_control</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 cgroup.procs</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 cgroup.sane_behavior</span><br><span class="line">drwxr-xr-x  4 root root 0 Oct 21 10:10 kubepods.slice</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.failcnt</span><br><span class="line">--w-------  1 root root 0 Oct 21 10:10 memory.force_empty</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.failcnt</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.numa_stat</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.oom_control</span><br><span class="line">----------  1 root root 0 Oct 21 10:10 memory.pressure_level</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.stat</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.swappiness</span><br><span class="line">-r--r--r--  1 root root 0 Oct 21 10:10 memory.usage_in_bytes</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 memory.use_hierarchy</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 notify_on_release</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 release_agent</span><br><span class="line">drwxr-xr-x 65 root root 0 Oct 21 10:25 system.slice</span><br><span class="line">-rw-r--r--  1 root root 0 Oct 21 10:10 tasks</span><br><span class="line">drwxr-xr-x  2 root root 0 Oct 21 10:10 user.slice</span><br></pre></td></tr></table></figure>

<p>上面包含 <code>kubepods.slice</code>、<code>system.slice</code>、<code>user.slice</code> 等目录，这些目录下可能还会有子目录，相当于一颗有层级关系的树来进行组织：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/</span><br><span class="line">├── kubepods.slice</span><br><span class="line">├── system.slice</span><br><span class="line">└── user.slice</span><br></pre></td></tr></table></figure>

<p>例如我在节点上使用 systemd 管理了一个 Prometheus 的应用，我们可以使用 <code>systemctl status prometheus</code> 命令查看 Prometheus 进程所在的 cgroup 为 <code>/system.slice/prometheus.service</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ systemctl status prometheus</span><br><span class="line">● prometheus.service - prometheus service</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2021-10-21 10:10:12 CST; 1h 40min ago</span><br><span class="line">     Docs: https://prometheus.io</span><br><span class="line"> Main PID: 1065 (prometheus)</span><br><span class="line">    Tasks: 10</span><br><span class="line">   Memory: 167.4M</span><br><span class="line">   CGroup: /system.slice/prometheus.service</span><br><span class="line">           └─1065 /root/p8strain/prometheus-2.30.2.linux-amd64/prometheus --config.file=/root/p8strain/prometheu...</span><br></pre></td></tr></table></figure>

<p>上面显示的 CGroup 只是一个相对的路径，实际的文件系统目录是在对应的子系统下面，比如 <code>/sys/fs/cgroup/cpu/system.slice/prometheus.service</code>、<code>/sys/fs/cgroup/memory/system.slice/prometheus.service</code> 目录：</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/ld20w2.png" alt="prometheus cgroup"></p>
<p>这其实可以理解为 cpu 和 memory 子系统被附加到了 <code>/system.slice/prometheus.service</code> 这个 cgroup 上。</p>
<blockquote>
<p>如果 linux 系统使用 systemd 初始化系统，初始化进程会生成一个 root cgroup，每个 <code>systemd unit</code> 都将会被分配一个 cgroup，同样可以配置容器运行时如 containerd 选择使用 cgroupfs 或 systemd 作为 cgroup 驱动，containerd 默认使用的是 cgroupfs，但对于使用了 systemd 的 linux 发行版来说就同时存在两个 cgroup 管理器，对于该服务器上启动的容器使用的是 cgroupfs，而对于其他 systemd 管理的进程使用的是 systemd，这样在服务器资源负载高的情况下可能会变的不稳定。因此对于使用了 systemd 的 linux 系统，推荐将容器运行时的 cgroup 驱动使用 systemd。</p>
</blockquote>
<h2 id="CGroup-测试"><a href="#CGroup-测试" class="headerlink" title="CGroup 测试"></a>CGroup 测试</h2><p>接下来我们来尝试手动设置下 cgroup，以 CPU 这个子系统为例进行说明，首先我们在 <code>/sys/fs/cgroup/cpu</code> 目录下面创建一个名为 <code>ydzs.test</code> 的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ mkdir -p /sys/fs/cgroup/cpu/ydzs.test</span><br><span class="line">➜  ~ ls /sys/fs/cgroup/cpu/ydzs.test/</span><br><span class="line">cgroup.clone_children  cpuacct.stat          cpu.cfs_period_us  cpu.rt_runtime_us  notify_on_release</span><br><span class="line">cgroup.event_control   cpuacct.usage         cpu.cfs_quota_us   cpu.shares         tasks</span><br><span class="line">cgroup.procs           cpuacct.usage_percpu  cpu.rt_period_us   cpu.stat</span><br></pre></td></tr></table></figure>

<p>我们可以看到目录创建完成后，下面就会已经自动创建 cgroup 的相关文件，这里我们重点关注 <code>cpu.cfs_period_us</code> 和 <code>cpu.cfs_quota_us</code> 这两个文件，前面一个是用来配置 CPU 时间周期长度的，默认为 <code>100000us</code>，后者用来设置在此时间周期长度内所能使用的 CPU 时间数，默认值为-1，表示不受时间限制。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_period_us</span><br><span class="line">100000</span><br><span class="line">➜  ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</span><br><span class="line">-1</span><br></pre></td></tr></table></figure>

<p>现在我们写一个简单的 Python 脚本来消耗 CPU：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cgroup.py</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>直接执行这个死循环脚本即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ python cgroup.py &amp;</span><br><span class="line">[1] 2113</span><br></pre></td></tr></table></figure>

<p>使用 top 命令可以看到进程号 2113 的 CPU 使用率达到了 100%</p>
<p><img src="https://picdn.youdianzhishi.com/images/20211021121336.png" alt="cgroup test"></p>
<p>现在我们将这个进程 ID 写入到 <code>/sys/fs/cgroup/cpu/ydzs.test/tasks</code> 文件下面去，然后设置 <code>/sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</code> 为 <code>10000us</code>，因为 <code>cpu.cfs_period_us</code> 默认值为 <code>100000us</code>，所以这表示我们要限制 CPU 使用率为 10%：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ echo 2113 &gt; /sys/fs/cgroup/cpu/ydzs.test/tasks</span><br><span class="line">➜  ~ echo 10000 &gt; /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us</span><br></pre></td></tr></table></figure>

<p>设置完过后上面我们的测试进程 CPU 就会被限制在 10% 左右了，再次使用 top 命令查看该进程可以验证。</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/5hh68b.png" alt="cgroup test valid"></p>
<p>如果要限制内存等其他资源的话，同样去对应的子系统下面设置资源，并将进程 ID 加入 tasks 中即可。如果要删除这个 cgroup，直接删除文件夹是不行的，需要使用 <code>libcgroup</code> 工具：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ yum install libcgroup libcgroup-tools</span><br><span class="line">➜  ~ cgdelete cpu:ydzs.test</span><br><span class="line">➜  ~ ls /sys/fs/cgroup/cpu/ydzs.test</span><br><span class="line">ls: cannot access /sys/fs/cgroup/cpu/ydzs.test: No such file or directory</span><br></pre></td></tr></table></figure>

<h2 id="在容器中使用-CGroups"><a href="#在容器中使用-CGroups" class="headerlink" title="在容器中使用 CGroups"></a>在容器中使用 CGroups</h2><p>上面我们测试了一个普通应用如何配置 cgroup，接下来我们在 Containerd 的容器中来使用 cgroup，比如使用 nerdctl 启动一个 nginx 容器，并限制其使用内存为 50M:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ nerdctl run -d -m 50m --name nginx nginx:alpine</span><br><span class="line">8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">➜  ~ nerdctl ps</span><br><span class="line">CONTAINER ID    IMAGE                             COMMAND                   CREATED           STATUS    PORTS    NAMES</span><br><span class="line">8690c7dba4ff    docker.io/library/nginx:alpine    &quot;/docker-entrypoint.…&quot;    53 seconds ago    Up                 nginx</span><br></pre></td></tr></table></figure>

<p>在使用 <code>nerdctl run</code> 启动容器的时候可以使用 <code>-m</code> 或 <code>--memory</code> 参数来现在内存，启动完成后该容器的 cgroup 会出现在 名为 <code>default</code> 的目录下面，比如查看内存子系统的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/default/</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Oct 21 15:01 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>上面我们启动的 nginx 容器 ID 的目录会出现在 <code>/sys/fs/cgroup/memory/default/</code> 下面，该文件夹下面有很多和内存相关的 cgroup 配置文件，要进行相关的配置就需要在该目录下对应的文件中去操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.failcnt</span><br><span class="line">--w------- 1 root root 0 Oct 21 15:01 memory.force_empty</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.slabinfo</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.tcp.usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.max_usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.failcnt</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.max_usage_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.memsw.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.move_charge_at_immigrate</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.numa_stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.oom_control</span><br><span class="line">---------- 1 root root 0 Oct 21 15:01 memory.pressure_level</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.soft_limit_in_bytes</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.swappiness</span><br><span class="line">-r--r--r-- 1 root root 0 Oct 21 15:01 memory.usage_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 memory.use_hierarchy</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 notify_on_release</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 21 15:01 tasks</span><br></pre></td></tr></table></figure>

<p>我们这里需要关心的是 <code>memory.limit_in_bytes</code> 文件，该文件就是用来设置内存大小的，正常应该是 50M 的内存限制：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/memory.limit_in_bytes</span><br><span class="line">52428800</span><br></pre></td></tr></table></figure>

<p>同样我们的 nginx 容器进程 ID 也会出现在上面的 <code>tasks</code> 文件中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/tasks</span><br><span class="line">2686</span><br><span class="line">2815</span><br><span class="line">2816</span><br><span class="line">2817</span><br><span class="line">2818</span><br></pre></td></tr></table></figure>

<p>我们可以通过如下命令过滤该进程号，可以看出第一行的 2686 就是 nginx 进程在主机上的进程 ID，下面几个是这个进程下的线程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ps -ef | grep 2686</span><br><span class="line">root       2686   2656  0 15:01 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">101        2815   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2816   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2817   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">101        2818   2686  0 15:01 ?        00:00:00 nginx: worker process</span><br><span class="line">root       2950   1976  0 15:36 pts/0    00:00:00 grep --color=auto 2686</span><br></pre></td></tr></table></figure>

<p>我们删除这个容器后，<code>/sys/fs/cgroup/memory/default/</code> 目录下的容器 ID 文件夹也会自动删除。</p>
<h2 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h2><p><code>namespace</code> 也称命名空间，是 Linux 为我们提供的用于隔离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用个人 PC 时，我们并没有运行多个完全分离的服务器的需求，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这是我们不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。而我们这里的容器其实就通过 Linux 的 Namespaces 技术来实现的对不同的容器进行隔离。</p>
<p>linux 共有 6(7)种命名空间:</p>
<ul>
<li><code>ipc namespace</code>: 管理对 IPC 资源（进程间通信（信号量、消息队列和共享内存）的访问</li>
<li><code>net namespace</code>: 网络设备、网络栈、端口等隔离</li>
<li><code>mnt namespace</code>: 文件系统挂载点隔离</li>
<li><code>pid namespace</code>: 用于进程隔离</li>
<li><code>user namespace</code>: 用户和用户组隔离（3.8 以后的内核才支持）</li>
<li><code>uts namespace</code>: 主机和域名隔离</li>
<li><code>cgroup namespace</code>：用于 cgroup 根目录隔离（4.6 以后版本的内核才支持）</li>
</ul>
<p>我们可以通过 <code>lsns</code> 命令查看当前系统已经创建的命名空间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ lsns</span><br><span class="line">        NS TYPE  NPROCS   PID USER    COMMAND</span><br><span class="line">4026531836 pid      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531837 user     143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531838 uts      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531839 ipc      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531840 mnt      138     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026531856 mnt        1    28 root    kdevtmpfs</span><br><span class="line">4026531956 net      143     1 root    /usr/lib/systemd/systemd --switched-root --system --deserialize 22</span><br><span class="line">4026532503 mnt        2   728 root    /usr/sbin/NetworkManager --no-daemon</span><br><span class="line">4026532504 mnt        1   745 chrony  /usr/sbin/chronyd</span><br><span class="line">4026532642 mnt        1  1076 grafana /usr/sbin/grafana-server --config=/etc/grafana/grafana.ini --pidfile=/var/run</span><br></pre></td></tr></table></figure>

<p>要查看一个进程所属的命名空间信息，可以到 <code>/proc/&lt;pid&gt;/ns</code> 目录下查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ps -ef | grep prometheus</span><br><span class="line">root       1065      1  0 10:10 ?        00:01:13 /root/p8strain/prometheus-2.30.2.linux-amd64/prometheus --config.file=/root/p8strain/prometheus-2.30.2.linux-amd64/prometheus.yml</span><br><span class="line">➜  ~ ll /proc/1065/ns</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 ipc -&gt; ipc:[4026531839]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 mnt -&gt; mnt:[4026531840]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 net -&gt; net:[4026531956]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 pid -&gt; pid:[4026531836]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx 1 root root 0 Oct 21 15:58 uts -&gt; uts:[4026531838]</span><br></pre></td></tr></table></figure>

<p>这些 namespace 都是链接文件, 格式为 <code>namespaceType:[inode number]</code>，<code>inode number</code> 用来标识一个 namespace，可以理解为 namespace id，如果两个进程的某个命名空间的链接文件指向同一个，那么其相关资源在同一个命名空间中，也就没有隔离了。比如同样针对上面运行的 nginx 容器，我们查看其命名空间：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ lsns  |grep nginx</span><br><span class="line">4026532505 mnt        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532506 uts        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532507 ipc        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532508 pid        5  3171 root    nginx: master process nginx -g daemon off</span><br><span class="line">4026532510 net        5  3171 root    nginx: master process nginx -g daemon off</span><br></pre></td></tr></table></figure>

<p>可以看出 nginx 容器启动后，已经为该容器自动创建了单独的 <code>mtn</code>、<code>uts</code>、<code>ipc</code>、<code>pid</code>、<code>net</code> 命名空间，也就是这个容器在这些方面是独立隔离的，其他容器想要和该容器共享某一个命名空间，那么就需要指向同一个命名空间。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">六一</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/">http://example.com/2025/09/11/CGroups%20%E4%B8%8E%20Namespaces/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Hui's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%BF%90%E7%BB%B4/">运维</a><a class="post-meta__tags" href="/tags/k8s/">k8s</a><a class="post-meta__tags" href="/tags/Linux/">Linux</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/09/11/CRD/" title="CRD"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">CRD</div></div><div class="info-2"><div class="info-item-1">CRDCustom Resource Define 简称 CRD，是 Kubernetes（v1.7+）为提高可扩展性，让开发者去自定义资源的一种方式。CRD 资源可以动态注册到集群中，注册完毕后，用户可以通过 kubectl 来创建访问这个自定义的资源对象，类似于操作 Pod 一样。不过需要注意的是 CRD 仅仅是资源的定义而已，需要一个 Controller 去监听 CRD 的各种事件来添加自定义的业务逻辑。 定义如果说只是对 CRD 资源本身进行 CRUD 操作的话，不需要 Controller 也是可以实现的，相当于就是只有数据存入了 etcd 中，而没有对这个数据的相关操作而已。比如我们可以定义一个如下所示的 CRD 资源清单文件： 12345678910111213141516171819202122232425262728293031323334353637383940# crd-demo.yamlapiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  # name 必须...</div></div></div></a><a class="pagination-related" href="/2025/09/11/BGP%20%E5%8D%8F%E8%AE%AE/" title="BGP 协议"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">BGP 协议</div></div><div class="info-2"><div class="info-item-1">什么是 BGP (Border Gateway Protocol)？BGP (Border Gateway Protocol)，即边界网关协议，是互联网上路由选择的核心协议，也被称为“互联网的胶水”或“互联网的骨干”。它是一种外部网关协议 (EGP)，用于在自治系统 (Autonomous System, AS) 之间交换路由信息，从而实现全球互联网的互联互通。 核心概念：  自治系统 (AS)： 互联网上一个由单一行政实体或组织控制的、拥有统一路由策略的 IP 网络集合。每个 AS 都被分配一个唯一的 16 位或 32 位的数字，称为 **AS 号 (ASN)**。例如，大型的互联网服务提供商 (ISP)、大学网络、大型企业网络等都可能是一个 AS。 路由： 数据包从源头到达目的地的路径选择过程。 外部网关协议 (EGP)： 指在不同自治系统之间交换路由信息的协议。 内部网关协议 (IGP)： 指在同一个自治系统内部交换路由信息的协议，例如 OSPF、EIGRP、RIP 等。  BGP 的主要目标是确保数据包能够从全球任何一个 AS 发送到另一个 AS，并找到最佳的（或符合策略...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/11/k8s%20ResourceQuota/" title="k8s ResourceQuota"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s ResourceQuota</div></div><div class="info-2"><div class="info-item-1">描述在 Kubernetes 中如何配置资源配额，并解释其作用。	 🤔 分析过程：此问题旨在考察对Kubernetes资源治理和多租户管理的理解。核心是ResourceQuota对象。一个高质量的回答不仅要说明如何创建ResourceQuota，更要阐明其核心目的：为命名空间（Namespace）设置资源“预算”。这包括计算资源（CPU&#x2F;Memory）、存储资源和对象数量的限制。此外，还应提及它与LimitRange对象的关系，这能体现出对资源管理体系的深入理解。 💡 答案生成：1. 概念或定义ResourceQuota（资源配额）是Kubernetes中的一个策略对象，它为命名空间（Namespace）提供了资源消耗的总量限制。它允许集群管理员为每个命名空间分配一个“预算”，确保单个团队或应用不会消耗掉超出其分配额度的资源，从而影响到集群中的其他用户。 核心要点： 资源配额是作用于命名空间级别的，不是作用于单个Pod或节点的。 2. 作用与目的配置资源配额的主要作用是实现集群的治理和公平性，具体目标包括：  防止“邻居吵闹”问题 (Noisy Neighbor):...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Thanos%20sidecar/" title="Thanos sidecar"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Thanos sidecar</div></div><div class="info-2"><div class="info-item-1">Thanos Sidecar 组件首先将前面章节中的 Prometheus 相关的资源对象全部删除，然后我们需要在 Prometheus 中去自动发现集群的一些资源对象，所以依然需要对应的 RBAC 权限声明： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# prometheus-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata:  name: prometheus  namespace: kube-mon---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:  name: prometheusrules:  - apiGroups:      - &quot;&quot;    resources:      - nodes      - services      - endpoi...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E7%9B%91%E6%8E%A7%20Pod/" title="监控 Pod"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">监控 Pod</div></div><div class="info-2"><div class="info-item-1">监控 Pod前面的 apiserver 实际上就是一种特殊的 Endpoints，现在我们同样来配置一个任务用来专门发现普通类型的 Endpoint，其实就是 Service 关联的 Pod 列表，由于并不是所有的 Endpoints 都会提供 metrics 接口，所以需要我们主动告诉 Prometheus 去发现哪些 Endpoints，当然告诉的方式有很多，不过约定俗成的一种方式是通过 annotations 注解进行通知，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940- job_name: &#x27;endpoints&#x27;  kubernetes_sd_configs:    - role: endpoints  relabel_configs:    # 保留 Service 的注解为 prometheus.io/scrape: true 的 Endpoints    - source_labels: [__meta_kubernetes_service...</div></div></div></a><a class="pagination-related" href="/2025/09/11/LocalDNS/" title="LocalDNS"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">LocalDNS</div></div><div class="info-2"><div class="info-item-1">DNS 优化前面我们讲解了在 Kubernetes 中我们可以使用 CoreDNS 来进行集群的域名解析，但是如果在集群规模较大并发较高的情况下我们仍然需要对 DNS 进行优化，典型的就是大家比较熟悉的 CoreDNS 会出现超时 5s 的情况。 超时原因在 iptables 模式下（默认情况下），每个服务的 kube-proxy 在主机网络名称空间的 nat 表中创建一些 iptables 规则。 比如在集群中具有两个 DNS 服务器实例的 kube-dns 服务，其相关规则大致如下所示： 12345678910(1) -A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES&lt;...&gt;(2) -A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment &quot;kube-system/kube-dns:dns cluster IP&quot; -m udp --dport 53 ...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Linux%20%E7%B3%BB%E7%BB%9F%E4%B8%AD%20crontab/" title="Linux 系统中 crontab"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Linux 系统中 crontab</div></div><div class="info-2"><div class="info-item-1">Linux 系统中 crontab Crontab 的工作原理crontab（cron table） 是 Linux&#x2F;Unix 系统用于设置周期性执行任务的工具。它的核心是 cron 服务（或称为 crond 守护进程），这个服务在系统启动时就会自动运行，并且会一直在后台持续运行，它的主要职责是：  读取定时任务配置： cron 服务会周期性地（通常是每分钟）扫描特定的配置文件，这些文件包含了用户定义的定时任务列表。主要的配置文件位置包括：  系统级别的定时任务： /etc/crontab：系统主 cron 表，通常用于定义系统级的任务。 /etc/cron.d/：该目录下的所有文件都会被 cron 服务读取，每个文件可以定义独立的定时任务。 /etc/cron.hourly/, /etc/cron.daily/, /etc/cron.weekly/, /etc/cron.monthly/：这些目录下的脚本会分别每小时、每天、每周、每月被 cron 服务执行一次。   用户级别的定时任务： /var/spool/cron/：每个用户的 crontab -e 命令编辑的任...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/" title="磁盘管理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">磁盘管理</div></div><div class="info-2"><div class="info-item-1">Linux 系统中的磁盘管理 一、查看磁盘分区信息在 Linux 系统中，有多种命令可以查看磁盘和分区的信息。 1. lsblk：列出块设备信息（最常用、最直观）lsblk 命令以树状结构列出所有块设备（包括磁盘、分区、RAID设备、LVM逻辑卷等），清晰地展示了设备之间的层次关系。 1lsblk  示例输出及解释： 12345678910NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTsda           8:0    0   200G  0 disk ├─sda1        8:1    0     1G  0 part /boot├─sda2        8:2    0     4G  0 part [SWAP]└─sda3        8:3    0 195.9G  0 part   ├─vg01-root 253:0    0    50G  0 lvm  /  └─vg01-home 253:1    0 145.9G  0 lvm  /homesdb           8:16   0     1T...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">六一</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#CGroups-%E4%B8%8E-Namespaces"><span class="toc-number">1.</span> <span class="toc-text">CGroups 与 Namespaces</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CGroups-%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">CGroups 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CGroup-%E6%B5%8B%E8%AF%95"><span class="toc-number">1.2.</span> <span class="toc-text">CGroup 测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8-CGroups"><span class="toc-number">1.3.</span> <span class="toc-text">在容器中使用 CGroups</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Namespaces"><span class="toc-number">1.4.</span> <span class="toc-text">Namespaces</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/27/hexo-beautify/" title="Hexo 博客美化笔记：从零搭建高颜值技术博客">Hexo 博客美化笔记：从零搭建高颜值技术博客</a><time datetime="2026-02-27T04:00:00.000Z" title="发表于 2026-02-27 12:00:00">2026-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/Jenkens-Blue%20Ocean%20%E6%8F%92%E4%BB%B6/" title="Jenkins Blue Ocean">Jenkins Blue Ocean</a><time datetime="2025-09-11T12:42:57.000Z" title="发表于 2025-09-11 20:42:57">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/jenkins-jenkinsfile/" title="Jenkins Jenkinsfile">Jenkins Jenkinsfile</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-trap/" title="Linux-trap">Linux-trap</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-%E6%95%B0%E7%BB%84/" title="Shell-数组">Shell-数组</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 六一</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body></html>