<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>如何修改 Kubernetes 节点 IP 地址? | Hui's Blog</title><meta name="author" content="六一"><meta name="copyright" content="六一"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="如何修改 Kubernetes 节点 IP 地址?昨天网络环境出了点问题，本地的虚拟机搭建的 Kubernetes 环境没有固定 IP，结果节点 IP 变了，当然最简单的方式是将节点重新固定回之前的 IP 地址，但是自己头铁想去修改下集群的 IP 地址，结果一路下来踩了好多坑，压根就没那么简单~ 环境首先看下之前的环境： 1234➜  ~ cat &#x2F;etc&#x2F;hosts192.168.0.111 m">
<meta property="og:type" content="article">
<meta property="og:title" content="如何修改 Kubernetes 节点 IP 地址?">
<meta property="og:url" content="http://example.com/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:description" content="如何修改 Kubernetes 节点 IP 地址?昨天网络环境出了点问题，本地的虚拟机搭建的 Kubernetes 环境没有固定 IP，结果节点 IP 变了，当然最简单的方式是将节点重新固定回之前的 IP 地址，但是自己头铁想去修改下集群的 IP 地址，结果一路下来踩了好多坑，压根就没那么简单~ 环境首先看下之前的环境： 1234➜  ~ cat &#x2F;etc&#x2F;hosts192.168.0.111 m">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-11T12:32:34.000Z">
<meta property="article:modified_time" content="2025-09-11T14:15:52.624Z">
<meta property="article:author" content="六一">
<meta property="article:tag" content="运维">
<meta property="article:tag" content="k8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "如何修改 Kubernetes 节点 IP 地址?",
  "url": "http://example.com/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80?/",
  "image": "http://example.com/img/butterfly-icon.png",
  "datePublished": "2025-09-11T12:32:34.000Z",
  "dateModified": "2025-09-11T14:15:52.624Z",
  "author": [
    {
      "@type": "Person",
      "name": "六一",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '如何修改 Kubernetes 节点 IP 地址?',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hui's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">如何修改 Kubernetes 节点 IP 地址?</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">如何修改 Kubernetes 节点 IP 地址?</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-11T12:32:34.000Z" title="发表于 2025-09-11 20:32:34">2025-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-11T14:15:52.624Z" title="更新于 2025-09-11 22:15:52">2025-09-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/">k8s</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="如何修改-Kubernetes-节点-IP-地址"><a href="#如何修改-Kubernetes-节点-IP-地址" class="headerlink" title="如何修改 Kubernetes 节点 IP 地址?"></a>如何修改 Kubernetes 节点 IP 地址?</h1><p>昨天网络环境出了点问题，本地的虚拟机搭建的 Kubernetes 环境没有固定 IP，结果节点 IP 变了，当然最简单的方式是将节点重新固定回之前的 IP 地址，但是自己头铁想去修改下集群的 IP 地址，结果一路下来踩了好多坑，压根就没那么简单~</p>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>首先看下之前的环境：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /etc/hosts</span><br><span class="line">192.168.0.111 master1</span><br><span class="line">192.168.0.109 node1</span><br><span class="line">192.168.0.110 node2</span><br></pre></td></tr></table></figure>

<p>新的 IP 地址：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ cat /etc/hosts</span><br><span class="line">192.168.0.106 master1</span><br><span class="line">192.168.0.101 node1</span><br><span class="line">192.168.0.105 node2</span><br></pre></td></tr></table></figure>

<p>所以我们需要修改所有节点的 IP 地址。</p>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>首先将所有节点的 <code>/etc/hosts</code> 更改为新的地址。</p>
<blockquote>
<p>提示：在操作任何文件之前<strong>强烈建议先备份</strong>。</p>
</blockquote>
<h3 id="master-节点"><a href="#master-节点" class="headerlink" title="master 节点"></a>master 节点</h3><p>1.备份 <code>/etc/kubernetes</code> 目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ cp -Rf /etc/kubernetes/ /etc/kubernetes-bak</span><br></pre></td></tr></table></figure>

<p>2.替换 <code>/etc/kubernetes</code> 中所有配置文件的 APIServer 地址。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ oldip=192.168.0.111</span><br><span class="line">➜ newip=192.168.0.106</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看之前的</span></span><br><span class="line">➜ find . -type f | xargs grep $oldip</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">替换IP地址</span></span><br><span class="line">➜ find . -type f | xargs sed -i &quot;s/$oldip/$newip/&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查更新后的</span></span><br><span class="line">➜ find . -type f | xargs grep $newip</span><br></pre></td></tr></table></figure>

<p>3.识别 <code>/etc/kubernetes/pki</code> 中以旧的 IP 地址作为 <code>alt name</code> 的证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ cd /etc/kubernetes/pki</span><br><span class="line">➜ for f in $(find -name &quot;*.crt&quot;); do</span><br><span class="line">  openssl x509 -in $f -text -noout &gt; $f.txt;</span><br><span class="line">done</span><br><span class="line">➜ grep -Rl $oldip .</span><br><span class="line">➜ for f in $(find -name &quot;*.crt&quot;); do rm $f.txt; done</span><br></pre></td></tr></table></figure>

<p>4.找到 <code>kube-system</code> 命名空间中引用旧 IP 的 ConfigMap。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取所有的 kube-system 命名空间下面所有的 ConfigMap</span></span><br><span class="line">➜ configmaps=$(kubectl -n kube-system get cm -o name | \</span><br><span class="line">  awk &#x27;&#123;print $1&#125;&#x27; | \</span><br><span class="line">  cut -d &#x27;/&#x27; -f 2)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">获取所有的ConfigMap资源清单</span></span><br><span class="line">➜ dir=$(mktemp -d)</span><br><span class="line">➜ for cf in $configmaps; do</span><br><span class="line">  kubectl -n kube-system get cm $cf -o yaml &gt; $dir/$cf.yaml</span><br><span class="line">done</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">找到所有包含旧 IP 的 ConfigMap</span></span><br><span class="line">➜ grep -Hn $dir/* -e $oldip</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">然后编辑这些 ConfigMap，将旧 IP 替换成新的 IP</span></span><br><span class="line">➜ kubectl -n kube-system edit cm kubeadm-config</span><br><span class="line">➜ kubectl -n kube-system edit cm kube-proxy</span><br></pre></td></tr></table></figure>

<p>这一步非常非常重要，我在操作的时候忽略了这一步，导致 Flannel CNI 启动不起来，一直报错，类似下面的日志信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl logs -f kube-flannel-ds-pspzf -n kube-system</span><br><span class="line">I0512 14:46:26.044229       1 main.go:205] CLI flags config: &#123;etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[ens33] ifaceRegex:[] ipMasq:true subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true&#125;</span><br><span class="line">W0512 14:46:26.044617       1 client_config.go:614] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.</span><br><span class="line">E0512 14:46:56.142921       1 main.go:222] Failed to create SubnetManager: error retrieving pod spec for &#x27;kube-system/kube-flannel-ds-pspzf&#x27;: Get &quot;https://10.96.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-pspzf&quot;: dial tcp 10.96.0.1:443: i/o timeout</span><br></pre></td></tr></table></figure>

<p>其实就是连不上 apiserver，排查了好久才想起来查看 <code>kube-proxy</code> 的日志，其中出现了如下所示的错误信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">E0512 14:53:03.260817       1 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get &quot;https://192.168.0.111:6443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&amp;limit=500&amp;resourceVersion=0&quot;: dial tcp 192.168.0.111:6443: connect: no route to host</span><br></pre></td></tr></table></figure>

<p>这就是因为 kube-proxy 的 ConfigMap 中配置的 apiserver 地址是旧的 IP 地址，所以一定要将其替换成新的。</p>
<p>5.删除第 3 步中 grep 出的证书和私钥，重新生成这些证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ cd /etc/kubernetes/pki</span><br><span class="line">➜ rm apiserver.crt apiserver.key</span><br><span class="line">➜ kubeadm init phase certs apiserver</span><br><span class="line"></span><br><span class="line">➜ rm etcd/peer.crt etcd/peer.key</span><br><span class="line">➜ kubeadm init phase certs etcd-peer</span><br></pre></td></tr></table></figure>

<p>当然也可以全部重新生成：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ kubeadm init phase certs all</span><br></pre></td></tr></table></figure>

<p>6.生成新的 kubeconfig 文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜ cd /etc/kubernetes</span><br><span class="line">➜ rm -f admin.conf kubelet.conf controller-manager.conf scheduler.conf</span><br><span class="line">➜ kubeadm init phase kubeconfig all</span><br><span class="line">I0513 15:33:34.404780   52280 version.go:255] remote version is much newer: v1.24.0; falling back to: stable-1.22</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">覆盖默认的 kubeconfig 文件</span></span><br><span class="line">➜ cp /etc/kubernetes/admin.conf $HOME/.kube/config</span><br></pre></td></tr></table></figure>

<p>7.重启 kubelet。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl restart containerd</span><br><span class="line">➜ systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<p>正常现在可以访问的 Kubernetes 集群了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get nodes</span><br><span class="line">NAME      STATUS     ROLES                  AGE   VERSION</span><br><span class="line">master1   Ready      control-plane,master   48d   v1.22.8</span><br><span class="line">node1     NotReady   &lt;none&gt;                 48d   v1.22.8</span><br><span class="line">node2     NotReady   &lt;none&gt;                 48d   v1.22.8</span><br></pre></td></tr></table></figure>

<h3 id="node-节点"><a href="#node-节点" class="headerlink" title="node 节点"></a>node 节点</h3><p>虽然现在可以访问集群了，但是我们可以看到 Node 节点现在处于 <code>NotReady</code> 状态，我们可以去查看 node2 节点的 kubelet 日志：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜ journalctl -u kubelet -f</span><br><span class="line">......</span><br><span class="line">May 13 15:47:55 node2 kubelet[1194]: E0513 15:47:55.470896    1194 kubelet.go:2412] &quot;Error getting node&quot; err=&quot;node \&quot;node2\&quot; not found&quot;</span><br><span class="line">May 13 15:47:55 node2 kubelet[1194]: E0513 15:47:55.531695    1194 reflector.go:138] k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get &quot;https://192.168.0.111:6443/api/v1/services?limit=500&amp;resourceVersion=0&quot;: dial tcp 192.168.0.111:6443: connect: no route to host</span><br><span class="line">May 13 15:47:55 node2 kubelet[1194]: E0513 15:47:55.571958    1194 kubelet.go:2412] &quot;Error getting node&quot; err=&quot;node \&quot;node2\&quot; not found&quot;</span><br><span class="line">May 13 15:47:55 node2 kubelet[1194]: E0513 15:47:55.673379    1194 kubelet.go:2412] &quot;Error getting node&quot; err=&quot;node \&quot;node2\&quot; not found&quot;</span><br></pre></td></tr></table></figure>

<p>可以看到仍然是在访问之前的 APIServer 地址，那么在什么地方会明确使用 APIServer 的地址呢？我们可以通过下面的命令来查看 kubelet 的启动参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl status kubelet</span><br><span class="line">● kubelet.service - kubelet: The Kubernetes Node Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)</span><br><span class="line">  Drop-In: /usr/lib/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br><span class="line">   Active: active (running) since Fri 2022-05-13 14:37:31 CST; 1h 13min ago</span><br><span class="line">     Docs: https://kubernetes.io/docs/</span><br><span class="line"> Main PID: 1194 (kubelet)</span><br><span class="line">    Tasks: 15</span><br><span class="line">   Memory: 126.9M</span><br><span class="line">   CGroup: /system.slice/kubelet.service</span><br><span class="line">           └─1194 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kub...</span><br><span class="line"></span><br><span class="line">May 13 15:51:08 node2 kubelet[1194]: E0513 15:51:08.787677    1194 kubelet.go:2412] &quot;Error getting node&quot; err=&quot;node \&quot;node2... found&quot;</span><br><span class="line">May 13 15:51:08 node2 kubelet[1194]: E0513 15:51:08.888194    1194 kubelet.go:2412] &quot;Error getting node&quot; err=&quot;node \&quot;node2... found&quot;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>其核心配置文件为 <code>/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</code>，内容如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Note: This dropin only works with kubeadm and kubelet v1.11+</span></span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">Environment=&quot;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that <span class="string">&quot;kubeadm init&quot;</span> and <span class="string">&quot;kubeadm join&quot;</span> generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically</span></span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">This is a file that the user can use <span class="keyword">for</span> overrides of the kubelet args as a last resort. Preferably, the user should use</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">the .NodeRegistration.KubeletExtraArgs object <span class="keyword">in</span> the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.</span></span><br><span class="line">EnvironmentFile=-/etc/sysconfig/kubelet</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS</span><br></pre></td></tr></table></figure>

<p>其中有一个配置 <code>KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf</code>，这里提到了两个配置文件 <code>bootstrap-kubelet.conf</code> 与 <code>kubelet.conf</code>，其中第一个文件不存在：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ cat /etc/kubernetes/bootstrap-kubelet.conf</span><br><span class="line">cat: /etc/kubernetes/bootstrap-kubelet.conf: No such file or directory</span><br></pre></td></tr></table></figure>

<p>而第二个配置文件就是一个 kubeconfig 文件的格式，这个文件中就指定了 APIServer 的地址，可以看到还是之前的 IP 地址：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜ cat /etc/kubernetes/kubelet.conf</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: &lt;......&gt;</span><br><span class="line">    server: https://192.168.0.111:6443</span><br><span class="line">  name: default-cluster</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: default-cluster</span><br><span class="line">    namespace: default</span><br><span class="line">    user: default-auth</span><br><span class="line">  name: default-context</span><br><span class="line">current-context: default-context</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: default-auth</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem</span><br><span class="line">    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem</span><br></pre></td></tr></table></figure>

<p>所以我们最先想到的肯定就是去将这里的 APIServer 地址修改成新的 IP 地址，但是这显然是有问题的，因为相关证书还是以前的，需要重新生成，那么要怎样重新生成该文件呢？</p>
<p>首先备份 kubelet 工作目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf.bak</span><br><span class="line">➜ cp -rf /var/lib/kubelet/ /var/lib/kubelet-bak</span><br></pre></td></tr></table></figure>

<p>删除 kubelet 客户端证书：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜ rm /var/lib/kubelet/pki/kubelet-client*</span><br></pre></td></tr></table></figure>

<p>然后在 master1 节点（具有 <code>/etc/kubernetes/pki/ca.key</code> 文件的节点）去生成 kubelet.conf 文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在master1节点</span></span><br><span class="line">➜ kubeadm kubeconfig user --org system:nodes --client-name system:node:node2 --config kubeadm.yaml &gt; kubelet.conf</span><br></pre></td></tr></table></figure>

<p>然后将 kubelet.conf 文件复制到 node2 节点 <code>/etc/kubernetes/kubelet.conf</code>，然后重新启动 node2 节点上的 kubelet，并等待 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 重新创建。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl restart kubelet</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">重启后等待重新生成 kubelet 客户端证书</span></span><br><span class="line">➜ ll /var/lib/kubelet/pki/</span><br><span class="line">total 12</span><br><span class="line">-rw------- 1 root root 1106 May 13 16:32 kubelet-client-2022-05-13-16-32-35.pem</span><br><span class="line">lrwxrwxrwx 1 root root   59 May 13 16:32 kubelet-client-current.pem -&gt; /var/lib/kubelet/pki/kubelet-client-2022-05-13-16-32-35.pem</span><br><span class="line">-rw-r--r-- 1 root root 2229 Mar 26 14:39 kubelet.crt</span><br><span class="line">-rw------- 1 root root 1675 Mar 26 14:39 kubelet.key</span><br></pre></td></tr></table></figure>

<p>最好我们可以通过手动编辑 <code>kubelet.conf</code> 的方式来指向轮转的 kubelet 客户端证书，将文件中的 <code>client-certificate-data</code> 和 <code>client-key-data</code> 替换为 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem</span><br><span class="line">client-key: /var/lib/kubelet/pki/kubelet-client-current.pem</span><br></pre></td></tr></table></figure>

<p>再次重启 kubelet，正常现在 node2 节点就会变成 <code>Ready</code> 状态了，用同样的方法再次去配置 node1 节点即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get nodes</span><br><span class="line">NAME      STATUS   ROLES                  AGE   VERSION</span><br><span class="line">master1   Ready    control-plane,master   48d   v1.22.8</span><br><span class="line">node1     Ready    &lt;none&gt;                 48d   v1.22.8</span><br><span class="line">node2     Ready    &lt;none&gt;                 48d   v1.22.8</span><br></pre></td></tr></table></figure>

<h2 id="推荐操作"><a href="#推荐操作" class="headerlink" title="推荐操作"></a>推荐操作</h2><p>上面的操作方式虽然可以正常完成我们的需求，但是需要我们对相关证书有一定的了解。除了这种方式之外还有一种更简单的操作。</p>
<p>首先停止 kubelet 并备份要操作的目录：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ systemctl stop kubelet</span><br><span class="line">➜ mv /etc/kubernetes /etc/kubernetes-bak</span><br><span class="line">➜ mv /var/lib/kubelet/ /var/lib/kubelet-bak</span><br></pre></td></tr></table></figure>

<p>将 pki 证书目录保留下来：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ mkdir -p /etc/kubernetes</span><br><span class="line">➜ cp -r /etc/kubernetes-bak/pki /etc/kubernetes</span><br><span class="line">➜ rm /etc/kubernetes/pki/&#123;apiserver.*,etcd/peer.*&#125;</span><br><span class="line">rm: remove regular file ‘/etc/kubernetes/pki/apiserver.crt’? y</span><br><span class="line">rm: remove regular file ‘/etc/kubernetes/pki/apiserver.key’? y</span><br><span class="line">rm: remove regular file ‘/etc/kubernetes/pki/etcd/peer.crt’? y</span><br><span class="line">rm: remove regular file ‘/etc/kubernetes/pki/etcd/peer.key’? y</span><br></pre></td></tr></table></figure>

<p>现在我们使用下面的命令来重新初始化控制平面节点，但是最重要的一点是要<strong>使用 etcd 的数据目录</strong>，可以通过 <code>--ignore-preflight-errors=DirAvailable--var-lib-etcd</code> 标志来告诉 kubeadm 使用预先存在的 etcd 数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">➜ kubeadm init --config kubeadm.yaml --ignore-preflight-errors=DirAvailable--var-lib-etcd</span><br><span class="line">[init] Using Kubernetes version: v1.22.8</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING DirAvailable--var-lib-etcd]: /var/lib/etcd is not empty</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Using existing ca certificate authority</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [api.k8s.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master1] and IPs [10.96.0.1 192.168.0.106]</span><br><span class="line">[certs] Using existing apiserver-kubelet-client certificate and key on disk</span><br><span class="line">[certs] Using existing front-proxy-ca certificate authority</span><br><span class="line">[certs] Using existing front-proxy-client certificate and key on disk</span><br><span class="line">[certs] Using existing etcd/ca certificate authority</span><br><span class="line">[certs] Using existing etcd/server certificate and key on disk</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [localhost master1] and IPs [192.168.0.106 127.0.0.1 ::1]</span><br><span class="line">[certs] Using existing etcd/healthcheck-client certificate and key on disk</span><br><span class="line">[certs] Using existing apiserver-etcd-client certificate and key on disk</span><br><span class="line">[certs] Using the existing &quot;sa&quot; key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 12.003599 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.22&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node master1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]</span><br><span class="line">[mark-control-plane] Marking the node master1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: abcdef.0123456789abcdef</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, if you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  export KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.0.106:6443 --token abcdef.0123456789abcdef \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:27993cae9c76d18a1b82b800182c4c7ebc7a704ba1093400ed886f65e709ec04</span><br></pre></td></tr></table></figure>

<p>上面的操作和我们平时去初始化集群的时候几乎是一样的，唯一不同的地方是加了一个 <code>--ignore-preflight-errors=DirAvailable--var-lib-etcd</code> 参数，意思就是使用之前 etcd 的数据。然后我们可以验证下 APIServer 的 IP 地址是否变成了新的地址：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">cp: overwrite ‘/root/.kube/config’? y</span><br><span class="line">➜ kubectl cluster-info</span><br><span class="line">Kubernetes control plane is running at https://192.168.0.106:6443</span><br><span class="line">CoreDNS is running at https://192.168.0.106:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.</span><br></pre></td></tr></table></figure>

<p>对于 node 节点我们可以 reset 后重新加入到集群即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node节点操作</span></span><br><span class="line">➜ kubeadm reset</span><br></pre></td></tr></table></figure>

<p>重置后重新 join 集群即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node节点操作</span></span><br><span class="line">➜ kubeadm join 192.168.0.106:6443 --token abcdef.0123456789abcdef \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:27993cae9c76d18a1b82b800182c4c7ebc7a704ba1093400ed886f65e709ec04</span><br></pre></td></tr></table></figure>

<p>这种方式比上面的方式要简单很多。正常操作后集群也正常了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get nodes</span><br><span class="line">NAME      STATUS   ROLES                  AGE     VERSION</span><br><span class="line">master1   Ready    control-plane,master   48d     v1.22.8</span><br><span class="line">node1     Ready    &lt;none&gt;                 48d     v1.22.8</span><br><span class="line">node2     Ready    &lt;none&gt;                 4m50s   v1.22.8</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对于 Kubernetes 集群节点的 IP 地址最好使用静态 IP，避免 IP 变动对业务产生影响，如果不是静态 IP，也强烈建议增加一个自定义域名进行签名，这样当 IP 变化后还可以直接重新映射下这个域名即可，只需要在 kubeadm 配置文件中通过 <code>ClusterConfiguration</code> 配置 <code>apiServer.certSANs</code> 即可，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta3</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line">  <span class="attr">timeoutForControlPlane:</span> <span class="string">4m0s</span></span><br><span class="line">  <span class="attr">certSANs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">api.k8s.local</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">master1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.106</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="string">......</span></span><br></pre></td></tr></table></figure>

<p>将需要进行前面的地址加入到 <code>certSANs</code> 中，比如这里我们额外添加了一个 <code>api.k8s.local</code> 的地址，这样即使以后 IP 变了可以直接将这个域名映射到新的 IP 地址即可，同样如果你想通过外网访问 IP 访问你的集群，那么你也需要将你的外网 IP 地址加进来进行签名认证。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">六一</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80?/">http://example.com/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80?/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Hui's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%BF%90%E7%BB%B4/">运维</a><a class="post-meta__tags" href="/tags/k8s/">k8s</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/09/11/%E5%A4%9A%E5%80%BC%E8%BF%94%E5%9B%9E/" title="多值返回"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">多值返回</div></div><div class="info-2"><div class="info-item-1">多值返回 (Multiple Return Values) 是其最独特且强大的特性之一，它深刻影响了 Go 程序的风格、错误处理机制和 API 设计。它的“意义”在于解决了传统编程语言中一些常见的痛点，并带来了诸多便利。 作为一名后端开发专家，我将为您详细阐述 Go 语言多值返回的用途和其背后的设计哲学。 1. 核心理念：显式的错误处理这是 Go 语言多值返回最常见、也最重要的用途。 在许多其他语言中，错误处理通常依赖于异常（Exceptions）：当发生错误时，函数会抛出异常，中止当前执行流程，并将控制权转移到最近的 try-catch 块。虽然这在某些情况下很方便，但也可能导致：  隐藏的控制流： 调用者可能不知道某个函数会抛出异常，或者忘记捕获。 性能开销： 异常处理机制通常伴随着性能开销。 不明确的 API： 函数签名无法直接体现它可能抛出的所有异常。  Go 语言拒绝了异常机制，转而采用显式的、基于返回值的错误处理。一个函数通常会返回两个值：  结果值 (Result Value): 如果操作成功，这是您期望得到的值。 错误值 (Error Value): 如果操作失败...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E5%9F%BA%E4%BA%8E%E6%96%87%E4%BB%B6%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/" title="基于文件的服务发现"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">基于文件的服务发现</div></div><div class="info-2"><div class="info-item-1">基于文件的服务发现除了基于 Consul 的服务发现之外，Prometheus 也允许我们进行自定义的发现集成，可以通过 watch 一组本地文件来获取抓取目标以及标签信息，也就是我们常说的基于文件的服务发现方式。  基于文件的服务发现提供了一种更通用的方式来配置静态目标，并作为一个接口插入自定义服务发现机制。 它读取一组包含零个或多个 &lt;static_config&gt; 列表的文件，对所有定义的文件的变更通过磁盘监视被检测到并立即应用，文件可以以 YAML 或 JSON 格式提供。文件必须包含一个静态配置的列表: 123JSON json [ &#123; &quot;targets&quot;: [ &quot;&lt;host&gt;&quot;, ... ], &quot;labels&quot;: &#123; &quot;&lt;labelname&gt;&quot;: &quot;&lt;labelvalue&gt;&quot;, ... &#125; &#125;, ... ]YAML yaml - targets: [ - &#x27;&lt;host&...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/11/Crane/" title="Crane"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Crane</div></div><div class="info-2"><div class="info-item-1">CraneCrane 是一个基于 FinOps 的云资源分析与成本优化平台，它的愿景是在保证客户应用运行质量的前提下实现极致的降本。Crane 已经在腾讯内部自研业务实现了大规模落地，部署数百个 Kubernetes 集群、管控 CPU 核数达百万，在降本增效方面取得了阶段性成果。以腾讯某部门集群优化为例，通过使用 FinOps Crane，该部门在保障业务稳定的情况下，资源利用率提升了 3 倍；腾讯另一自研业务落地 Crane 后，在一个月内实现了总 CPU 规模 40 万核的节省量，相当于成本节约超 1000 万元&#x2F;月。   FinOps 是将 DevOps、财务和业务整合在一起的变革，其目标在于优化一个组织在云计算上的支出的财务规范和技术解决方案，即根据支出的历史记录和来自预期负载的信息，FinOps 可以在需要时预分配资源或估算成本。FinOps 可以称为“财务运维” ，或者更直白地称为“成本优化”，是将财务问责制引入云的 IT 支持，进行调整以优化质量和支出。  Crane 会通过下面 3 个方面来开启成本优化之旅：  成本展示: Kubernetes 资源(...</div></div></div></a><a class="pagination-related" href="/2025/09/11/Thanos-%E6%9E%B6%E6%9E%84/" title="Thanos-架构"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">Thanos-架构</div></div><div class="info-2"><div class="info-item-1">Thanos 架构Thanos 是一个基于 Prometheus 实现的监控方案，其主要设计目的是解决原生 Prometheus 上的痛点，并且做进一步的提升，主要的特性有：全局查询，高可用，动态拓展，长期存储。 架构Thanos 主要由如下几个特定功能的组件组成：  边车组件（Sidecar）：连接到 Prometheus，并把 Prometheus 暴露给查询网关（Querier&#x2F;Query），以供实时查询，并且可以上传 Prometheus 数据到云存储，以供长期保存 查询网关（Querier）：实现 Prometheus API 以聚合来自底层组件（如边车组件 Sidecar，或是存储网关 Store Gateway）的数据 存储网关（Store Gateway）：将云存储中的数据内容暴露出来 压缩器（Compactor）：将云存储中的数据进行压缩和下采样和保留 接收器（Receiver）：从 Prometheus 的远程写入 WAL 接收数据，将其暴露出去或者上传到云存储 规则组件（Ruler）：根据 Thanos 中的数据评估记录和警报规则 查询前端：实现 ...</div></div></div></a><a class="pagination-related" href="/2025/09/11/RBAC/" title="RBAC"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">RBAC</div></div><div class="info-2"><div class="info-item-1">RBAC 权限控制前面我们已经学习一些常用的资源对象的使用，我们知道对于资源对象的操作都是通过 APIServer 进行的，那么集群是怎样知道我们的请求就是合法的请求呢？这个就需要了解 Kubernetes 中另外一个非常重要的知识点了：RBAC（基于角色的权限控制）。 管理员可以通过 Kubernetes API 动态配置策略来启用RBAC，需要在 kube-apiserver 中添加参数--authorization-mode=RBAC，如果使用的 kubeadm 安装的集群那么是默认开启了 RBAC 的，可以通过查看 Master 节点上 apiserver 的静态 Pod 定义文件： 1234➜  ~ cat /etc/kubernetes/manifests/kube-apiserver.yaml...    - --authorization-mode=Node,RBAC...    如果是二进制的方式搭建的集群，添加这个参数过后，记得要重启 kube-apiserver 服务。 API 对象在学习 RBAC 之前，我们还需要再去理解下 Kubernetes 集群中的...</div></div></div></a><a class="pagination-related" href="/2025/09/11/k8s_10/" title="k8s_10"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s_10</div></div><div class="info-2"><div class="info-item-1">请描述 Kubernetes 中的 Helm 的作用，并解释其使用场景。Of course. Helm is an indispensable tool in the Kubernetes ecosystem, and understanding its role is crucial for anyone managing applications at scale. Let’s dive into its purpose and common use cases. 🤔 分析过程：此问题旨在考察对Kubernetes应用部署和管理工具——Helm的理解。一个优秀的回答不能仅仅说“它是一个包管理器”，而需要深入解释它解决了什么核心痛点。核心痛点是：Kubernetes原生YAML文件缺乏模板化、版本控制和依赖管理能力，导致在多环境、复杂应用场景下难以管理。因此，回答的重点应围绕Helm如何通过打包（Charts）、模板化（Templating）和版本发布（Releases）来解决这些问题。 💡 答案生成：1. 概念或定义Helm 是Kubernetes的官方包管理器。它可以被...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E8%81%9A%E5%90%88/" title="聚合"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">聚合</div></div><div class="info-2"><div class="info-item-1">聚合我们知道 Prometheus 的时间序列数据是多维数据模型，我们经常就有根据各个维度进行汇总的需求。 基于标签聚合例如我们想知道我们的 demo 服务每秒处理的请求数，那么可以将单个的速率相加就可以。 1sum(rate(demo_api_request_duration_seconds_count&#123;job=&quot;demo&quot;&#125;[5m]))    可以得到如下所示的结果：  但是我们可以看到绘制出来的图形没有保留任何标签维度，一般来说可能我们希望保留一些维度，例如，我们可能更希望计算每个 instance 和 path 的变化率，但并不关心单个 method 或者 status 的结果，这个时候我们可以在 sum() 聚合器中添加一个 without() 的修饰符： 1sum without(method, status) (rate(demo_api_request_duration_seconds_count&#123;job=&quot;demo&quot;&#125;[5m]))    上面的查询语句相当于用 by() 修饰符来保留...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C/" title="集合操作"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">集合操作</div></div><div class="info-2"><div class="info-item-1">集合操作有的时候我们需要过滤或将一组时间序列与另一组时间序列进行合并，Prometheus 提供了 3 个在瞬时向量之间操作的集合运算符。  and（集合交集）：比如对较高错误率触发报警，但是只有当对应的总错误率超过某个阈值的时候才会触发报警 or（集合并集）：对序列进行并集计算 unless（除非）：比如要对磁盘空间不足进行告警，除非它是只读文件系统。   与算术和过滤二元运算符类似，这些集合运算符会尝试根据相同的标签集在左侧和右侧之间查找来匹配序列，除非你提供 on() 或 ignoring() 修饰符来指定应该如何找到匹配。  注意：与算术和过滤二进制运算符相比，集合运算符没有 group_left() 或 group_right() 修饰符，因为集合运算符总是进行多对多的匹配，也就是说，它们总是允许任何一边的匹配序列与另一边的多个序列相匹配。  对于 and 运算符，如果找到一个匹配的，左边的序列就会成为输出结果的一部分，如果右边没有匹配的序列，则不会输出任何结果。 例如我们想筛选出第 90 个百分位延迟高于 50ms 的所有 HTTP 端点，但只针对每秒收到多个请求的维...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">六一</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9-Kubernetes-%E8%8A%82%E7%82%B9-IP-%E5%9C%B0%E5%9D%80"><span class="toc-number">1.</span> <span class="toc-text">如何修改 Kubernetes 节点 IP 地址?</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.</span> <span class="toc-text">环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#master-%E8%8A%82%E7%82%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">master 节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#node-%E8%8A%82%E7%82%B9"><span class="toc-number">1.2.2.</span> <span class="toc-text">node 节点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E6%93%8D%E4%BD%9C"><span class="toc-number">1.3.</span> <span class="toc-text">推荐操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/27/hexo-beautify/" title="Hexo 博客美化笔记：从零搭建高颜值技术博客">Hexo 博客美化笔记：从零搭建高颜值技术博客</a><time datetime="2026-02-27T04:00:00.000Z" title="发表于 2026-02-27 12:00:00">2026-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/Jenkens-Blue%20Ocean%20%E6%8F%92%E4%BB%B6/" title="Jenkins Blue Ocean">Jenkins Blue Ocean</a><time datetime="2025-09-11T12:42:57.000Z" title="发表于 2025-09-11 20:42:57">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/jenkins-jenkinsfile/" title="Jenkins Jenkinsfile">Jenkins Jenkinsfile</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-trap/" title="Linux-trap">Linux-trap</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-%E6%95%B0%E7%BB%84/" title="Shell-数组">Shell-数组</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 六一</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body></html>