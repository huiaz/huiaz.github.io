<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HPA | Hui's Blog</title><meta name="author" content="六一"><meta name="copyright" content="六一"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="HPA 控制器在前面的学习中我们使用了一个 kubectl scale 命令可以来实现 Pod 的扩缩容功能，但是这个是完全手动操作的，要应对线上的各种复杂情况，我们需要能够做到自动化去感知业务，来自动进行扩缩容。为此，Kubernetes 也为我们提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称 HPA，HPA 通过监控分析一些控制器控">
<meta property="og:type" content="article">
<meta property="og:title" content="HPA">
<meta property="og:url" content="https://huiaz.github.io/2025/09/11/HPA/index.html">
<meta property="og:site_name" content="Hui&#39;s Blog">
<meta property="og:description" content="HPA 控制器在前面的学习中我们使用了一个 kubectl scale 命令可以来实现 Pod 的扩缩容功能，但是这个是完全手动操作的，要应对线上的各种复杂情况，我们需要能够做到自动化去感知业务，来自动进行扩缩容。为此，Kubernetes 也为我们提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称 HPA，HPA 通过监控分析一些控制器控">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://huiaz.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-09-11T12:32:34.000Z">
<meta property="article:modified_time" content="2025-09-11T13:39:09.600Z">
<meta property="article:author" content="六一">
<meta property="article:tag" content="运维">
<meta property="article:tag" content="k8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://huiaz.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "HPA",
  "url": "https://huiaz.github.io/2025/09/11/HPA/",
  "image": "https://huiaz.github.io/img/butterfly-icon.png",
  "datePublished": "2025-09-11T12:32:34.000Z",
  "dateModified": "2025-09-11T13:39:09.600Z",
  "author": [
    {
      "@type": "Person",
      "name": "六一",
      "url": "https://huiaz.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://huiaz.github.io/2025/09/11/HPA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HPA',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Hui's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">HPA</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">HPA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-11T12:32:34.000Z" title="发表于 2025-09-11 20:32:34">2025-09-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-11T13:39:09.600Z" title="更新于 2025-09-11 21:39:09">2025-09-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/">k8s</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/k8s/%E6%8E%A7%E5%88%B6%E5%99%A8/">控制器</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="HPA-控制器"><a href="#HPA-控制器" class="headerlink" title="HPA 控制器"></a>HPA 控制器</h1><p>在前面的学习中我们使用了一个 <code>kubectl scale</code> 命令可以来实现 Pod 的扩缩容功能，但是这个是完全手动操作的，要应对线上的各种复杂情况，我们需要能够做到自动化去感知业务，来自动进行扩缩容。为此，Kubernetes 也为我们提供了这样的一个资源对象：<code>Horizontal Pod Autoscaling（Pod 水平自动伸缩）</code>，简称 <code>HPA</code>，HPA 通过监控分析一些控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量，这是 HPA 最基本的原理：</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/3v1m3o.png" alt="HPA"></p>
<p>我们可以简单的通过 <code>kubectl autoscale</code> 命令来创建一个 HPA 资源对象，<code>HPA Controller</code> 默认<code>30s</code>轮询一次（可通过 <code>kube-controller-manager</code> 的<code>--horizontal-pod-autoscaler-sync-period</code> 参数进行设置），查询指定的资源中的 Pod 资源使用率，并且与创建时设定的值和指标做对比，从而实现自动伸缩的功能。</p>
<h2 id="Metrics-Server"><a href="#Metrics-Server" class="headerlink" title="Metrics Server"></a>Metrics Server</h2><p>在 HPA 的第一个版本中，我们需要 <code>Heapster</code> 提供 CPU 和内存指标，在 HPA v2 过后就需要安装 Metrcis Server 了，<code>Metrics Server</code> 可以通过标准的 Kubernetes API 把监控数据暴露出来，有了 <code>Metrics Server</code> 之后，我们就完全可以通过标准的 Kubernetes API 来访问我们想要获取的监控数据了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://10.96.0.1/apis/metrics.k8s.io/v1beta1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</span><br></pre></td></tr></table></figure>



<p>比如当我们访问上面的 API 的时候，我们就可以获取到该 Pod 的资源数据，这些数据其实是来自于 kubelet 的 <code>Summary API</code> 采集而来的。不过需要说明的是我们这里可以通过标准的 API 来获取资源监控数据，并不是因为 <code>Metrics Server</code> 就是 APIServer 的一部分，而是通过 Kubernetes 提供的 <code>Aggregator</code> 汇聚插件来实现的，是独立于 APIServer 之外运行的。</p>
<p><img src="https://mudutestmenu.mudu.tv/upload/oyooqu.jpg" alt="HAP Metrics Server"></p>
<h3 id="聚合-API"><a href="#聚合-API" class="headerlink" title="聚合 API"></a>聚合 API</h3><p><code>Aggregator</code> 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，我们把自己的服务运行在 Kubernetes 集群里面，然后 Kubernetes 的 <code>Aggregator</code> 通过 Service 名称就可以转发到我们自己写的 Service 里面去了。这样这个聚合层就带来了很多好处：</p>
<ul>
<li>增加了 API 的扩展性，开发人员可以编写自己的 API 服务来暴露他们想要的 API。</li>
<li>丰富了 API，核心 kubernetes 团队阻止了很多新的 API 提案，通过允许开发人员将他们的 API 作为单独的服务公开，这样就无须社区繁杂的审查了。</li>
<li>开发分阶段实验性 API，新的 API 可以在单独的聚合服务中开发，当它稳定之后，在合并会 APIServer 就很容易了。</li>
<li>确保新 API 遵循 Kubernetes 约定，如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这样很可能造成社区成员和社区约定不一致。</li>
</ul>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>所以现在我们要使用 HPA，就需要在集群中安装 <code>Metrics Server</code> 服务，要安装 <code>Metrics Server</code> 就需要开启 <code>Aggregator</code>，因为 <code>Metrics Server</code> 就是通过该代理进行扩展的，不过我们集群是通过 Kubeadm 搭建的，默认已经开启了，如果是二进制方式安装的集群，需要单独配置 kube-apsierver 添加如下所示的参数：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--requestheader-client-ca-file=&lt;path</span> <span class="string">to</span> <span class="string">aggregator</span> <span class="string">CA</span> <span class="string">cert&gt;</span></span><br><span class="line"><span class="string">--requestheader-allowed-names=aggregator</span></span><br><span class="line"><span class="string">--requestheader-extra-headers-prefix=X-Remote-Extra-</span></span><br><span class="line"><span class="string">--requestheader-group-headers=X-Remote-Group</span></span><br><span class="line"><span class="string">--requestheader-username-headers=X-Remote-User</span></span><br><span class="line"><span class="string">--proxy-client-cert-file=&lt;path</span> <span class="string">to</span> <span class="string">aggregator</span> <span class="string">proxy</span> <span class="string">cert&gt;</span></span><br><span class="line"><span class="string">--proxy-client-key-file=&lt;path</span> <span class="string">to</span> <span class="string">aggregator</span> <span class="string">proxy</span> <span class="string">key&gt;</span></span><br></pre></td></tr></table></figure>



<p>如果 <code>kube-proxy</code> 没有和 APIServer 运行在同一台主机上，那么需要确保启用了如下 kube-apsierver 的参数：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">--enable-aggregator-routing=true</span></span><br></pre></td></tr></table></figure>



<p>对于这些证书的生成方式，我们可以查看官方文档：<a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md%E3%80%82">https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md。</a></p>
<p><code>Aggregator</code> 聚合层启动完成后，就可以来安装 <code>Metrics Server</code> 了，我们可以获取该仓库的官方安装资源清单：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">官方仓库地址：https://github.com/kubernetes-sigs/metrics-server</span></span><br><span class="line">➜  ~ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.1/components.yaml</span><br></pre></td></tr></table></figure>



<p>在部署之前，修改 <code>components.yaml</code> 的镜像地址为：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hostNetwork:</span> <span class="literal">true</span> <span class="comment"># 使用hostNetwork模式</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">cnych/metrics-server:v0.5.1</span></span><br></pre></td></tr></table></figure>



<p>等部署完成后，可以查看 Pod 日志是否正常：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f components.yaml</span><br><span class="line">➜  ~ kubectl get pods -n kube-system -l k8s-app=metrics-server</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-6f667d74b6-6c9ps   0/1     Running   0          7m52s</span><br><span class="line">➜  ~ manifests kubectl logs -f metrics-server-6f667d74b6-6c9ps -n kube-system</span><br><span class="line">I1115 10:06:02.381541       1 serving.go:341] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)</span><br><span class="line">E1115 10:06:02.735837       1 scraper.go:139] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.31.31:10250/stats/summary?only_cpu_and_memory=true\&quot;: x509: cannot validate certificate for 192.168.31.31 because it doesn&#x27;t contain any IP SANs&quot; node=&quot;master1&quot;</span><br><span class="line">E1115 10:06:02.744967       1 scraper.go:139] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.31.108:10250/stats/summary?only_cpu_and_memory=true\&quot;: x509: cannot validate certificate for 192.168.31.108 because it doesn&#x27;t contain any IP SANs&quot; node=&quot;node1&quot;</span><br><span class="line">I1115 10:06:02.751391       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController</span><br><span class="line">I1115 10:06:02.751410       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController</span><br><span class="line">I1115 10:06:02.751413       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file</span><br><span class="line">I1115 10:06:02.751397       1 configmap_cafile_content.go:202] Starting client-ca::kube-system::extension-apiserver-authentication::client-ca-file</span><br><span class="line">I1115 10:06:02.751423       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file</span><br><span class="line">I1115 10:06:02.751424       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file</span><br><span class="line">I1115 10:06:02.751473       1 dynamic_serving_content.go:130] Starting serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key</span><br><span class="line">I1115 10:06:02.751822       1 secure_serving.go:202] Serving securely on [::]:443</span><br><span class="line">I1115 10:06:02.751896       1 tlsconfig.go:240] Starting DynamicServingCertificateController</span><br><span class="line">E1115 10:06:02.756987       1 scraper.go:139] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.31.46:10250/stats/summary?only_cpu_and_memory=true\&quot;: x509: cannot validate certificate for 192.168.31.46 because it doesn&#x27;t contain any IP SANs&quot; node=&quot;node2&quot;</span><br><span class="line">I1115 10:06:02.851642       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file</span><br><span class="line">I1115 10:06:02.851739       1 shared_informer.go:247] Caches are synced for RequestHeaderAuthRequestController</span><br><span class="line">I1115 10:06:02.851748       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file</span><br><span class="line">E1115 10:06:17.742350       1 scraper.go:139] &quot;Failed to scrape node&quot; err=&quot;Get \&quot;https://192.168.31.108:10250/stats/summary?only_cpu_and_memory=true\&quot;: x509: cannot validate certificate for 192.168.31.108 because it doesn&#x27;t contain any IP SANs&quot; node=&quot;node1&quot;</span><br><span class="line">......</span><br></pre></td></tr></table></figure>



<p>因为部署集群的时候，CA 证书并没有把各个节点的 IP 签上去，所以这里 <code>Metrics Server</code> 通过 IP 去请求时，提示签的证书没有对应的 IP（错误：<code>x509: cannot validate certificate for 192.168.31.108 because it doesn&#39;t contain any IP SANs</code>），我们可以添加一个<code>--kubelet-insecure-tls</code>参数跳过证书校验：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">args:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--cert-dir=/tmp</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--secure-port=443</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--kubelet-insecure-tls</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--kubelet-preferred-address-types=InternalIP</span></span><br></pre></td></tr></table></figure>



<p>然后再重新安装即可成功！可以通过如下命令来验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f components.yaml</span><br><span class="line">➜  ~ kubectl get pods -n kube-system -l k8s-app=metrics-server</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">metrics-server-85499dc4f5-mgpcb   1/1     Running   0          32s</span><br><span class="line">➜  ~ kubectl logs -f metrics-server-85499dc4f5-mgpcb -n kube-system</span><br><span class="line">I1115 10:14:19.401808       1 serving.go:341] Generated self-signed cert (/tmp/apiserver.crt, /tmp/apiserver.key)</span><br><span class="line">I1115 10:14:19.840290       1 secure_serving.go:202] Serving securely on [::]:443</span><br><span class="line">I1115 10:14:19.840395       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController</span><br><span class="line">I1115 10:14:19.840403       1 shared_informer.go:240] Waiting for caches to sync for RequestHeaderAuthRequestController</span><br><span class="line">I1115 10:14:19.840411       1 dynamic_serving_content.go:130] Starting serving-cert::/tmp/apiserver.crt::/tmp/apiserver.key</span><br><span class="line">I1115 10:14:19.840438       1 tlsconfig.go:240] Starting DynamicServingCertificateController</span><br><span class="line">......</span><br><span class="line">➜  ~ kubectl get apiservice | grep metrics</span><br><span class="line">v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        10m</span><br><span class="line">➜  ~ kubectl get --raw &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</span><br><span class="line">&#123;&quot;kind&quot;:&quot;NodeMetricsList&quot;,&quot;apiVersion&quot;:&quot;metrics.k8s.io/v1beta1&quot;,&quot;metadata&quot;:&#123;&#125;,&quot;items&quot;:[&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;master1&quot;,&quot;creationTimestamp&quot;:&quot;2021-11-15T10:15:38Z&quot;,&quot;labels&quot;:&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;kubernetes.io/hostname&quot;:&quot;master1&quot;,&quot;kubernetes.io/os&quot;:&quot;linux&quot;,&quot;node-role.kubernetes.io/control-plane&quot;:&quot;&quot;,&quot;node-role.kubernetes.io/master&quot;:&quot;&quot;,&quot;node.kubernetes.io/exclude-from-external-load-balancers&quot;:&quot;&quot;&#125;&#125;,&quot;timestamp&quot;:&quot;2021-11-15T10:15:33Z&quot;,&quot;window&quot;:&quot;20s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;132348072n&quot;,&quot;memory&quot;:&quot;813200Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;node1&quot;,&quot;creationTimestamp&quot;:&quot;2021-11-15T10:15:38Z&quot;,&quot;labels&quot;:&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;kubernetes.io/hostname&quot;:&quot;node1&quot;,&quot;kubernetes.io/os&quot;:&quot;linux&quot;&#125;&#125;,&quot;timestamp&quot;:&quot;2021-11-15T10:15:32Z&quot;,&quot;window&quot;:&quot;20s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;60153492n&quot;,&quot;memory&quot;:&quot;520628Ki&quot;&#125;&#125;,&#123;&quot;metadata&quot;:&#123;&quot;name&quot;:&quot;node2&quot;,&quot;creationTimestamp&quot;:&quot;2021-11-15T10:15:38Z&quot;,&quot;labels&quot;:&#123;&quot;beta.kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;beta.kubernetes.io/os&quot;:&quot;linux&quot;,&quot;kubernetes.io/arch&quot;:&quot;amd64&quot;,&quot;kubernetes.io/hostname&quot;:&quot;node2&quot;,&quot;kubernetes.io/os&quot;:&quot;linux&quot;&#125;&#125;,&quot;timestamp&quot;:&quot;2021-11-15T10:15:29Z&quot;,&quot;window&quot;:&quot;20s&quot;,&quot;usage&quot;:&#123;&quot;cpu&quot;:&quot;81697469n&quot;,&quot;memory&quot;:&quot;557208Ki&quot;&#125;&#125;]&#125;</span><br><span class="line">➜  ~ kubectl top nodes</span><br><span class="line">NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">master1   115m         5%     794Mi           21%</span><br><span class="line">node1     58m          1%     505Mi           6%</span><br><span class="line">node2     55m          1%     545Mi           7%</span><br></pre></td></tr></table></figure>



<p>现在我们可以通过 <code>kubectl top</code> 命令来获取到资源数据了，证明 <code>Metrics Server</code> 已经安装成功了。</p>
<h2 id="HPA-对象"><a href="#HPA-对象" class="headerlink" title="HPA 对象"></a>HPA 对象</h2><p>现在我们用 Deployment 来创建一个 Nginx Pod，然后利用 <code>HPA</code> 来进行自动扩缩容。资源清单如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hpa-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpa-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>



<p>然后直接创建 Deployment，注意一定先把之前创建的具有 <code>app=nginx</code> 的 Pod 先清除掉：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f hpa-demo.yaml</span><br><span class="line">deployment.apps/hpa-demo created</span><br><span class="line">➜  ~ kubectl get pods -l app=nginx</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">hpa-demo-7848d4b86f-khndb   1/1     Running   0          56s</span><br></pre></td></tr></table></figure>



<p>现在我们来创建一个 <code>HPA</code> 资源对象，可以使用<code>kubectl autoscale</code>命令来创建：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10</span><br><span class="line">horizontalpodautoscaler.autoscaling/hpa-demo autoscaled</span><br><span class="line">➜  ~ kubectl get hpa</span><br><span class="line">NAME       REFERENCE             TARGETS         MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-demo   Deployment/hpa-demo   &lt;unknown&gt;/10%   1         10        0          6s</span><br></pre></td></tr></table></figure>



<p>此命令创建了一个关联资源 hpa-demo 的 HPA，最小的 Pod 副本数为 1，最大为 10。HPA 会根据设定的 cpu 使用率（10%）动态的增加或者减少 Pod 数量。</p>
<p>当然我们依然还是可以通过创建 YAML 文件的形式来创建 HPA 资源对象。如果我们不知道怎么编写的话，可以查看上面命令行创建的 HPA 的 YAML 文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get hpa hpa-demo -o yaml</span><br><span class="line">apiVersion: autoscaling/v1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    autoscaling.alpha.kubernetes.io/conditions: &#x27;[&#123;&quot;type&quot;:&quot;AbleToScale&quot;,&quot;status&quot;:&quot;True&quot;,&quot;lastTransitionTime&quot;:&quot;2021-11-15T10:19:06Z&quot;,&quot;reason&quot;:&quot;SucceededGetScale&quot;,&quot;message&quot;:&quot;the</span><br><span class="line">      HPA controller was able to get the target&#x27;&#x27;s current scale&quot;&#125;,&#123;&quot;type&quot;:&quot;ScalingActive&quot;,&quot;status&quot;:&quot;False&quot;,&quot;lastTransitionTime&quot;:&quot;2021-11-15T10:19:06Z&quot;,&quot;reason&quot;:&quot;FailedGetResourceMetric&quot;,&quot;message&quot;:&quot;the</span><br><span class="line">      HPA was unable to compute the replica count: failed to get cpu utilization:</span><br><span class="line">      missing request for cpu&quot;&#125;]&#x27;</span><br><span class="line">  creationTimestamp: &quot;2021-11-15T10:18:51Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: autoscaling/v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:spec:</span><br><span class="line">        f:maxReplicas: &#123;&#125;</span><br><span class="line">        f:minReplicas: &#123;&#125;</span><br><span class="line">        f:scaleTargetRef: &#123;&#125;</span><br><span class="line">        f:targetCPUUtilizationPercentage: &#123;&#125;</span><br><span class="line">    manager: kubectl</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2021-11-15T10:18:51Z&quot;</span><br><span class="line">  - apiVersion: autoscaling/v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:metadata:</span><br><span class="line">        f:annotations:</span><br><span class="line">          .: &#123;&#125;</span><br><span class="line">          f:autoscaling.alpha.kubernetes.io/conditions: &#123;&#125;</span><br><span class="line">      f:status:</span><br><span class="line">        f:currentReplicas: &#123;&#125;</span><br><span class="line">    manager: kube-controller-manager</span><br><span class="line">    operation: Update</span><br><span class="line">    subresource: status</span><br><span class="line">    time: &quot;2021-11-15T10:19:06Z&quot;</span><br><span class="line">  name: hpa-demo</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;631809&quot;</span><br><span class="line">  uid: 34b91709-d003-4039-9cf0-05bb3fa4da73</span><br><span class="line">spec:</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: hpa-demo</span><br><span class="line">  targetCPUUtilizationPercentage: 10</span><br><span class="line">status:</span><br><span class="line">  currentReplicas: 1</span><br><span class="line">  desiredReplicas: 0</span><br></pre></td></tr></table></figure>



<p>然后我们可以根据上面的 YAML 文件就可以自己来创建一个基于 YAML 的 HPA 描述文件了。但是我们发现上面信息里面出现了一些 Fail 信息，我们来查看下这个 HPA 对象的信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl describe hpa hpa-demo</span><br><span class="line">Name:                                                  hpa-demo</span><br><span class="line">Namespace:                                             default</span><br><span class="line">Labels:                                                &lt;none&gt;</span><br><span class="line">Annotations:                                           &lt;none&gt;</span><br><span class="line">CreationTimestamp:                                     Mon, 15 Nov 2021 18:18:51 +0800</span><br><span class="line">Reference:                                             Deployment/hpa-demo</span><br><span class="line">Metrics:                                               ( current / target )</span><br><span class="line">  resource cpu on pods  (as a percentage of request):  &lt;unknown&gt; / 10%</span><br><span class="line">Min replicas:                                          1</span><br><span class="line">Max replicas:                                          10</span><br><span class="line">Deployment pods:                                       1 current / 0 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason                   Message</span><br><span class="line">  ----           ------  ------                   -------</span><br><span class="line">  AbleToScale    True    SucceededGetScale        the HPA controller was able to get the target&#x27;s current scale</span><br><span class="line">  ScalingActive  False   FailedGetResourceMetric  the HPA was unable to compute the replica count: failed to get cpu utilization: missing request for cpu</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                        Age               From                       Message</span><br><span class="line">  ----     ------                        ----              ----                       -------</span><br><span class="line">  Warning  FailedGetResourceMetric       1s (x3 over 31s)  horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu</span><br><span class="line">  Warning  FailedComputeMetricsReplicas  1s (x3 over 31s)  horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get cpu utilization: missing request for cpu</span><br></pre></td></tr></table></figure>



<p>我们可以看到上面的事件信息里面出现了 <code>failed to get cpu utilization: missing request for cpu</code> 这样的错误信息。这是因为我们上面创建的 Pod 对象<strong>没有添加 request 资源</strong>声明，这样导致 HPA 读取不到 CPU 指标信息，所以如果要想让 HPA 生效，对应的 Pod 资源必须添加 requests 资源声明，更新我们的资源清单文件：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpa-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">50Mi</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">50m</span></span><br></pre></td></tr></table></figure>



<p>然后重新更新 Deployment，重新创建 HPA 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f hpa-demo.yaml</span><br><span class="line">deployment.apps/hpa-demo configured</span><br><span class="line">➜  ~ kubectl get pods -o wide -l app=nginx</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">hpa-demo-6b4467b546-h489x   1/1     Running   0          18s   10.244.1.11   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">➜  ~ kubectl delete hpa hpa-demo</span><br><span class="line">horizontalpodautoscaler.autoscaling &quot;hpa-demo&quot; deleted</span><br><span class="line">➜  ~ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10</span><br><span class="line">horizontalpodautoscaler.autoscaling/hpa-demo autoscaled</span><br><span class="line">➜  ~ kubectl describe hpa hpa-demo</span><br><span class="line">Name:                                                  hpa-demo</span><br><span class="line">Namespace:                                             default</span><br><span class="line">Labels:                                                &lt;none&gt;</span><br><span class="line">Annotations:                                           &lt;none&gt;</span><br><span class="line">CreationTimestamp:                                     Mon, 15 Nov 2021 18:21:12 +0800</span><br><span class="line">Reference:                                             Deployment/hpa-demo</span><br><span class="line">Metrics:                                               ( current / target )</span><br><span class="line">  resource cpu on pods  (as a percentage of request):  0% (0) / 10%</span><br><span class="line">Min replicas:                                          1</span><br><span class="line">Max replicas:                                          10</span><br><span class="line">Deployment pods:                                       1 current / 1 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason               Message</span><br><span class="line">  ----            ------  ------               -------</span><br><span class="line">  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation</span><br><span class="line">  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)</span><br><span class="line">  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range</span><br><span class="line">Events:           &lt;none&gt;</span><br><span class="line">➜  ~ kubectl get hpa</span><br><span class="line">NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-demo   Deployment/hpa-demo   0%/10%    1         10        1          35s</span><br></pre></td></tr></table></figure>



<p>现在可以看到 HPA 资源对象已经正常了，现在我们来增大负载进行测试，我们来创建一个 busybox 的 Pod，并且循环访问上面创建的 Pod：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl run -it --image busybox test-hpa --restart=Never --rm /bin/sh</span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line">/ # while true; do wget -q -O- http://10.244.1.11; done</span><br></pre></td></tr></table></figure>



<p>然后观察 Pod 列表，可以看到，HPA 已经开始工作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get hpa</span><br><span class="line">NAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-demo   Deployment/hpa-demo   310%/10%   1         10        1          105s</span><br><span class="line">➜  ~ kubectl get pods -l app=nginx --watch</span><br><span class="line">NAME                        READY   STATUS              RESTARTS   AGE</span><br><span class="line">hpa-demo-6b4467b546-h489x   1/1     Running             0          2m25s</span><br><span class="line">hpa-demo-6b4467b546-pg4fz   0/1     ContainerCreating   0          9s</span><br><span class="line">hpa-demo-6b4467b546-qrwv5   0/1     ContainerCreating   0          9s</span><br><span class="line">hpa-demo-6b4467b546-s4vdz   0/1     ContainerCreating   0          9s</span><br></pre></td></tr></table></figure>



<p>我们可以看到已经自动拉起了很多新的 Pod，最后会定格在了我们上面设置的 10 个 Pod，同时查看资源 hpa-demo 的副本数量，副本数量已经从原来的 1 变成了 10 个：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get deployment hpa-demo</span><br><span class="line">NAME       READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">hpa-demo   10/10    10           10           2m56s</span><br></pre></td></tr></table></figure>



<p>查看 HPA 资源的对象了解工作过程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl describe hpa hpa-demo</span><br><span class="line">Name:                                                  hpa-demo</span><br><span class="line">Namespace:                                             default</span><br><span class="line">Labels:                                                &lt;none&gt;</span><br><span class="line">Annotations:                                           &lt;none&gt;</span><br><span class="line">CreationTimestamp:                                     Mon, 15 Nov 2021 18:21:12 +0800</span><br><span class="line">Reference:                                             Deployment/hpa-demo</span><br><span class="line">Metrics:                                               ( current / target )</span><br><span class="line">  resource cpu on pods  (as a percentage of request):  110% (55m) / 10%</span><br><span class="line">Min replicas:                                          1</span><br><span class="line">Max replicas:                                          10</span><br><span class="line">Deployment pods:                                       10 current / 10 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason               Message</span><br><span class="line">  ----            ------  ------               -------</span><br><span class="line">  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation</span><br><span class="line">  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)</span><br><span class="line">  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                       Message</span><br><span class="line">  ----    ------             ----  ----                       -------</span><br><span class="line">  Normal  SuccessfulRescale  67s   horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target</span><br><span class="line">  Normal  SuccessfulRescale  52s   horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target</span><br><span class="line">  Normal  SuccessfulRescale  37s   horizontal-pod-autoscaler  New size: 10; reason: cpu resource utilization (percentage of request) above target</span><br></pre></td></tr></table></figure>



<p>同样的这个时候我们来关掉 busybox 来减少负载，然后等待一段时间观察下 HPA 和 Deployment 对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ k8strain3 kubectl get hpa</span><br><span class="line">NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-demo   Deployment/hpa-demo   0%/10%    1         10        10         3m46s</span><br><span class="line">➜  ~ kubectl get deployment hpa-demo</span><br><span class="line">NAME       READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">hpa-demo   1/1     1            1           24m</span><br></pre></td></tr></table></figure>



<p>!!! info “缩放间隙”</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从 Kubernetes `v1.12` 版本开始我们可以通过设置 `kube-controller-manager` 组件的`--horizontal-pod-autoscaler-downscale-stabilization` 参数来设置一个持续时间，用于指定在当前操作完成后，`HPA` 必须等待多长时间才能执行另一次缩放操作。默认为5分钟，也就是默认需要等待5分钟后才会开始自动缩放。</span><br></pre></td></tr></table></figure>



<p>可以看到副本数量已经由 10 变为 1，当前我们只是演示了 CPU 使用率这一个指标，在后面的课程中我们还会学习到根据自定义的监控指标来自动对 Pod 进行扩缩容。</p>
<h2 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h2><p>要使用基于内存或者自定义指标进行扩缩容（现在的版本都必须依赖 metrics-server 这个项目）。现在我们再用 Deployment 来创建一个 Nginx Pod，然后利用 HPA 来进行自动扩缩容。资源清单如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hpa-mem-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpa-mem-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">increase-mem-script</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">increase-mem-config</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">increase-mem-script</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/script</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">50Mi</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">50m</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">privileged:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>



<p>这里和前面普通的应用有一些区别，我们将一个名为 <code>increase-mem-config</code> 的 ConfigMap 资源对象挂载到了容器中，该配置文件是用于后面增加容器内存占用的脚本，配置文件如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># increase-mem-cm.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">increase-mem-config</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">increase-mem.sh:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    #!/bin/bash</span></span><br><span class="line"><span class="string">    mkdir /tmp/memory</span></span><br><span class="line"><span class="string">    mount -t tmpfs -o size=40M tmpfs /tmp/memory</span></span><br><span class="line"><span class="string">    dd if=/dev/zero of=/tmp/memory/block</span></span><br><span class="line"><span class="string">    sleep 60</span></span><br><span class="line"><span class="string">    rm /tmp/memory/block</span></span><br><span class="line"><span class="string">    umount /tmp/memory</span></span><br><span class="line"><span class="string">    rmdir /tmp/memory</span></span><br></pre></td></tr></table></figure>



<p>由于这里增加内存的脚本需要使用到 <code>mount</code> 命令，这需要声明为特权模式，所以我们添加了 <code>securityContext.privileged=true</code> 这个配置。现在我们直接创建上面的资源对象即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f increase-mem-cm.yaml</span><br><span class="line">➜  ~ kubectl apply -f hpa-mem-demo.yaml</span><br><span class="line">➜  ~ kubectl get pods -l app=nginx</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">hpa-mem-demo-74675cc6c9-sqz2l   1/1     Running   0          17s</span><br></pre></td></tr></table></figure>



<p>然后需要创建一个基于内存的 HPA 资源对象：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hpa-mem.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling/v2beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HorizontalPodAutoscaler</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">hpa-mem-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">maxReplicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">minReplicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hpa-mem-demo</span></span><br><span class="line">  <span class="attr">metrics:</span> <span class="comment"># 指定内存的一个配置</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Resource</span></span><br><span class="line">      <span class="attr">resource:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">memory</span></span><br><span class="line">        <span class="attr">targetAverageUtilization:</span> <span class="number">30</span></span><br></pre></td></tr></table></figure>



<p>要注意这里使用的 <code>apiVersion</code> 是 <code>autoscaling/v2beta1</code>，然后 <code>metrics</code> 属性里面指定的是内存的配置，直接创建上面的资源对象即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl apply -f hpa-mem.yaml</span><br><span class="line">horizontalpodautoscaler.autoscaling/hpa-mem-demo created</span><br><span class="line">➜  ~ kubectl get hpa</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-mem-demo   Deployment/hpa-mem-demo   6%/30%    1         5         1          32s</span><br></pre></td></tr></table></figure>



<p>到这里证明 HPA 资源对象已经部署成功了，接下来我们对应用进行压测，将内存压上去，直接执行上面我们挂载到容器中的 <code>increase-mem.sh</code> 脚本即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl exec -it hpa-mem-demo-74675cc6c9-sqz2l -- /bin/bash</span><br><span class="line">root@hpa-mem-demo-74675cc6c9-sqz2l:/# ls /etc/script/</span><br><span class="line">increase-mem.sh</span><br><span class="line">root@hpa-mem-demo-74675cc6c9-sqz2l:/# source /etc/script/increase-mem.sh</span><br><span class="line">dd: writing to &#x27;/tmp/memory/block&#x27;: No space left on device</span><br><span class="line">81921+0 records in</span><br><span class="line">81920+0 records out</span><br><span class="line">41943040 bytes (42 MB, 40 MiB) copied, 0.0908717 s, 462 MB/s</span><br></pre></td></tr></table></figure>



<p>然后打开另外一个终端观察 HPA 资源对象的变化情况：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get hpa -w</span><br><span class="line">NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">hpa-mem-demo   Deployment/hpa-mem-demo   87%/30%   1         5         3          90s</span><br><span class="line">➜  ~ kubectl describe hpa hpa-mem-demo</span><br><span class="line">Name:                                                     hpa-mem-demo</span><br><span class="line">Namespace:                                                default</span><br><span class="line">Labels:                                                   &lt;none&gt;</span><br><span class="line">Annotations:                                              kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                                                            &#123;&quot;apiVersion&quot;:&quot;autoscaling/v2beta1&quot;,&quot;kind&quot;:&quot;HorizontalPodAutoscaler&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;hpa-mem-demo&quot;,&quot;namespace&quot;:&quot;defau...</span><br><span class="line">CreationTimestamp:                                        Mon, 15 Nov 2021 18:40:37 +0800</span><br><span class="line">Reference:                                                Deployment/hpa-mem-demo</span><br><span class="line">Metrics:                                                  ( current / target )</span><br><span class="line">  resource memory on pods  (as a percentage of request):  87% (45752320) / 30%</span><br><span class="line">Min replicas:                                             1</span><br><span class="line">Max replicas:                                             5</span><br><span class="line">Deployment pods:                                          3 current / 3 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason              Message</span><br><span class="line">  ----            ------  ------              -------</span><br><span class="line">  AbleToScale     True    ReadyForNewScale    recommended size matches current size</span><br><span class="line">  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource utilization (percentage of request)</span><br><span class="line">  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                        Age   From                       Message</span><br><span class="line">  ----     ------                        ----  ----                       -------</span><br><span class="line">  Warning  FailedGetResourceMetric       87s   horizontal-pod-autoscaler  failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API</span><br><span class="line">  Warning  FailedComputeMetricsReplicas  87s   horizontal-pod-autoscaler  invalid metrics (1 invalid out of 1), first error is: failed to get memory utilization: unable to get metrics for resource memory: no metrics returned from resource metrics API</span><br><span class="line">  Normal   SuccessfulRescale             27s   horizontal-pod-autoscaler  New size: 3; reason: memory resource utilization (percentage of request) above target</span><br><span class="line">  Normal   SuccessfulRescale             46s    horizontal-pod-autoscaler  New size: 4; reason: memory resource utilization (percentage of request) above target</span><br><span class="line">➜  ~ kubectl top pod hpa-mem-demo-74675cc6c9-gbj9t</span><br><span class="line">NAME                            CPU(cores)   MEMORY(bytes)</span><br><span class="line">hpa-mem-demo-66944b79bf-tqrn9   0m           41Mi</span><br></pre></td></tr></table></figure>

<p>可以看到内存使用已经超过了我们设定的 30% 这个阈值了，HPA 资源对象也已经触发了自动扩容，变成了 4 个副本了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ kubectl get pods -l app=nginx</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">hpa-mem-demo-74675cc6c9-cpdw4   1/1     Running   0          69s</span><br><span class="line">hpa-mem-demo-74675cc6c9-s8bz4   1/1     Running   0          114s</span><br><span class="line">hpa-mem-demo-74675cc6c9-sqz2l   1/1     Running   0          3m9s</span><br><span class="line">hpa-mem-demo-74675cc6c9-z8cx8   1/1     Running   0          114s</span><br></pre></td></tr></table></figure>

<p>当内存释放掉后，controller-manager 默认 5 分钟过后会进行缩放，到这里就完成了基于内存的 HPA 操作。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://huiaz.github.io">六一</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://huiaz.github.io/2025/09/11/HPA/">https://huiaz.github.io/2025/09/11/HPA/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://huiaz.github.io" target="_blank">Hui's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%BF%90%E7%BB%B4/">运维</a><a class="post-meta__tags" href="/tags/k8s/">k8s</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/09/11/ICMP/" title="ICMP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">ICMP</div></div><div class="info-2"><div class="info-item-1">什么是 ICMP 协议？ICMP (Internet Control Message Protocol)，即互联网控制消息协议，是 IP 协议的辅助协议，是 TCP&#x2F;IP 协议族中的一个核心组成部分。ICMP 通常被认为是网络层的协议（与 IP 协议同层），因为它处理 IP 数据包的错误和控制信息。 核心特点：  控制和错误报告： ICMP 的主要作用是报告 IP 数据包传输过程中的错误以及提供网络诊断信息。它不传输用户数据，而是传输网络层面的控制和错误消息。 不可靠性： ICMP 报文本身不提供可靠传输机制。如果一个 ICMP 报文在传输过程中丢失，IP 层不会重传它。 承载于 IP： ICMP 报文是“封装”在 IP 数据包中的。换句话说，一个 ICMP 报文会作为 IP 数据包的有效载荷（payload）进行传输。  ICMP 的主要作用：ICMP 的主要作用可以归纳为以下几点，主要围绕错误报告和网络诊断：  报告差错信息 (Error Reporting):当 IP 数据包在传输过程中遇到问题时，ICMP 被用来向数据包的源主机报告这些错误。常见的 ICMP 差...</div></div></div></a><a class="pagination-related" href="/2025/09/11/HTTP%20%E7%8A%B6%E6%80%81%E7%A0%81/" title="HTTP 状态码"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">HTTP 状态码</div></div><div class="info-2"><div class="info-item-1">HTTP 状态码HTTP 状态码是服务器对请求的响应。它们是三位数字，分为五个类别，指示请求是否已成功处理，或者是否存在错误。理解这些状态码对于调试网络问题和理解Web应用程序的行为至关重要。 以下是常见的 HTTP 状态码及其简要说明：  1xx - 信息响应 (Informational Responses)指示请求已被接收，继续处理。这些是临时响应，不带任何内容。  100 Continue: 客户端应继续其请求。通常用于客户端发送一个大型请求体到服务器之前，先发送头部，服务器如果允许，则返回 100 Continue，然后客户端再发送请求体。   2xx - 成功响应 (Successful Responses)指示请求已被成功接收、理解和接受。  200 OK: 请求已成功。这是最常见的状态码，表示请求的一切正常，服务器已返回所请求的数据。 201 Created: 请求已成功，并因此创建了一个新的资源。这通常是 PUT 或 POST 请求的响应。响应体中通常包含新创建资源的URI。 202 Accepted: 请求已被接受进行处理，但处理尚未完成。请求可能最终被执行，...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/11/k8s_12/" title="k8s_12"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s_12</div></div><div class="info-2"><div class="info-item-1">如何在 Kubernetes 中实现服务的自动伸缩（autoscaling）？Of course. Automating the scaling of services is one of Kubernetes’ most powerful features, enabling applications to be both resilient to traffic spikes and cost-effective during quiet periods. This is achieved through a combination of several components. 🤔 分析过程：该问题旨在考察对Kubernetes核心动态管理能力的理解。一个全面的回答不能只提及一种自动伸缩方式，而应结构化地介绍Kubernetes中三个主要层次的自动伸缩器：HPA (水平), VPA (垂直), 和 CA (集群)。回答的重点在于阐明每种伸缩器的触发机制、作用范围和典型用例，并解释它们如何协同工作，共同构建一个弹性的、资源高效的系统。 💡 答案生成：1. 概念或定义Kube...</div></div></div></a><a class="pagination-related" href="/2025/09/11/k8s%20service/" title="k8s service"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s service</div></div><div class="info-2"><div class="info-item-1">Service 主要有以下几种类型：  ClusterIP (默认类型) NodePort LoadBalancer ExternalName  接下来，我将分别详细介绍每种类型及其应用场景。  1. ClusterIP (默认类型) 作用： 为 Service 在 Kubernetes 集群内部分配一个唯一的、虚拟的 IP 地址（ClusterIP）。这个 IP 地址只在集群内部可达。  访问方式： 集群内部的其他 Pod 或组件可以通过这个 ClusterIP 和端口来访问该 Service 后端的 Pod。  特点：  内部可见： 只能在集群内部访问，外部无法直接访问。 负载均衡： Pod 流量通过 kube-proxy 代理转发到后端的 Pod，并自动进行简单的轮询式负载均衡。 稳定性： 提供了稳定的 IP 地址和 DNS 名称， Pod 的 IP 变化不会影响 Service 的可访问性。   使用场景：  内部服务通信： 最常见的用于微服务之间互相调用的场景。例如，前端服务访问后端服务，或其他服务访问数据库服务等。 集群内部调试。   示例 YAML 片段: 1234...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E5%A6%82%E4%BD%95%E4%BF%AE%E6%94%B9%20Kubernetes%20%E8%8A%82%E7%82%B9%20IP%20%E5%9C%B0%E5%9D%80?/" title="如何修改 Kubernetes 节点 IP 地址?"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">如何修改 Kubernetes 节点 IP 地址?</div></div><div class="info-2"><div class="info-item-1">如何修改 Kubernetes 节点 IP 地址?昨天网络环境出了点问题，本地的虚拟机搭建的 Kubernetes 环境没有固定 IP，结果节点 IP 变了，当然最简单的方式是将节点重新固定回之前的 IP 地址，但是自己头铁想去修改下集群的 IP 地址，结果一路下来踩了好多坑，压根就没那么简单~ 环境首先看下之前的环境： 1234➜  ~ cat /etc/hosts192.168.0.111 master1192.168.0.109 node1192.168.0.110 node2  新的 IP 地址： 1234➜  ~ cat /etc/hosts192.168.0.106 master1192.168.0.101 node1192.168.0.105 node2  所以我们需要修改所有节点的 IP 地址。 操作首先将所有节点的 /etc/hosts 更改为新的地址。  提示：在操作任何文件之前强烈建议先备份。  master 节点1.备份 /etc/kubernetes 目录。 1➜ cp -Rf /etc/kubernetes/ /etc/kubernetes-bak  ...</div></div></div></a><a class="pagination-related" href="/2025/09/11/%E7%9B%91%E6%8E%A7%20APIServer/" title="监控 APIServer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">监控 APIServer</div></div><div class="info-2"><div class="info-item-1">监控 APIServerAPIServer 作为 Kubernetes 最核心的组件，当然他的监控也是非常有必要的，对于 APIServer 的监控我们可以直接通过 Kubernetes 的 Service 来获取： 123☸ ➜ kubectl get svcNAME             TYPE           CLUSTER-IP       EXTERNAL-IP             PORT(S)          AGEkubernetes       ClusterIP      10.96.0.1        &lt;none&gt;                  443/TCP          33d  自动发现上面这个 Service 就是我们集群的 apiserver 在集群内部的 Service 地址，要自动发现 Service 类型的服务，我们就需要用到 role 为 Endpoints 的 kubernetes_sd_configs，我们可以在 ConfigMap 对象中添加上一个 Endpoints 类型的服务的监控任务： 123...</div></div></div></a><a class="pagination-related" href="/2025/09/11/k8s%20Logs/" title="k8s Logs"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">k8s Logs</div></div><div class="info-2"><div class="info-item-1">描述在 Kubernetes 中如何进行日志管理，并解释常用的方法。 Of course. Log management is a cornerstone of observability in any production system, and Kubernetes presents unique challenges and solutions. This is a great question to assess a candidate’s operational maturity. 🤔 分析过程：该问题旨在考察对Kubernetes中日志生命周期和集中式日志解决方案的理解。一个全面的回答需要首先解释为什么基础的kubectl logs命令在生产环境中不足够，从而引出集中式日志管理的需求。接着，需要清晰地描述最主流的实现架构（节点级日志代理），并能解释其工作流程。提及Sidecar模式作为补充方案，以及介绍市面上流行的工具栈（如EFK、PLG），可以充分展现面试者在该领域的知识广度和深度。 💡 答案生成：1. 概念或定义Kubernetes日志管理是指一套系统性的方法...</div></div></div></a><a class="pagination-related" href="/2025/09/11/GitLab%20CI%20%E4%B8%8E%20Argo%20CD%20%E8%BF%9B%E8%A1%8C%20GitOps%20%E5%AE%9E%E8%B7%B5/" title="GitLab CI 与 Argo CD 进行 GitOps 实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-11</div><div class="info-item-2">GitLab CI 与 Argo CD 进行 GitOps 实践</div></div><div class="info-2"><div class="info-item-1">使用 GitLab CI 与 Argo CD 进行 GitOps 实践在现在的云原生世界里面 GitOps 不断的被提及，这种持续交付的模式越来越受到了大家的青睐，我们前面也有文章详细讲解了 GitOps 的相关概念，在网上也可以找到很多关于它的资源，但是关于 GitOps 相关的工作流实践的示例却并不多见，我们这里就将详细介绍一个使用示例，希望对大家实践 GitOps 有所帮助。 介绍 上图是当前示例中的 GitOps 工作流程。GitLab 和 Argo CD 是两个主要的核心组件： Argo CD 是一个声明式、GitOps 持续交付的 Kubernetes 工具，它的配置和使用分非常简单，并且自带一个简单一用的 Dashboard 页面，更重要的是 Argo CD 支持 kustomzie、helm、ksonnet 等多种工具。应用程序可以通过 Argo CD 提供的 CRD 资源对象进行配置，可以在指定的目标环境中自动部署所需的应用程序。关于 Argo CD 更多的信息可以查看官方文档了解更多。 GitLab CI 是 GitLab 的持续集成和持续交付的工具，也是非常...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">六一</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">274</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">45</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/huiaz"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#HPA-%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-number">1.</span> <span class="toc-text">HPA 控制器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Metrics-Server"><span class="toc-number">1.1.</span> <span class="toc-text">Metrics Server</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E5%90%88-API"><span class="toc-number">1.1.1.</span> <span class="toc-text">聚合 API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.2.</span> <span class="toc-text">安装</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HPA-%E5%AF%B9%E8%B1%A1"><span class="toc-number">1.2.</span> <span class="toc-text">HPA 对象</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98"><span class="toc-number">1.3.</span> <span class="toc-text">内存</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/27/hexo-beautify/" title="Hexo 博客美化笔记：从零搭建高颜值技术博客">Hexo 博客美化笔记：从零搭建高颜值技术博客</a><time datetime="2026-02-27T04:00:00.000Z" title="发表于 2026-02-27 12:00:00">2026-02-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/Jenkens-Blue%20Ocean%20%E6%8F%92%E4%BB%B6/" title="Jenkins Blue Ocean">Jenkins Blue Ocean</a><time datetime="2025-09-11T12:42:57.000Z" title="发表于 2025-09-11 20:42:57">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/jenkins-jenkinsfile/" title="Jenkins Jenkinsfile">Jenkins Jenkinsfile</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-trap/" title="Linux-trap">Linux-trap</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/11/shell-%E6%95%B0%E7%BB%84/" title="Shell-数组">Shell-数组</a><time datetime="2025-09-11T12:40:44.000Z" title="发表于 2025-09-11 20:40:44">2025-09-11</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By 六一</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body></html>